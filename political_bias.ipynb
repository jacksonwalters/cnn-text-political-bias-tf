{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "WARNING:tensorflow:From /home/jackson/GitHub/cnn-text-political-bias-tf/train.py:55: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/jackson/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/jackson/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Vocabulary Size: 13449\n",
      "Train/Dev split: 4885/542\n",
      "WARNING:tensorflow:From /home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jackson/GitHub/cnn-text-political-bias-tf/text_cnn.py:62: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/jackson/GitHub/cnn-text-political-bias-tf/text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664\n",
      "\n",
      "2019-03-27T13:11:09.012865: step 1, loss 3.09929, acc 0.40625\n",
      "2019-03-27T13:11:09.081866: step 2, loss 3.37106, acc 0.328125\n",
      "2019-03-27T13:11:09.140068: step 3, loss 3.44977, acc 0.375\n",
      "2019-03-27T13:11:09.199436: step 4, loss 3.63744, acc 0.3125\n",
      "2019-03-27T13:11:09.258783: step 5, loss 3.37485, acc 0.234375\n",
      "2019-03-27T13:11:09.316933: step 6, loss 3.65052, acc 0.328125\n",
      "2019-03-27T13:11:09.375820: step 7, loss 3.39307, acc 0.328125\n",
      "2019-03-27T13:11:09.435251: step 8, loss 3.11838, acc 0.328125\n",
      "2019-03-27T13:11:09.492297: step 9, loss 2.82894, acc 0.390625\n",
      "2019-03-27T13:11:09.552178: step 10, loss 3.43481, acc 0.40625\n",
      "2019-03-27T13:11:09.607829: step 11, loss 2.84196, acc 0.421875\n",
      "2019-03-27T13:11:09.667954: step 12, loss 3.02752, acc 0.359375\n",
      "2019-03-27T13:11:09.725501: step 13, loss 3.36402, acc 0.359375\n",
      "2019-03-27T13:11:09.782229: step 14, loss 2.89245, acc 0.40625\n",
      "2019-03-27T13:11:09.837215: step 15, loss 2.56473, acc 0.328125\n",
      "2019-03-27T13:11:09.896158: step 16, loss 3.28999, acc 0.296875\n",
      "2019-03-27T13:11:09.951440: step 17, loss 3.36461, acc 0.3125\n",
      "2019-03-27T13:11:10.007987: step 18, loss 4.06575, acc 0.296875\n",
      "2019-03-27T13:11:10.066273: step 19, loss 2.65134, acc 0.328125\n",
      "2019-03-27T13:11:10.127878: step 20, loss 3.43773, acc 0.296875\n",
      "2019-03-27T13:11:10.192068: step 21, loss 3.2316, acc 0.21875\n",
      "2019-03-27T13:11:10.255143: step 22, loss 3.93203, acc 0.234375\n",
      "2019-03-27T13:11:10.316204: step 23, loss 3.64344, acc 0.34375\n",
      "2019-03-27T13:11:10.377061: step 24, loss 2.83983, acc 0.40625\n",
      "2019-03-27T13:11:10.435132: step 25, loss 2.62346, acc 0.40625\n",
      "2019-03-27T13:11:10.496236: step 26, loss 2.75065, acc 0.3125\n",
      "2019-03-27T13:11:10.557710: step 27, loss 3.00687, acc 0.34375\n",
      "2019-03-27T13:11:10.614277: step 28, loss 2.84258, acc 0.375\n",
      "2019-03-27T13:11:10.675741: step 29, loss 2.8192, acc 0.375\n",
      "2019-03-27T13:11:10.735261: step 30, loss 2.77958, acc 0.453125\n",
      "2019-03-27T13:11:10.791043: step 31, loss 3.34903, acc 0.3125\n",
      "2019-03-27T13:11:10.847022: step 32, loss 3.22639, acc 0.296875\n",
      "2019-03-27T13:11:10.906933: step 33, loss 2.48757, acc 0.390625\n",
      "2019-03-27T13:11:10.971629: step 34, loss 2.66762, acc 0.34375\n",
      "2019-03-27T13:11:11.032214: step 35, loss 2.78483, acc 0.359375\n",
      "2019-03-27T13:11:11.095377: step 36, loss 3.77387, acc 0.265625\n",
      "2019-03-27T13:11:11.154887: step 37, loss 2.39868, acc 0.40625\n",
      "2019-03-27T13:11:11.216409: step 38, loss 3.47343, acc 0.390625\n",
      "2019-03-27T13:11:11.274976: step 39, loss 3.07568, acc 0.28125\n",
      "2019-03-27T13:11:11.331994: step 40, loss 2.80492, acc 0.359375\n",
      "2019-03-27T13:11:11.391594: step 41, loss 3.05786, acc 0.265625\n",
      "2019-03-27T13:11:11.453214: step 42, loss 3.61364, acc 0.28125\n",
      "2019-03-27T13:11:11.510001: step 43, loss 2.43036, acc 0.390625\n",
      "2019-03-27T13:11:11.566824: step 44, loss 3.47614, acc 0.328125\n",
      "2019-03-27T13:11:11.630790: step 45, loss 2.5073, acc 0.375\n",
      "2019-03-27T13:11:11.689301: step 46, loss 2.90482, acc 0.359375\n",
      "2019-03-27T13:11:11.749536: step 47, loss 2.83053, acc 0.359375\n",
      "2019-03-27T13:11:11.805530: step 48, loss 3.24519, acc 0.3125\n",
      "2019-03-27T13:11:11.865387: step 49, loss 3.18233, acc 0.34375\n",
      "2019-03-27T13:11:11.922678: step 50, loss 2.34016, acc 0.40625\n",
      "2019-03-27T13:11:11.981749: step 51, loss 3.12169, acc 0.3125\n",
      "2019-03-27T13:11:12.040094: step 52, loss 3.55055, acc 0.3125\n",
      "2019-03-27T13:11:12.099063: step 53, loss 3.16134, acc 0.421875\n",
      "2019-03-27T13:11:12.157808: step 54, loss 3.39348, acc 0.296875\n",
      "2019-03-27T13:11:12.218775: step 55, loss 3.32529, acc 0.3125\n",
      "2019-03-27T13:11:12.277473: step 56, loss 3.1069, acc 0.40625\n",
      "2019-03-27T13:11:12.334690: step 57, loss 3.70705, acc 0.25\n",
      "2019-03-27T13:11:12.392872: step 58, loss 3.54758, acc 0.234375\n",
      "2019-03-27T13:11:12.451828: step 59, loss 2.81857, acc 0.359375\n",
      "2019-03-27T13:11:12.509797: step 60, loss 2.39987, acc 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:11:12.568655: step 61, loss 3.23363, acc 0.21875\n",
      "2019-03-27T13:11:12.628319: step 62, loss 3.77402, acc 0.21875\n",
      "2019-03-27T13:11:12.685331: step 63, loss 2.8438, acc 0.359375\n",
      "2019-03-27T13:11:12.742958: step 64, loss 3.36367, acc 0.25\n",
      "2019-03-27T13:11:12.801094: step 65, loss 3.07237, acc 0.296875\n",
      "2019-03-27T13:11:12.858599: step 66, loss 3.31516, acc 0.3125\n",
      "2019-03-27T13:11:12.920489: step 67, loss 3.91277, acc 0.328125\n",
      "2019-03-27T13:11:12.979697: step 68, loss 2.90659, acc 0.296875\n",
      "2019-03-27T13:11:13.033755: step 69, loss 2.72537, acc 0.34375\n",
      "2019-03-27T13:11:13.092771: step 70, loss 2.55778, acc 0.484375\n",
      "2019-03-27T13:11:13.149105: step 71, loss 2.6913, acc 0.34375\n",
      "2019-03-27T13:11:13.206741: step 72, loss 3.16175, acc 0.28125\n",
      "2019-03-27T13:11:13.264582: step 73, loss 3.15437, acc 0.265625\n",
      "2019-03-27T13:11:13.329784: step 74, loss 3.0399, acc 0.328125\n",
      "2019-03-27T13:11:13.391592: step 75, loss 2.59445, acc 0.390625\n",
      "2019-03-27T13:11:13.452554: step 76, loss 3.0694, acc 0.40625\n",
      "2019-03-27T13:11:13.693934: step 77, loss 3.771, acc 0.142857\n",
      "2019-03-27T13:11:13.755119: step 78, loss 1.96926, acc 0.34375\n",
      "2019-03-27T13:11:13.817986: step 79, loss 2.34236, acc 0.390625\n",
      "2019-03-27T13:11:13.877509: step 80, loss 2.33296, acc 0.390625\n",
      "2019-03-27T13:11:13.936820: step 81, loss 2.8403, acc 0.375\n",
      "2019-03-27T13:11:13.995974: step 82, loss 2.62484, acc 0.390625\n",
      "2019-03-27T13:11:14.056769: step 83, loss 2.85487, acc 0.296875\n",
      "2019-03-27T13:11:14.116794: step 84, loss 3.0075, acc 0.375\n",
      "2019-03-27T13:11:14.179218: step 85, loss 2.93563, acc 0.359375\n",
      "2019-03-27T13:11:14.237655: step 86, loss 1.96266, acc 0.484375\n",
      "2019-03-27T13:11:14.296350: step 87, loss 2.68863, acc 0.328125\n",
      "2019-03-27T13:11:14.358281: step 88, loss 2.21655, acc 0.390625\n",
      "2019-03-27T13:11:14.417748: step 89, loss 2.40392, acc 0.390625\n",
      "2019-03-27T13:11:14.479373: step 90, loss 1.72084, acc 0.515625\n",
      "2019-03-27T13:11:14.538102: step 91, loss 2.04425, acc 0.390625\n",
      "2019-03-27T13:11:14.602587: step 92, loss 1.90764, acc 0.4375\n",
      "2019-03-27T13:11:14.661553: step 93, loss 2.31185, acc 0.40625\n",
      "2019-03-27T13:11:14.722612: step 94, loss 3.30132, acc 0.296875\n",
      "2019-03-27T13:11:14.784489: step 95, loss 1.95783, acc 0.5\n",
      "2019-03-27T13:11:14.839919: step 96, loss 2.14648, acc 0.421875\n",
      "2019-03-27T13:11:14.898087: step 97, loss 2.01131, acc 0.421875\n",
      "2019-03-27T13:11:14.954961: step 98, loss 2.44291, acc 0.375\n",
      "2019-03-27T13:11:15.013997: step 99, loss 2.27514, acc 0.421875\n",
      "2019-03-27T13:11:15.070713: step 100, loss 2.23996, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:11:15.206202: step 100, loss 1.3459, acc 0.372694\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-100\n",
      "\n",
      "2019-03-27T13:11:15.408329: step 101, loss 2.51248, acc 0.359375\n",
      "2019-03-27T13:11:15.470943: step 102, loss 2.51563, acc 0.328125\n",
      "2019-03-27T13:11:15.528307: step 103, loss 2.4961, acc 0.34375\n",
      "2019-03-27T13:11:15.586789: step 104, loss 2.17585, acc 0.375\n",
      "2019-03-27T13:11:15.646916: step 105, loss 1.84673, acc 0.5\n",
      "2019-03-27T13:11:15.709796: step 106, loss 2.61911, acc 0.375\n",
      "2019-03-27T13:11:15.767416: step 107, loss 2.47392, acc 0.296875\n",
      "2019-03-27T13:11:15.823231: step 108, loss 2.44386, acc 0.421875\n",
      "2019-03-27T13:11:15.885079: step 109, loss 2.68015, acc 0.359375\n",
      "2019-03-27T13:11:15.943797: step 110, loss 2.27288, acc 0.359375\n",
      "2019-03-27T13:11:16.000171: step 111, loss 2.88528, acc 0.296875\n",
      "2019-03-27T13:11:16.059634: step 112, loss 2.60981, acc 0.34375\n",
      "2019-03-27T13:11:16.121068: step 113, loss 2.47603, acc 0.3125\n",
      "2019-03-27T13:11:16.179761: step 114, loss 2.90182, acc 0.265625\n",
      "2019-03-27T13:11:16.237774: step 115, loss 2.56637, acc 0.296875\n",
      "2019-03-27T13:11:16.294144: step 116, loss 2.34218, acc 0.40625\n",
      "2019-03-27T13:11:16.354467: step 117, loss 2.27455, acc 0.328125\n",
      "2019-03-27T13:11:16.415998: step 118, loss 2.15941, acc 0.390625\n",
      "2019-03-27T13:11:16.477104: step 119, loss 2.59378, acc 0.375\n",
      "2019-03-27T13:11:16.534939: step 120, loss 2.52224, acc 0.3125\n",
      "2019-03-27T13:11:16.597660: step 121, loss 2.13643, acc 0.453125\n",
      "2019-03-27T13:11:16.656070: step 122, loss 1.97045, acc 0.484375\n",
      "2019-03-27T13:11:16.718906: step 123, loss 1.8289, acc 0.5\n",
      "2019-03-27T13:11:16.779926: step 124, loss 2.54178, acc 0.328125\n",
      "2019-03-27T13:11:16.844416: step 125, loss 2.79135, acc 0.3125\n",
      "2019-03-27T13:11:16.907021: step 126, loss 2.52406, acc 0.328125\n",
      "2019-03-27T13:11:16.964682: step 127, loss 2.54999, acc 0.359375\n",
      "2019-03-27T13:11:17.029217: step 128, loss 2.02068, acc 0.46875\n",
      "2019-03-27T13:11:17.091245: step 129, loss 1.84378, acc 0.421875\n",
      "2019-03-27T13:11:17.151605: step 130, loss 2.552, acc 0.34375\n",
      "2019-03-27T13:11:17.210098: step 131, loss 2.05148, acc 0.328125\n",
      "2019-03-27T13:11:17.267162: step 132, loss 1.95851, acc 0.40625\n",
      "2019-03-27T13:11:17.326024: step 133, loss 2.00083, acc 0.4375\n",
      "2019-03-27T13:11:17.387738: step 134, loss 2.32931, acc 0.3125\n",
      "2019-03-27T13:11:17.443185: step 135, loss 2.60099, acc 0.328125\n",
      "2019-03-27T13:11:17.501388: step 136, loss 2.15546, acc 0.4375\n",
      "2019-03-27T13:11:17.561001: step 137, loss 1.92628, acc 0.453125\n",
      "2019-03-27T13:11:17.620615: step 138, loss 2.61659, acc 0.34375\n",
      "2019-03-27T13:11:17.679345: step 139, loss 2.31556, acc 0.296875\n",
      "2019-03-27T13:11:17.740190: step 140, loss 1.87704, acc 0.46875\n",
      "2019-03-27T13:11:17.804944: step 141, loss 2.06003, acc 0.34375\n",
      "2019-03-27T13:11:17.863337: step 142, loss 2.29736, acc 0.34375\n",
      "2019-03-27T13:11:17.926292: step 143, loss 2.17735, acc 0.453125\n",
      "2019-03-27T13:11:17.982561: step 144, loss 1.98773, acc 0.390625\n",
      "2019-03-27T13:11:18.043812: step 145, loss 1.96777, acc 0.453125\n",
      "2019-03-27T13:11:18.102485: step 146, loss 1.75043, acc 0.40625\n",
      "2019-03-27T13:11:18.159209: step 147, loss 2.17254, acc 0.359375\n",
      "2019-03-27T13:11:18.217111: step 148, loss 2.07387, acc 0.421875\n",
      "2019-03-27T13:11:18.281196: step 149, loss 2.41487, acc 0.265625\n",
      "2019-03-27T13:11:18.339372: step 150, loss 2.18987, acc 0.34375\n",
      "2019-03-27T13:11:18.395475: step 151, loss 1.81162, acc 0.484375\n",
      "2019-03-27T13:11:18.454509: step 152, loss 2.31545, acc 0.40625\n",
      "2019-03-27T13:11:18.512814: step 153, loss 2.01905, acc 0.390625\n",
      "2019-03-27T13:11:18.559920: step 154, loss 1.9911, acc 0.380952\n",
      "2019-03-27T13:11:18.615960: step 155, loss 2.13544, acc 0.421875\n",
      "2019-03-27T13:11:18.675570: step 156, loss 1.75951, acc 0.390625\n",
      "2019-03-27T13:11:18.736131: step 157, loss 2.07095, acc 0.4375\n",
      "2019-03-27T13:11:18.794165: step 158, loss 2.1016, acc 0.359375\n",
      "2019-03-27T13:11:18.852866: step 159, loss 1.62046, acc 0.453125\n",
      "2019-03-27T13:11:18.911889: step 160, loss 2.01603, acc 0.359375\n",
      "2019-03-27T13:11:18.970922: step 161, loss 1.49394, acc 0.5625\n",
      "2019-03-27T13:11:19.030146: step 162, loss 2.05237, acc 0.421875\n",
      "2019-03-27T13:11:19.090093: step 163, loss 1.79434, acc 0.5\n",
      "2019-03-27T13:11:19.148185: step 164, loss 2.06485, acc 0.375\n",
      "2019-03-27T13:11:19.210701: step 165, loss 2.02015, acc 0.4375\n",
      "2019-03-27T13:11:19.271891: step 166, loss 1.84435, acc 0.421875\n",
      "2019-03-27T13:11:19.331854: step 167, loss 1.98282, acc 0.5\n",
      "2019-03-27T13:11:19.393294: step 168, loss 2.47462, acc 0.390625\n",
      "2019-03-27T13:11:19.456658: step 169, loss 2.1204, acc 0.390625\n",
      "2019-03-27T13:11:19.516966: step 170, loss 2.15215, acc 0.375\n",
      "2019-03-27T13:11:19.574096: step 171, loss 2.17876, acc 0.421875\n",
      "2019-03-27T13:11:19.632505: step 172, loss 2.01167, acc 0.40625\n",
      "2019-03-27T13:11:19.693908: step 173, loss 1.98361, acc 0.375\n",
      "2019-03-27T13:11:19.754393: step 174, loss 1.94451, acc 0.375\n",
      "2019-03-27T13:11:19.812261: step 175, loss 1.38994, acc 0.546875\n",
      "2019-03-27T13:11:19.873128: step 176, loss 1.24314, acc 0.53125\n",
      "2019-03-27T13:11:19.932215: step 177, loss 1.75082, acc 0.46875\n",
      "2019-03-27T13:11:19.992735: step 178, loss 1.71801, acc 0.453125\n",
      "2019-03-27T13:11:20.048187: step 179, loss 2.09031, acc 0.28125\n",
      "2019-03-27T13:11:20.106653: step 180, loss 2.32178, acc 0.34375\n",
      "2019-03-27T13:11:20.168398: step 181, loss 1.7788, acc 0.390625\n",
      "2019-03-27T13:11:20.227447: step 182, loss 1.94827, acc 0.4375\n",
      "2019-03-27T13:11:20.286205: step 183, loss 1.95367, acc 0.375\n",
      "2019-03-27T13:11:20.343626: step 184, loss 1.95604, acc 0.34375\n",
      "2019-03-27T13:11:20.401601: step 185, loss 1.84475, acc 0.34375\n",
      "2019-03-27T13:11:20.461227: step 186, loss 1.69952, acc 0.484375\n",
      "2019-03-27T13:11:20.518918: step 187, loss 1.69653, acc 0.421875\n",
      "2019-03-27T13:11:20.578015: step 188, loss 1.80609, acc 0.34375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:11:20.637595: step 189, loss 1.77723, acc 0.421875\n",
      "2019-03-27T13:11:20.701575: step 190, loss 1.69877, acc 0.421875\n",
      "2019-03-27T13:11:20.759006: step 191, loss 2.13052, acc 0.359375\n",
      "2019-03-27T13:11:20.819803: step 192, loss 1.52846, acc 0.453125\n",
      "2019-03-27T13:11:20.882507: step 193, loss 2.20008, acc 0.359375\n",
      "2019-03-27T13:11:20.944175: step 194, loss 1.70467, acc 0.4375\n",
      "2019-03-27T13:11:21.004973: step 195, loss 1.98693, acc 0.359375\n",
      "2019-03-27T13:11:21.067242: step 196, loss 2.05972, acc 0.328125\n",
      "2019-03-27T13:11:21.124168: step 197, loss 2.05245, acc 0.375\n",
      "2019-03-27T13:11:21.182100: step 198, loss 1.53187, acc 0.4375\n",
      "2019-03-27T13:11:21.240799: step 199, loss 2.03445, acc 0.40625\n",
      "2019-03-27T13:11:21.299876: step 200, loss 1.59551, acc 0.40625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:11:21.331761: step 200, loss 1.32008, acc 0.369004\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-200\n",
      "\n",
      "2019-03-27T13:11:21.490182: step 201, loss 1.64901, acc 0.46875\n",
      "2019-03-27T13:11:21.552290: step 202, loss 2.02654, acc 0.375\n",
      "2019-03-27T13:11:21.613192: step 203, loss 1.86317, acc 0.390625\n",
      "2019-03-27T13:11:21.675295: step 204, loss 1.70833, acc 0.359375\n",
      "2019-03-27T13:11:21.732393: step 205, loss 1.49793, acc 0.46875\n",
      "2019-03-27T13:11:21.790122: step 206, loss 1.55489, acc 0.5\n",
      "2019-03-27T13:11:21.846601: step 207, loss 1.4952, acc 0.390625\n",
      "2019-03-27T13:11:21.905590: step 208, loss 1.87453, acc 0.390625\n",
      "2019-03-27T13:11:21.965372: step 209, loss 1.90834, acc 0.375\n",
      "2019-03-27T13:11:22.022824: step 210, loss 1.85927, acc 0.375\n",
      "2019-03-27T13:11:22.081297: step 211, loss 1.34551, acc 0.515625\n",
      "2019-03-27T13:11:22.139523: step 212, loss 1.59876, acc 0.484375\n",
      "2019-03-27T13:11:22.202624: step 213, loss 1.96421, acc 0.34375\n",
      "2019-03-27T13:11:22.264978: step 214, loss 1.53521, acc 0.375\n",
      "2019-03-27T13:11:22.324553: step 215, loss 1.57088, acc 0.421875\n",
      "2019-03-27T13:11:22.382440: step 216, loss 1.47256, acc 0.484375\n",
      "2019-03-27T13:11:22.439636: step 217, loss 1.64429, acc 0.484375\n",
      "2019-03-27T13:11:22.501453: step 218, loss 1.59495, acc 0.5\n",
      "2019-03-27T13:11:22.562441: step 219, loss 1.83041, acc 0.40625\n",
      "2019-03-27T13:11:22.620343: step 220, loss 2.06885, acc 0.328125\n",
      "2019-03-27T13:11:22.678110: step 221, loss 1.73426, acc 0.421875\n",
      "2019-03-27T13:11:22.737374: step 222, loss 1.52612, acc 0.375\n",
      "2019-03-27T13:11:22.798841: step 223, loss 1.56098, acc 0.484375\n",
      "2019-03-27T13:11:22.859310: step 224, loss 1.95006, acc 0.328125\n",
      "2019-03-27T13:11:22.917461: step 225, loss 1.45631, acc 0.546875\n",
      "2019-03-27T13:11:22.973221: step 226, loss 1.84468, acc 0.34375\n",
      "2019-03-27T13:11:23.032082: step 227, loss 1.67184, acc 0.359375\n",
      "2019-03-27T13:11:23.090858: step 228, loss 2.06525, acc 0.3125\n",
      "2019-03-27T13:11:23.149241: step 229, loss 1.75007, acc 0.375\n",
      "2019-03-27T13:11:23.208311: step 230, loss 1.48285, acc 0.484375\n",
      "2019-03-27T13:11:23.254672: step 231, loss 2.26758, acc 0.380952\n",
      "2019-03-27T13:11:23.318416: step 232, loss 1.35181, acc 0.546875\n",
      "2019-03-27T13:11:23.378288: step 233, loss 1.82519, acc 0.40625\n",
      "2019-03-27T13:11:23.439640: step 234, loss 1.86421, acc 0.28125\n",
      "2019-03-27T13:11:23.499359: step 235, loss 1.70554, acc 0.375\n",
      "2019-03-27T13:11:23.557173: step 236, loss 1.55268, acc 0.4375\n",
      "2019-03-27T13:11:23.615189: step 237, loss 1.82148, acc 0.421875\n",
      "2019-03-27T13:11:23.675239: step 238, loss 1.78395, acc 0.359375\n",
      "2019-03-27T13:11:23.732892: step 239, loss 1.80593, acc 0.421875\n",
      "2019-03-27T13:11:23.790336: step 240, loss 1.54632, acc 0.390625\n",
      "2019-03-27T13:11:23.848453: step 241, loss 1.53509, acc 0.46875\n",
      "2019-03-27T13:11:23.905338: step 242, loss 1.79451, acc 0.359375\n",
      "2019-03-27T13:11:23.964495: step 243, loss 1.45324, acc 0.390625\n",
      "2019-03-27T13:11:24.021866: step 244, loss 1.40597, acc 0.46875\n",
      "2019-03-27T13:11:24.078977: step 245, loss 1.64805, acc 0.421875\n",
      "2019-03-27T13:11:24.140790: step 246, loss 1.38665, acc 0.484375\n",
      "2019-03-27T13:11:24.200263: step 247, loss 1.3141, acc 0.421875\n",
      "2019-03-27T13:11:24.260689: step 248, loss 1.61668, acc 0.40625\n",
      "2019-03-27T13:11:24.321272: step 249, loss 1.62873, acc 0.4375\n",
      "2019-03-27T13:11:24.379977: step 250, loss 1.68174, acc 0.28125\n",
      "2019-03-27T13:11:24.440161: step 251, loss 1.5844, acc 0.46875\n",
      "2019-03-27T13:11:24.495879: step 252, loss 1.72488, acc 0.453125\n",
      "2019-03-27T13:11:24.558551: step 253, loss 1.48072, acc 0.4375\n",
      "2019-03-27T13:11:24.615064: step 254, loss 1.60215, acc 0.453125\n",
      "2019-03-27T13:11:24.675909: step 255, loss 1.19715, acc 0.421875\n",
      "2019-03-27T13:11:24.734007: step 256, loss 1.65976, acc 0.421875\n",
      "2019-03-27T13:11:24.792029: step 257, loss 1.5073, acc 0.453125\n",
      "2019-03-27T13:11:24.850502: step 258, loss 1.57742, acc 0.40625\n",
      "2019-03-27T13:11:24.910662: step 259, loss 1.41572, acc 0.453125\n",
      "2019-03-27T13:11:24.970304: step 260, loss 1.54204, acc 0.46875\n",
      "2019-03-27T13:11:25.028420: step 261, loss 1.91565, acc 0.375\n",
      "2019-03-27T13:11:25.083962: step 262, loss 1.5809, acc 0.4375\n",
      "2019-03-27T13:11:25.147121: step 263, loss 1.32081, acc 0.5\n",
      "2019-03-27T13:11:25.210373: step 264, loss 1.31114, acc 0.4375\n",
      "2019-03-27T13:11:25.270420: step 265, loss 1.50506, acc 0.4375\n",
      "2019-03-27T13:11:25.331108: step 266, loss 1.1374, acc 0.53125\n",
      "2019-03-27T13:11:25.391990: step 267, loss 1.53339, acc 0.46875\n",
      "2019-03-27T13:11:25.449329: step 268, loss 1.37409, acc 0.390625\n",
      "2019-03-27T13:11:25.508103: step 269, loss 1.41497, acc 0.375\n",
      "2019-03-27T13:11:25.569145: step 270, loss 1.62578, acc 0.390625\n",
      "2019-03-27T13:11:25.629692: step 271, loss 1.39975, acc 0.484375\n",
      "2019-03-27T13:11:25.687856: step 272, loss 1.63157, acc 0.40625\n",
      "2019-03-27T13:11:25.747487: step 273, loss 1.80795, acc 0.40625\n",
      "2019-03-27T13:11:25.808442: step 274, loss 1.51909, acc 0.421875\n",
      "2019-03-27T13:11:25.866109: step 275, loss 1.24856, acc 0.4375\n",
      "2019-03-27T13:11:25.922893: step 276, loss 1.58416, acc 0.390625\n",
      "2019-03-27T13:11:25.977734: step 277, loss 1.67422, acc 0.421875\n",
      "2019-03-27T13:11:26.036682: step 278, loss 1.72358, acc 0.296875\n",
      "2019-03-27T13:11:26.095635: step 279, loss 1.6005, acc 0.390625\n",
      "2019-03-27T13:11:26.151759: step 280, loss 1.84232, acc 0.296875\n",
      "2019-03-27T13:11:26.209777: step 281, loss 1.65169, acc 0.40625\n",
      "2019-03-27T13:11:26.266909: step 282, loss 1.431, acc 0.421875\n",
      "2019-03-27T13:11:26.327184: step 283, loss 1.17474, acc 0.484375\n",
      "2019-03-27T13:11:26.384620: step 284, loss 1.77877, acc 0.28125\n",
      "2019-03-27T13:11:26.443676: step 285, loss 1.70324, acc 0.296875\n",
      "2019-03-27T13:11:26.500352: step 286, loss 1.7009, acc 0.296875\n",
      "2019-03-27T13:11:26.557172: step 287, loss 1.53387, acc 0.421875\n",
      "2019-03-27T13:11:26.617910: step 288, loss 1.54713, acc 0.40625\n",
      "2019-03-27T13:11:26.675788: step 289, loss 1.45139, acc 0.5\n",
      "2019-03-27T13:11:26.733711: step 290, loss 1.71633, acc 0.390625\n",
      "2019-03-27T13:11:26.791315: step 291, loss 1.31444, acc 0.5\n",
      "2019-03-27T13:11:26.854097: step 292, loss 1.47612, acc 0.484375\n",
      "2019-03-27T13:11:26.910050: step 293, loss 1.30748, acc 0.453125\n",
      "2019-03-27T13:11:26.967335: step 294, loss 1.35875, acc 0.484375\n",
      "2019-03-27T13:11:27.025959: step 295, loss 1.59993, acc 0.375\n",
      "2019-03-27T13:11:27.086117: step 296, loss 1.75865, acc 0.296875\n",
      "2019-03-27T13:11:27.142512: step 297, loss 1.17428, acc 0.484375\n",
      "2019-03-27T13:11:27.201502: step 298, loss 1.44857, acc 0.359375\n",
      "2019-03-27T13:11:27.260675: step 299, loss 1.04902, acc 0.46875\n",
      "2019-03-27T13:11:27.322065: step 300, loss 1.41163, acc 0.453125\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:11:27.352100: step 300, loss 1.23946, acc 0.265683\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-300\n",
      "\n",
      "2019-03-27T13:11:27.515458: step 301, loss 1.34942, acc 0.46875\n",
      "2019-03-27T13:11:27.571914: step 302, loss 1.42974, acc 0.390625\n",
      "2019-03-27T13:11:27.631238: step 303, loss 1.44494, acc 0.34375\n",
      "2019-03-27T13:11:27.688261: step 304, loss 1.47247, acc 0.359375\n",
      "2019-03-27T13:11:27.747215: step 305, loss 1.42442, acc 0.484375\n",
      "2019-03-27T13:11:27.805459: step 306, loss 1.16077, acc 0.5\n",
      "2019-03-27T13:11:27.864372: step 307, loss 1.23398, acc 0.4375\n",
      "2019-03-27T13:11:27.915318: step 308, loss 1.34691, acc 0.52381\n",
      "2019-03-27T13:11:27.977859: step 309, loss 0.993958, acc 0.578125\n",
      "2019-03-27T13:11:28.036605: step 310, loss 0.975013, acc 0.625\n",
      "2019-03-27T13:11:28.094266: step 311, loss 1.42589, acc 0.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:11:28.151595: step 312, loss 1.25768, acc 0.515625\n",
      "2019-03-27T13:11:28.207913: step 313, loss 1.15934, acc 0.484375\n",
      "2019-03-27T13:11:28.265731: step 314, loss 1.07788, acc 0.515625\n",
      "2019-03-27T13:11:28.324777: step 315, loss 1.51771, acc 0.40625\n",
      "2019-03-27T13:11:28.385499: step 316, loss 1.2348, acc 0.421875\n",
      "2019-03-27T13:11:28.442183: step 317, loss 1.13271, acc 0.5\n",
      "2019-03-27T13:11:28.502862: step 318, loss 1.39071, acc 0.40625\n",
      "2019-03-27T13:11:28.562599: step 319, loss 1.19137, acc 0.484375\n",
      "2019-03-27T13:11:28.621094: step 320, loss 1.0365, acc 0.53125\n",
      "2019-03-27T13:11:28.681114: step 321, loss 1.60962, acc 0.359375\n",
      "2019-03-27T13:11:28.740282: step 322, loss 1.42408, acc 0.4375\n",
      "2019-03-27T13:11:28.801903: step 323, loss 1.13904, acc 0.53125\n",
      "2019-03-27T13:11:28.860166: step 324, loss 1.12721, acc 0.453125\n",
      "2019-03-27T13:11:28.918820: step 325, loss 1.57205, acc 0.40625\n",
      "2019-03-27T13:11:28.977298: step 326, loss 1.28772, acc 0.53125\n",
      "2019-03-27T13:11:29.035648: step 327, loss 1.36362, acc 0.484375\n",
      "2019-03-27T13:11:29.093008: step 328, loss 1.45443, acc 0.390625\n",
      "2019-03-27T13:11:29.149704: step 329, loss 1.32138, acc 0.40625\n",
      "2019-03-27T13:11:29.209455: step 330, loss 1.28915, acc 0.484375\n",
      "2019-03-27T13:11:29.267272: step 331, loss 1.26453, acc 0.5\n",
      "2019-03-27T13:11:29.325022: step 332, loss 1.37832, acc 0.4375\n",
      "2019-03-27T13:11:29.383493: step 333, loss 1.21662, acc 0.515625\n",
      "2019-03-27T13:11:29.439262: step 334, loss 1.24819, acc 0.4375\n",
      "2019-03-27T13:11:29.496016: step 335, loss 1.35756, acc 0.453125\n",
      "2019-03-27T13:11:29.553636: step 336, loss 1.3942, acc 0.34375\n",
      "2019-03-27T13:11:29.612467: step 337, loss 1.39427, acc 0.4375\n",
      "2019-03-27T13:11:29.669282: step 338, loss 1.14342, acc 0.4375\n",
      "2019-03-27T13:11:29.724533: step 339, loss 1.21304, acc 0.515625\n",
      "2019-03-27T13:11:29.782733: step 340, loss 1.07417, acc 0.53125\n",
      "2019-03-27T13:11:29.841222: step 341, loss 1.33707, acc 0.390625\n",
      "2019-03-27T13:11:29.900911: step 342, loss 1.24706, acc 0.4375\n",
      "2019-03-27T13:11:29.962153: step 343, loss 1.47621, acc 0.375\n",
      "2019-03-27T13:11:30.020375: step 344, loss 1.26626, acc 0.4375\n",
      "2019-03-27T13:11:30.077339: step 345, loss 1.0609, acc 0.453125\n",
      "2019-03-27T13:11:30.134550: step 346, loss 0.895761, acc 0.625\n",
      "2019-03-27T13:11:30.190966: step 347, loss 1.04788, acc 0.53125\n",
      "2019-03-27T13:11:30.249701: step 348, loss 1.07751, acc 0.5625\n",
      "2019-03-27T13:11:30.311724: step 349, loss 1.24592, acc 0.53125\n",
      "2019-03-27T13:11:30.369959: step 350, loss 1.10792, acc 0.53125\n",
      "2019-03-27T13:11:30.430570: step 351, loss 1.38568, acc 0.4375\n",
      "2019-03-27T13:11:30.488876: step 352, loss 1.25699, acc 0.5\n",
      "2019-03-27T13:11:30.548365: step 353, loss 1.0391, acc 0.546875\n",
      "2019-03-27T13:11:30.607956: step 354, loss 1.3892, acc 0.34375\n",
      "2019-03-27T13:11:30.668391: step 355, loss 1.10743, acc 0.484375\n",
      "2019-03-27T13:11:30.727672: step 356, loss 1.27113, acc 0.421875\n",
      "2019-03-27T13:11:30.782950: step 357, loss 1.38921, acc 0.4375\n",
      "2019-03-27T13:11:30.840433: step 358, loss 1.32964, acc 0.484375\n",
      "2019-03-27T13:11:30.897223: step 359, loss 1.54379, acc 0.296875\n",
      "2019-03-27T13:11:30.956512: step 360, loss 1.33677, acc 0.34375\n",
      "2019-03-27T13:11:31.009797: step 361, loss 1.33199, acc 0.3125\n",
      "2019-03-27T13:11:31.068891: step 362, loss 1.19329, acc 0.5\n",
      "2019-03-27T13:11:31.128406: step 363, loss 1.12977, acc 0.5625\n",
      "2019-03-27T13:11:31.186596: step 364, loss 0.862677, acc 0.609375\n",
      "2019-03-27T13:11:31.243242: step 365, loss 1.34633, acc 0.390625\n",
      "2019-03-27T13:11:31.298920: step 366, loss 1.25234, acc 0.484375\n",
      "2019-03-27T13:11:31.356552: step 367, loss 1.43159, acc 0.453125\n",
      "2019-03-27T13:11:31.416529: step 368, loss 1.21566, acc 0.484375\n",
      "2019-03-27T13:11:31.476745: step 369, loss 0.943769, acc 0.59375\n",
      "2019-03-27T13:11:31.535522: step 370, loss 1.21363, acc 0.390625\n",
      "2019-03-27T13:11:31.592410: step 371, loss 1.0754, acc 0.515625\n",
      "2019-03-27T13:11:31.651561: step 372, loss 1.07972, acc 0.515625\n",
      "2019-03-27T13:11:31.713493: step 373, loss 1.07304, acc 0.546875\n",
      "2019-03-27T13:11:31.772905: step 374, loss 1.31474, acc 0.4375\n",
      "2019-03-27T13:11:31.829586: step 375, loss 1.34248, acc 0.359375\n",
      "2019-03-27T13:11:31.884942: step 376, loss 1.065, acc 0.5\n",
      "2019-03-27T13:11:31.944160: step 377, loss 1.45486, acc 0.375\n",
      "2019-03-27T13:11:32.000916: step 378, loss 1.22004, acc 0.484375\n",
      "2019-03-27T13:11:32.059732: step 379, loss 1.29528, acc 0.40625\n",
      "2019-03-27T13:11:32.121639: step 380, loss 1.24091, acc 0.40625\n",
      "2019-03-27T13:11:32.179201: step 381, loss 1.02452, acc 0.46875\n",
      "2019-03-27T13:11:32.240975: step 382, loss 1.19396, acc 0.46875\n",
      "2019-03-27T13:11:32.301577: step 383, loss 0.99882, acc 0.484375\n",
      "2019-03-27T13:11:32.357850: step 384, loss 1.14637, acc 0.390625\n",
      "2019-03-27T13:11:32.402566: step 385, loss 1.28935, acc 0.380952\n",
      "2019-03-27T13:11:32.463086: step 386, loss 1.12832, acc 0.453125\n",
      "2019-03-27T13:11:32.521944: step 387, loss 1.1758, acc 0.46875\n",
      "2019-03-27T13:11:32.581849: step 388, loss 1.1601, acc 0.4375\n",
      "2019-03-27T13:11:32.639674: step 389, loss 0.978422, acc 0.53125\n",
      "2019-03-27T13:11:32.700202: step 390, loss 0.992147, acc 0.578125\n",
      "2019-03-27T13:11:32.759336: step 391, loss 1.05098, acc 0.53125\n",
      "2019-03-27T13:11:32.817049: step 392, loss 1.13168, acc 0.453125\n",
      "2019-03-27T13:11:32.873653: step 393, loss 1.14429, acc 0.5\n",
      "2019-03-27T13:11:32.932299: step 394, loss 1.25175, acc 0.375\n",
      "2019-03-27T13:11:32.990174: step 395, loss 1.11997, acc 0.453125\n",
      "2019-03-27T13:11:33.051998: step 396, loss 1.1945, acc 0.5\n",
      "2019-03-27T13:11:33.108270: step 397, loss 1.16562, acc 0.546875\n",
      "2019-03-27T13:11:33.164778: step 398, loss 1.0652, acc 0.5\n",
      "2019-03-27T13:11:33.224749: step 399, loss 0.963418, acc 0.546875\n",
      "2019-03-27T13:11:33.283887: step 400, loss 1.12098, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:11:33.314667: step 400, loss 1.1416, acc 0.359779\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-400\n",
      "\n",
      "2019-03-27T13:11:33.480653: step 401, loss 1.21537, acc 0.390625\n",
      "2019-03-27T13:11:33.539639: step 402, loss 1.0228, acc 0.5\n",
      "2019-03-27T13:11:33.596463: step 403, loss 0.993109, acc 0.515625\n",
      "2019-03-27T13:11:33.654545: step 404, loss 1.02089, acc 0.421875\n",
      "2019-03-27T13:11:33.712119: step 405, loss 1.03863, acc 0.421875\n",
      "2019-03-27T13:11:33.767965: step 406, loss 1.02267, acc 0.4375\n",
      "2019-03-27T13:11:33.826526: step 407, loss 1.18824, acc 0.390625\n",
      "2019-03-27T13:11:33.884372: step 408, loss 1.05113, acc 0.5625\n",
      "2019-03-27T13:11:33.944615: step 409, loss 1.13199, acc 0.40625\n",
      "2019-03-27T13:11:34.003673: step 410, loss 0.985464, acc 0.578125\n",
      "2019-03-27T13:11:34.065233: step 411, loss 0.918733, acc 0.59375\n",
      "2019-03-27T13:11:34.123689: step 412, loss 1.10689, acc 0.4375\n",
      "2019-03-27T13:11:34.181896: step 413, loss 1.16935, acc 0.46875\n",
      "2019-03-27T13:11:34.243167: step 414, loss 0.850395, acc 0.609375\n",
      "2019-03-27T13:11:34.302432: step 415, loss 1.18662, acc 0.421875\n",
      "2019-03-27T13:11:34.359751: step 416, loss 1.10896, acc 0.40625\n",
      "2019-03-27T13:11:34.415060: step 417, loss 1.01779, acc 0.5\n",
      "2019-03-27T13:11:34.470711: step 418, loss 1.20584, acc 0.484375\n",
      "2019-03-27T13:11:34.527898: step 419, loss 1.10947, acc 0.484375\n",
      "2019-03-27T13:11:34.586935: step 420, loss 1.09564, acc 0.515625\n",
      "2019-03-27T13:11:34.645929: step 421, loss 0.934638, acc 0.546875\n",
      "2019-03-27T13:11:34.704783: step 422, loss 1.07877, acc 0.453125\n",
      "2019-03-27T13:11:34.763890: step 423, loss 1.09459, acc 0.46875\n",
      "2019-03-27T13:11:34.820122: step 424, loss 1.09048, acc 0.484375\n",
      "2019-03-27T13:11:34.878418: step 425, loss 1.07951, acc 0.453125\n",
      "2019-03-27T13:11:34.937321: step 426, loss 1.21882, acc 0.46875\n",
      "2019-03-27T13:11:34.995078: step 427, loss 1.22852, acc 0.46875\n",
      "2019-03-27T13:11:35.052137: step 428, loss 1.16268, acc 0.421875\n",
      "2019-03-27T13:11:35.113210: step 429, loss 0.873569, acc 0.625\n",
      "2019-03-27T13:11:35.172224: step 430, loss 1.13867, acc 0.5\n",
      "2019-03-27T13:11:35.228023: step 431, loss 1.03755, acc 0.53125\n",
      "2019-03-27T13:11:35.288338: step 432, loss 1.19723, acc 0.46875\n",
      "2019-03-27T13:11:35.348483: step 433, loss 1.20515, acc 0.5\n",
      "2019-03-27T13:11:35.409738: step 434, loss 1.11627, acc 0.53125\n",
      "2019-03-27T13:11:35.467586: step 435, loss 1.05535, acc 0.546875\n",
      "2019-03-27T13:11:35.524937: step 436, loss 0.945666, acc 0.546875\n",
      "2019-03-27T13:11:35.580583: step 437, loss 1.05896, acc 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:11:35.638665: step 438, loss 1.13829, acc 0.390625\n",
      "2019-03-27T13:11:35.695390: step 439, loss 1.15968, acc 0.46875\n",
      "2019-03-27T13:11:35.753736: step 440, loss 1.04072, acc 0.5625\n",
      "2019-03-27T13:11:35.812930: step 441, loss 1.22605, acc 0.4375\n",
      "2019-03-27T13:11:35.871019: step 442, loss 0.977561, acc 0.5\n",
      "2019-03-27T13:11:35.934591: step 443, loss 0.858277, acc 0.609375\n",
      "2019-03-27T13:11:35.995876: step 444, loss 1.18559, acc 0.359375\n",
      "2019-03-27T13:11:36.054763: step 445, loss 1.13796, acc 0.484375\n",
      "2019-03-27T13:11:36.115401: step 446, loss 1.0363, acc 0.515625\n",
      "2019-03-27T13:11:36.178766: step 447, loss 1.11137, acc 0.453125\n",
      "2019-03-27T13:11:36.235942: step 448, loss 1.06123, acc 0.4375\n",
      "2019-03-27T13:11:36.296369: step 449, loss 1.17188, acc 0.453125\n",
      "2019-03-27T13:11:36.356399: step 450, loss 1.09009, acc 0.515625\n",
      "2019-03-27T13:11:36.415117: step 451, loss 0.987285, acc 0.546875\n",
      "2019-03-27T13:11:36.473584: step 452, loss 1.15658, acc 0.5\n",
      "2019-03-27T13:11:36.532111: step 453, loss 1.0121, acc 0.5\n",
      "2019-03-27T13:11:36.590822: step 454, loss 0.967306, acc 0.515625\n",
      "2019-03-27T13:11:36.648140: step 455, loss 1.05854, acc 0.53125\n",
      "2019-03-27T13:11:36.706332: step 456, loss 1.23537, acc 0.375\n",
      "2019-03-27T13:11:36.766674: step 457, loss 1.08103, acc 0.484375\n",
      "2019-03-27T13:11:36.824333: step 458, loss 0.964585, acc 0.484375\n",
      "2019-03-27T13:11:36.879050: step 459, loss 1.02535, acc 0.53125\n",
      "2019-03-27T13:11:36.935797: step 460, loss 1.13625, acc 0.4375\n",
      "2019-03-27T13:11:36.992753: step 461, loss 1.1174, acc 0.515625\n",
      "2019-03-27T13:11:37.042137: step 462, loss 1.28198, acc 0.333333\n",
      "2019-03-27T13:11:37.102750: step 463, loss 0.93077, acc 0.59375\n",
      "2019-03-27T13:11:37.161335: step 464, loss 1.02316, acc 0.484375\n",
      "2019-03-27T13:11:37.219253: step 465, loss 0.915713, acc 0.46875\n",
      "2019-03-27T13:11:37.279224: step 466, loss 1.0535, acc 0.5\n",
      "2019-03-27T13:11:37.335630: step 467, loss 0.97739, acc 0.546875\n",
      "2019-03-27T13:11:37.395466: step 468, loss 0.894337, acc 0.578125\n",
      "2019-03-27T13:11:37.452350: step 469, loss 1.175, acc 0.390625\n",
      "2019-03-27T13:11:37.507948: step 470, loss 0.862444, acc 0.640625\n",
      "2019-03-27T13:11:37.565624: step 471, loss 0.876298, acc 0.578125\n",
      "2019-03-27T13:11:37.623578: step 472, loss 1.0234, acc 0.578125\n",
      "2019-03-27T13:11:37.680958: step 473, loss 1.03705, acc 0.421875\n",
      "2019-03-27T13:11:37.741858: step 474, loss 0.890757, acc 0.578125\n",
      "2019-03-27T13:11:37.799895: step 475, loss 1.00614, acc 0.53125\n",
      "2019-03-27T13:11:37.858283: step 476, loss 0.943159, acc 0.578125\n",
      "2019-03-27T13:11:37.917818: step 477, loss 1.02706, acc 0.5625\n",
      "2019-03-27T13:11:37.974918: step 478, loss 0.923885, acc 0.53125\n",
      "2019-03-27T13:11:38.032619: step 479, loss 0.918171, acc 0.515625\n",
      "2019-03-27T13:11:38.092082: step 480, loss 0.912207, acc 0.53125\n",
      "2019-03-27T13:11:38.150119: step 481, loss 1.06692, acc 0.53125\n",
      "2019-03-27T13:11:38.212043: step 482, loss 0.919572, acc 0.609375\n",
      "2019-03-27T13:11:38.268436: step 483, loss 1.11536, acc 0.5\n",
      "2019-03-27T13:11:38.324836: step 484, loss 0.851282, acc 0.59375\n",
      "2019-03-27T13:11:38.381379: step 485, loss 0.977639, acc 0.5\n",
      "2019-03-27T13:11:38.439564: step 486, loss 0.858263, acc 0.578125\n",
      "2019-03-27T13:11:38.499440: step 487, loss 0.976075, acc 0.53125\n",
      "2019-03-27T13:11:38.560791: step 488, loss 0.995494, acc 0.515625\n",
      "2019-03-27T13:11:38.618002: step 489, loss 1.10354, acc 0.484375\n",
      "2019-03-27T13:11:38.675817: step 490, loss 1.03354, acc 0.484375\n",
      "2019-03-27T13:11:38.738877: step 491, loss 1.13263, acc 0.40625\n",
      "2019-03-27T13:11:38.800659: step 492, loss 0.992874, acc 0.53125\n",
      "2019-03-27T13:11:38.860941: step 493, loss 0.924305, acc 0.515625\n",
      "2019-03-27T13:11:38.921613: step 494, loss 0.962441, acc 0.484375\n",
      "2019-03-27T13:11:38.980918: step 495, loss 0.721684, acc 0.75\n",
      "2019-03-27T13:11:39.039126: step 496, loss 0.945302, acc 0.5625\n",
      "2019-03-27T13:11:39.099540: step 497, loss 0.991125, acc 0.5\n",
      "2019-03-27T13:11:39.159224: step 498, loss 1.02901, acc 0.5\n",
      "2019-03-27T13:11:39.220500: step 499, loss 0.911891, acc 0.59375\n",
      "2019-03-27T13:11:39.282338: step 500, loss 0.974236, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:11:39.317310: step 500, loss 1.08771, acc 0.302583\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-500\n",
      "\n",
      "2019-03-27T13:11:39.477708: step 501, loss 0.988751, acc 0.453125\n",
      "2019-03-27T13:11:39.537794: step 502, loss 1.00363, acc 0.484375\n",
      "2019-03-27T13:11:39.597469: step 503, loss 0.869031, acc 0.609375\n",
      "2019-03-27T13:11:39.657160: step 504, loss 0.939411, acc 0.5625\n",
      "2019-03-27T13:11:39.716791: step 505, loss 0.933147, acc 0.53125\n",
      "2019-03-27T13:11:39.777725: step 506, loss 1.06971, acc 0.40625\n",
      "2019-03-27T13:11:39.837628: step 507, loss 1.04685, acc 0.40625\n",
      "2019-03-27T13:11:39.894572: step 508, loss 0.877928, acc 0.59375\n",
      "2019-03-27T13:11:39.955211: step 509, loss 1.08591, acc 0.484375\n",
      "2019-03-27T13:11:40.012633: step 510, loss 0.929592, acc 0.515625\n",
      "2019-03-27T13:11:40.072846: step 511, loss 1.03391, acc 0.515625\n",
      "2019-03-27T13:11:40.134235: step 512, loss 0.921801, acc 0.515625\n",
      "2019-03-27T13:11:40.194505: step 513, loss 1.11863, acc 0.5\n",
      "2019-03-27T13:11:40.253701: step 514, loss 1.0792, acc 0.4375\n",
      "2019-03-27T13:11:40.318535: step 515, loss 0.999888, acc 0.5\n",
      "2019-03-27T13:11:40.375598: step 516, loss 1.03724, acc 0.5\n",
      "2019-03-27T13:11:40.436220: step 517, loss 1.00067, acc 0.546875\n",
      "2019-03-27T13:11:40.497583: step 518, loss 0.977033, acc 0.546875\n",
      "2019-03-27T13:11:40.556437: step 519, loss 1.06866, acc 0.453125\n",
      "2019-03-27T13:11:40.615488: step 520, loss 0.960562, acc 0.515625\n",
      "2019-03-27T13:11:40.674181: step 521, loss 1.08264, acc 0.421875\n",
      "2019-03-27T13:11:40.733074: step 522, loss 0.975641, acc 0.46875\n",
      "2019-03-27T13:11:40.791620: step 523, loss 1.0115, acc 0.484375\n",
      "2019-03-27T13:11:40.852283: step 524, loss 0.933496, acc 0.5625\n",
      "2019-03-27T13:11:40.910596: step 525, loss 1.05917, acc 0.46875\n",
      "2019-03-27T13:11:40.970856: step 526, loss 0.944252, acc 0.53125\n",
      "2019-03-27T13:11:41.028249: step 527, loss 0.931926, acc 0.46875\n",
      "2019-03-27T13:11:41.089548: step 528, loss 1.0365, acc 0.46875\n",
      "2019-03-27T13:11:41.148877: step 529, loss 0.987114, acc 0.5625\n",
      "2019-03-27T13:11:41.208895: step 530, loss 0.953066, acc 0.546875\n",
      "2019-03-27T13:11:41.267004: step 531, loss 1.28048, acc 0.3125\n",
      "2019-03-27T13:11:41.326935: step 532, loss 1.0591, acc 0.46875\n",
      "2019-03-27T13:11:41.384404: step 533, loss 1.0169, acc 0.484375\n",
      "2019-03-27T13:11:41.447296: step 534, loss 1.05009, acc 0.46875\n",
      "2019-03-27T13:11:41.507915: step 535, loss 1.02476, acc 0.53125\n",
      "2019-03-27T13:11:41.566611: step 536, loss 1.13126, acc 0.453125\n",
      "2019-03-27T13:11:41.625427: step 537, loss 0.980135, acc 0.53125\n",
      "2019-03-27T13:11:41.685763: step 538, loss 0.89848, acc 0.59375\n",
      "2019-03-27T13:11:41.732539: step 539, loss 0.837627, acc 0.619048\n",
      "2019-03-27T13:11:41.796150: step 540, loss 1.03343, acc 0.515625\n",
      "2019-03-27T13:11:41.858526: step 541, loss 0.940844, acc 0.453125\n",
      "2019-03-27T13:11:41.917215: step 542, loss 0.94934, acc 0.46875\n",
      "2019-03-27T13:11:41.975991: step 543, loss 0.992011, acc 0.5\n",
      "2019-03-27T13:11:42.035276: step 544, loss 0.887168, acc 0.484375\n",
      "2019-03-27T13:11:42.098147: step 545, loss 1.06625, acc 0.546875\n",
      "2019-03-27T13:11:42.162156: step 546, loss 0.983657, acc 0.53125\n",
      "2019-03-27T13:11:42.224463: step 547, loss 0.761721, acc 0.671875\n",
      "2019-03-27T13:11:42.284038: step 548, loss 0.939754, acc 0.5625\n",
      "2019-03-27T13:11:42.344354: step 549, loss 0.915594, acc 0.65625\n",
      "2019-03-27T13:11:42.406683: step 550, loss 0.864295, acc 0.578125\n",
      "2019-03-27T13:11:42.466062: step 551, loss 0.779583, acc 0.578125\n",
      "2019-03-27T13:11:42.523182: step 552, loss 1.05286, acc 0.421875\n",
      "2019-03-27T13:11:42.583452: step 553, loss 0.792825, acc 0.671875\n",
      "2019-03-27T13:11:42.642062: step 554, loss 0.894107, acc 0.5625\n",
      "2019-03-27T13:11:42.703158: step 555, loss 0.816287, acc 0.546875\n",
      "2019-03-27T13:11:42.764679: step 556, loss 0.877992, acc 0.546875\n",
      "2019-03-27T13:11:42.824044: step 557, loss 0.978702, acc 0.40625\n",
      "2019-03-27T13:11:42.882742: step 558, loss 1.11001, acc 0.421875\n",
      "2019-03-27T13:11:42.943072: step 559, loss 0.828477, acc 0.59375\n",
      "2019-03-27T13:11:43.002775: step 560, loss 0.911294, acc 0.53125\n",
      "2019-03-27T13:11:43.062971: step 561, loss 0.96044, acc 0.546875\n",
      "2019-03-27T13:11:43.124030: step 562, loss 1.01362, acc 0.546875\n",
      "2019-03-27T13:11:43.186396: step 563, loss 1.07185, acc 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:11:43.247271: step 564, loss 0.851013, acc 0.546875\n",
      "2019-03-27T13:11:43.306340: step 565, loss 1.01195, acc 0.546875\n",
      "2019-03-27T13:11:43.367336: step 566, loss 0.926143, acc 0.5\n",
      "2019-03-27T13:11:43.426010: step 567, loss 0.904123, acc 0.546875\n",
      "2019-03-27T13:11:43.487871: step 568, loss 0.907965, acc 0.5625\n",
      "2019-03-27T13:11:43.548574: step 569, loss 0.851411, acc 0.59375\n",
      "2019-03-27T13:11:43.608343: step 570, loss 1.00957, acc 0.5\n",
      "2019-03-27T13:11:43.670406: step 571, loss 1.0052, acc 0.515625\n",
      "2019-03-27T13:11:43.731252: step 572, loss 0.964979, acc 0.484375\n",
      "2019-03-27T13:11:43.792122: step 573, loss 1.06166, acc 0.375\n",
      "2019-03-27T13:11:43.853043: step 574, loss 0.881958, acc 0.515625\n",
      "2019-03-27T13:11:43.914105: step 575, loss 0.946479, acc 0.5\n",
      "2019-03-27T13:11:43.974694: step 576, loss 0.831117, acc 0.59375\n",
      "2019-03-27T13:11:44.035672: step 577, loss 0.836422, acc 0.609375\n",
      "2019-03-27T13:11:44.097893: step 578, loss 0.999215, acc 0.53125\n",
      "2019-03-27T13:11:44.157707: step 579, loss 1.01511, acc 0.515625\n",
      "2019-03-27T13:11:44.218143: step 580, loss 0.902686, acc 0.609375\n",
      "2019-03-27T13:11:44.277257: step 581, loss 0.916284, acc 0.53125\n",
      "2019-03-27T13:11:44.335830: step 582, loss 1.21061, acc 0.421875\n",
      "2019-03-27T13:11:44.396421: step 583, loss 0.93336, acc 0.53125\n",
      "2019-03-27T13:11:44.459024: step 584, loss 0.917538, acc 0.546875\n",
      "2019-03-27T13:11:44.520880: step 585, loss 0.914957, acc 0.53125\n",
      "2019-03-27T13:11:44.583197: step 586, loss 1.0558, acc 0.46875\n",
      "2019-03-27T13:11:44.644556: step 587, loss 0.889387, acc 0.5625\n",
      "2019-03-27T13:11:44.705743: step 588, loss 1.01751, acc 0.46875\n",
      "2019-03-27T13:11:44.766999: step 589, loss 0.823038, acc 0.578125\n",
      "2019-03-27T13:11:44.828982: step 590, loss 0.929024, acc 0.5\n",
      "2019-03-27T13:11:44.888938: step 591, loss 0.927588, acc 0.5\n",
      "2019-03-27T13:11:44.950911: step 592, loss 0.915016, acc 0.609375\n",
      "2019-03-27T13:11:45.009017: step 593, loss 0.927443, acc 0.5\n",
      "2019-03-27T13:11:45.069169: step 594, loss 0.895195, acc 0.453125\n",
      "2019-03-27T13:11:45.139381: step 595, loss 0.799813, acc 0.578125\n",
      "2019-03-27T13:11:45.200551: step 596, loss 0.783236, acc 0.546875\n",
      "2019-03-27T13:11:45.261213: step 597, loss 0.919642, acc 0.53125\n",
      "2019-03-27T13:11:45.326002: step 598, loss 0.859468, acc 0.5625\n",
      "2019-03-27T13:11:45.384563: step 599, loss 0.910054, acc 0.453125\n",
      "2019-03-27T13:11:45.442168: step 600, loss 0.895784, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:11:45.475371: step 600, loss 1.03672, acc 0.409594\n",
      "\n",
      "WARNING:tensorflow:From /home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-600\n",
      "\n",
      "2019-03-27T13:11:45.650169: step 601, loss 0.853635, acc 0.578125\n",
      "2019-03-27T13:11:45.709325: step 602, loss 0.848402, acc 0.578125\n",
      "2019-03-27T13:11:45.768272: step 603, loss 0.973771, acc 0.5\n",
      "2019-03-27T13:11:45.827261: step 604, loss 0.89248, acc 0.53125\n",
      "2019-03-27T13:11:45.889907: step 605, loss 0.950592, acc 0.53125\n",
      "2019-03-27T13:11:45.948837: step 606, loss 0.907026, acc 0.578125\n",
      "2019-03-27T13:11:46.010062: step 607, loss 1.00604, acc 0.46875\n",
      "2019-03-27T13:11:46.070591: step 608, loss 0.827557, acc 0.578125\n",
      "2019-03-27T13:11:46.132033: step 609, loss 0.910351, acc 0.5\n",
      "2019-03-27T13:11:46.192039: step 610, loss 0.909457, acc 0.484375\n",
      "2019-03-27T13:11:46.251803: step 611, loss 1.01107, acc 0.4375\n",
      "2019-03-27T13:11:46.312532: step 612, loss 1.09519, acc 0.453125\n",
      "2019-03-27T13:11:46.371917: step 613, loss 1.11627, acc 0.5\n",
      "2019-03-27T13:11:46.431044: step 614, loss 0.956784, acc 0.578125\n",
      "2019-03-27T13:11:46.488012: step 615, loss 0.981556, acc 0.53125\n",
      "2019-03-27T13:11:46.539113: step 616, loss 0.755493, acc 0.714286\n",
      "2019-03-27T13:11:46.603217: step 617, loss 0.783973, acc 0.59375\n",
      "2019-03-27T13:11:46.662763: step 618, loss 0.986149, acc 0.5\n",
      "2019-03-27T13:11:46.724531: step 619, loss 0.988692, acc 0.46875\n",
      "2019-03-27T13:11:46.784953: step 620, loss 1.02802, acc 0.484375\n",
      "2019-03-27T13:11:46.843545: step 621, loss 0.888066, acc 0.59375\n",
      "2019-03-27T13:11:46.902469: step 622, loss 0.866201, acc 0.5625\n",
      "2019-03-27T13:11:46.964011: step 623, loss 0.90407, acc 0.46875\n",
      "2019-03-27T13:11:47.027124: step 624, loss 0.822149, acc 0.640625\n",
      "2019-03-27T13:11:47.087733: step 625, loss 0.83521, acc 0.5\n",
      "2019-03-27T13:11:47.149908: step 626, loss 0.970745, acc 0.578125\n",
      "2019-03-27T13:11:47.210987: step 627, loss 0.91892, acc 0.5625\n",
      "2019-03-27T13:11:47.271877: step 628, loss 0.863598, acc 0.515625\n",
      "2019-03-27T13:11:47.330422: step 629, loss 0.892575, acc 0.515625\n",
      "2019-03-27T13:11:47.390736: step 630, loss 0.934854, acc 0.59375\n",
      "2019-03-27T13:11:47.453680: step 631, loss 0.836579, acc 0.625\n",
      "2019-03-27T13:11:47.514187: step 632, loss 0.839974, acc 0.53125\n",
      "2019-03-27T13:11:47.572308: step 633, loss 0.842827, acc 0.609375\n",
      "2019-03-27T13:11:47.632320: step 634, loss 0.80464, acc 0.625\n",
      "2019-03-27T13:11:47.692845: step 635, loss 1.019, acc 0.46875\n",
      "2019-03-27T13:11:47.753383: step 636, loss 0.742594, acc 0.59375\n",
      "2019-03-27T13:11:47.814238: step 637, loss 0.820658, acc 0.5625\n",
      "2019-03-27T13:11:47.873891: step 638, loss 0.864905, acc 0.625\n",
      "2019-03-27T13:11:47.933860: step 639, loss 0.84269, acc 0.515625\n",
      "2019-03-27T13:11:47.994854: step 640, loss 0.961416, acc 0.5\n",
      "2019-03-27T13:11:48.054397: step 641, loss 0.965564, acc 0.375\n",
      "2019-03-27T13:11:48.115763: step 642, loss 0.866013, acc 0.515625\n",
      "2019-03-27T13:11:48.177085: step 643, loss 0.901005, acc 0.53125\n",
      "2019-03-27T13:11:48.240805: step 644, loss 0.98347, acc 0.484375\n",
      "2019-03-27T13:11:48.301955: step 645, loss 0.931897, acc 0.453125\n",
      "2019-03-27T13:11:48.362307: step 646, loss 0.848557, acc 0.5625\n",
      "2019-03-27T13:11:48.424114: step 647, loss 0.910871, acc 0.546875\n",
      "2019-03-27T13:11:48.486355: step 648, loss 0.841916, acc 0.578125\n",
      "2019-03-27T13:11:48.549760: step 649, loss 0.998825, acc 0.515625\n",
      "2019-03-27T13:11:48.612124: step 650, loss 0.901411, acc 0.5\n",
      "2019-03-27T13:11:48.672507: step 651, loss 0.870796, acc 0.515625\n",
      "2019-03-27T13:11:48.729836: step 652, loss 0.854839, acc 0.546875\n",
      "2019-03-27T13:11:48.790553: step 653, loss 0.975841, acc 0.5\n",
      "2019-03-27T13:11:48.849338: step 654, loss 0.906848, acc 0.484375\n",
      "2019-03-27T13:11:48.910173: step 655, loss 0.842101, acc 0.546875\n",
      "2019-03-27T13:11:48.970409: step 656, loss 0.884109, acc 0.578125\n",
      "2019-03-27T13:11:49.031242: step 657, loss 1.00753, acc 0.4375\n",
      "2019-03-27T13:11:49.092380: step 658, loss 0.856343, acc 0.46875\n",
      "2019-03-27T13:11:49.150554: step 659, loss 0.954009, acc 0.421875\n",
      "2019-03-27T13:11:49.210433: step 660, loss 0.813665, acc 0.546875\n",
      "2019-03-27T13:11:49.276871: step 661, loss 0.782203, acc 0.546875\n",
      "2019-03-27T13:11:49.338431: step 662, loss 0.790763, acc 0.625\n",
      "2019-03-27T13:11:49.396336: step 663, loss 0.860518, acc 0.625\n",
      "2019-03-27T13:11:49.453785: step 664, loss 0.910545, acc 0.546875\n",
      "2019-03-27T13:11:49.513934: step 665, loss 0.996481, acc 0.53125\n",
      "2019-03-27T13:11:49.572411: step 666, loss 0.914732, acc 0.484375\n",
      "2019-03-27T13:11:49.633712: step 667, loss 0.76092, acc 0.546875\n",
      "2019-03-27T13:11:49.693498: step 668, loss 0.780083, acc 0.609375\n",
      "2019-03-27T13:11:49.757718: step 669, loss 0.887451, acc 0.484375\n",
      "2019-03-27T13:11:49.818656: step 670, loss 0.755353, acc 0.625\n",
      "2019-03-27T13:11:49.878524: step 671, loss 0.93248, acc 0.53125\n",
      "2019-03-27T13:11:49.938586: step 672, loss 0.891254, acc 0.46875\n",
      "2019-03-27T13:11:49.997528: step 673, loss 0.851633, acc 0.59375\n",
      "2019-03-27T13:11:50.060413: step 674, loss 0.940433, acc 0.59375\n",
      "2019-03-27T13:11:50.118054: step 675, loss 0.968157, acc 0.453125\n",
      "2019-03-27T13:11:50.177330: step 676, loss 0.753225, acc 0.546875\n",
      "2019-03-27T13:11:50.240278: step 677, loss 0.896778, acc 0.46875\n",
      "2019-03-27T13:11:50.301649: step 678, loss 0.97255, acc 0.5\n",
      "2019-03-27T13:11:50.362059: step 679, loss 0.887725, acc 0.546875\n",
      "2019-03-27T13:11:50.422863: step 680, loss 0.975112, acc 0.5\n",
      "2019-03-27T13:11:50.480910: step 681, loss 0.99696, acc 0.53125\n",
      "2019-03-27T13:11:50.543022: step 682, loss 0.943196, acc 0.5\n",
      "2019-03-27T13:11:50.603120: step 683, loss 0.837834, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:11:50.662666: step 684, loss 0.779953, acc 0.640625\n",
      "2019-03-27T13:11:50.720959: step 685, loss 0.867657, acc 0.5\n",
      "2019-03-27T13:11:50.781177: step 686, loss 0.918863, acc 0.53125\n",
      "2019-03-27T13:11:50.840681: step 687, loss 0.902169, acc 0.546875\n",
      "2019-03-27T13:11:50.899623: step 688, loss 0.980544, acc 0.53125\n",
      "2019-03-27T13:11:50.958877: step 689, loss 1.15522, acc 0.4375\n",
      "2019-03-27T13:11:51.020366: step 690, loss 0.747317, acc 0.640625\n",
      "2019-03-27T13:11:51.078670: step 691, loss 0.774791, acc 0.625\n",
      "2019-03-27T13:11:51.136409: step 692, loss 0.829848, acc 0.546875\n",
      "2019-03-27T13:11:51.184764: step 693, loss 0.916567, acc 0.571429\n",
      "2019-03-27T13:11:51.243576: step 694, loss 0.827072, acc 0.53125\n",
      "2019-03-27T13:11:51.306485: step 695, loss 0.785069, acc 0.578125\n",
      "2019-03-27T13:11:51.367211: step 696, loss 0.804192, acc 0.515625\n",
      "2019-03-27T13:11:51.429412: step 697, loss 0.942667, acc 0.5625\n",
      "2019-03-27T13:11:51.492784: step 698, loss 0.865096, acc 0.625\n",
      "2019-03-27T13:11:51.551481: step 699, loss 0.902459, acc 0.5\n",
      "2019-03-27T13:11:51.610530: step 700, loss 0.958944, acc 0.515625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:11:51.643608: step 700, loss 1.01334, acc 0.254613\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-700\n",
      "\n",
      "2019-03-27T13:11:51.811529: step 701, loss 0.823947, acc 0.546875\n",
      "2019-03-27T13:11:51.878318: step 702, loss 0.916668, acc 0.515625\n",
      "2019-03-27T13:11:51.938785: step 703, loss 0.86452, acc 0.546875\n",
      "2019-03-27T13:11:51.995556: step 704, loss 0.829256, acc 0.625\n",
      "2019-03-27T13:11:52.054734: step 705, loss 0.903552, acc 0.453125\n",
      "2019-03-27T13:11:52.113993: step 706, loss 0.924632, acc 0.46875\n",
      "2019-03-27T13:11:52.175081: step 707, loss 0.861013, acc 0.546875\n",
      "2019-03-27T13:11:52.237977: step 708, loss 0.71792, acc 0.671875\n",
      "2019-03-27T13:11:52.298204: step 709, loss 0.884875, acc 0.515625\n",
      "2019-03-27T13:11:52.357518: step 710, loss 0.789218, acc 0.53125\n",
      "2019-03-27T13:11:52.418650: step 711, loss 0.934955, acc 0.59375\n",
      "2019-03-27T13:11:52.479028: step 712, loss 0.814045, acc 0.578125\n",
      "2019-03-27T13:11:52.537614: step 713, loss 0.872033, acc 0.59375\n",
      "2019-03-27T13:11:52.596738: step 714, loss 0.952436, acc 0.5625\n",
      "2019-03-27T13:11:52.655453: step 715, loss 0.747047, acc 0.546875\n",
      "2019-03-27T13:11:52.713419: step 716, loss 0.893246, acc 0.5625\n",
      "2019-03-27T13:11:52.770901: step 717, loss 0.681569, acc 0.734375\n",
      "2019-03-27T13:11:52.829665: step 718, loss 0.902084, acc 0.484375\n",
      "2019-03-27T13:11:52.889493: step 719, loss 0.880739, acc 0.515625\n",
      "2019-03-27T13:11:52.948235: step 720, loss 0.878523, acc 0.484375\n",
      "2019-03-27T13:11:53.007704: step 721, loss 0.738696, acc 0.625\n",
      "2019-03-27T13:11:53.066710: step 722, loss 0.894224, acc 0.546875\n",
      "2019-03-27T13:11:53.127245: step 723, loss 0.859154, acc 0.4375\n",
      "2019-03-27T13:11:53.190949: step 724, loss 0.678647, acc 0.625\n",
      "2019-03-27T13:11:53.251326: step 725, loss 0.933785, acc 0.40625\n",
      "2019-03-27T13:11:53.312412: step 726, loss 0.858216, acc 0.5625\n",
      "2019-03-27T13:11:53.373383: step 727, loss 0.844911, acc 0.546875\n",
      "2019-03-27T13:11:53.431886: step 728, loss 0.809935, acc 0.609375\n",
      "2019-03-27T13:11:53.493782: step 729, loss 0.849079, acc 0.609375\n",
      "2019-03-27T13:11:53.555414: step 730, loss 0.764983, acc 0.546875\n",
      "2019-03-27T13:11:53.616483: step 731, loss 0.765812, acc 0.5625\n",
      "2019-03-27T13:11:53.678988: step 732, loss 0.851634, acc 0.53125\n",
      "2019-03-27T13:11:53.740691: step 733, loss 0.783403, acc 0.5625\n",
      "2019-03-27T13:11:53.801190: step 734, loss 0.833139, acc 0.515625\n",
      "2019-03-27T13:11:53.862148: step 735, loss 0.903797, acc 0.453125\n",
      "2019-03-27T13:11:53.920064: step 736, loss 0.723231, acc 0.734375\n",
      "2019-03-27T13:11:53.981128: step 737, loss 0.787121, acc 0.5625\n",
      "2019-03-27T13:11:54.045949: step 738, loss 0.850753, acc 0.546875\n",
      "2019-03-27T13:11:54.105315: step 739, loss 0.829393, acc 0.578125\n",
      "2019-03-27T13:11:54.168068: step 740, loss 0.773549, acc 0.609375\n",
      "2019-03-27T13:11:54.228531: step 741, loss 0.897036, acc 0.53125\n",
      "2019-03-27T13:11:54.288419: step 742, loss 0.946069, acc 0.53125\n",
      "2019-03-27T13:11:54.348690: step 743, loss 0.84403, acc 0.5625\n",
      "2019-03-27T13:11:54.404779: step 744, loss 0.799624, acc 0.578125\n",
      "2019-03-27T13:11:54.466194: step 745, loss 0.866277, acc 0.5625\n",
      "2019-03-27T13:11:54.528848: step 746, loss 0.826167, acc 0.5\n",
      "2019-03-27T13:11:54.591348: step 747, loss 0.79363, acc 0.59375\n",
      "2019-03-27T13:11:54.653294: step 748, loss 0.758218, acc 0.578125\n",
      "2019-03-27T13:11:54.712191: step 749, loss 0.92542, acc 0.546875\n",
      "2019-03-27T13:11:54.774231: step 750, loss 0.838775, acc 0.625\n",
      "2019-03-27T13:11:54.834257: step 751, loss 0.821409, acc 0.5\n",
      "2019-03-27T13:11:54.894434: step 752, loss 0.890777, acc 0.546875\n",
      "2019-03-27T13:11:54.955540: step 753, loss 0.863865, acc 0.5\n",
      "2019-03-27T13:11:55.014483: step 754, loss 0.703963, acc 0.65625\n",
      "2019-03-27T13:11:55.075277: step 755, loss 0.92275, acc 0.578125\n",
      "2019-03-27T13:11:55.137725: step 756, loss 0.904147, acc 0.53125\n",
      "2019-03-27T13:11:55.195482: step 757, loss 0.978433, acc 0.484375\n",
      "2019-03-27T13:11:55.255379: step 758, loss 0.923359, acc 0.5625\n",
      "2019-03-27T13:11:55.316826: step 759, loss 0.816632, acc 0.46875\n",
      "2019-03-27T13:11:55.377025: step 760, loss 0.923525, acc 0.484375\n",
      "2019-03-27T13:11:55.434631: step 761, loss 0.863071, acc 0.546875\n",
      "2019-03-27T13:11:55.492443: step 762, loss 0.843108, acc 0.53125\n",
      "2019-03-27T13:11:55.554303: step 763, loss 0.771048, acc 0.546875\n",
      "2019-03-27T13:11:55.616143: step 764, loss 0.827536, acc 0.609375\n",
      "2019-03-27T13:11:55.676028: step 765, loss 0.801063, acc 0.484375\n",
      "2019-03-27T13:11:55.735155: step 766, loss 0.93314, acc 0.5\n",
      "2019-03-27T13:11:55.793011: step 767, loss 0.777879, acc 0.609375\n",
      "2019-03-27T13:11:55.854280: step 768, loss 0.786542, acc 0.578125\n",
      "2019-03-27T13:11:55.914998: step 769, loss 0.918847, acc 0.5\n",
      "2019-03-27T13:11:55.964658: step 770, loss 0.848297, acc 0.52381\n",
      "2019-03-27T13:11:56.023282: step 771, loss 0.794271, acc 0.5625\n",
      "2019-03-27T13:11:56.083729: step 772, loss 0.760052, acc 0.578125\n",
      "2019-03-27T13:11:56.143173: step 773, loss 0.677799, acc 0.71875\n",
      "2019-03-27T13:11:56.206937: step 774, loss 0.771422, acc 0.609375\n",
      "2019-03-27T13:11:56.263759: step 775, loss 0.815075, acc 0.578125\n",
      "2019-03-27T13:11:56.324741: step 776, loss 0.901162, acc 0.609375\n",
      "2019-03-27T13:11:56.382100: step 777, loss 0.997585, acc 0.375\n",
      "2019-03-27T13:11:56.441998: step 778, loss 0.884363, acc 0.53125\n",
      "2019-03-27T13:11:56.498896: step 779, loss 0.758648, acc 0.65625\n",
      "2019-03-27T13:11:56.560457: step 780, loss 0.823135, acc 0.5625\n",
      "2019-03-27T13:11:56.622417: step 781, loss 0.678748, acc 0.734375\n",
      "2019-03-27T13:11:56.680083: step 782, loss 0.789262, acc 0.609375\n",
      "2019-03-27T13:11:56.739448: step 783, loss 0.772624, acc 0.65625\n",
      "2019-03-27T13:11:56.798315: step 784, loss 0.86553, acc 0.453125\n",
      "2019-03-27T13:11:56.856588: step 785, loss 0.827903, acc 0.5\n",
      "2019-03-27T13:11:56.916247: step 786, loss 0.776332, acc 0.625\n",
      "2019-03-27T13:11:56.974759: step 787, loss 0.845884, acc 0.640625\n",
      "2019-03-27T13:11:57.037895: step 788, loss 0.788969, acc 0.515625\n",
      "2019-03-27T13:11:57.096937: step 789, loss 0.883699, acc 0.53125\n",
      "2019-03-27T13:11:57.157713: step 790, loss 0.733381, acc 0.59375\n",
      "2019-03-27T13:11:57.218276: step 791, loss 0.938817, acc 0.609375\n",
      "2019-03-27T13:11:57.279108: step 792, loss 0.908516, acc 0.546875\n",
      "2019-03-27T13:11:57.335569: step 793, loss 0.991781, acc 0.484375\n",
      "2019-03-27T13:11:57.393413: step 794, loss 0.774279, acc 0.578125\n",
      "2019-03-27T13:11:57.452345: step 795, loss 0.790658, acc 0.53125\n",
      "2019-03-27T13:11:57.512230: step 796, loss 0.737671, acc 0.65625\n",
      "2019-03-27T13:11:57.571658: step 797, loss 0.841597, acc 0.5625\n",
      "2019-03-27T13:11:57.631523: step 798, loss 0.8763, acc 0.515625\n",
      "2019-03-27T13:11:57.691950: step 799, loss 0.771651, acc 0.625\n",
      "2019-03-27T13:11:57.750739: step 800, loss 0.940765, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:11:57.783308: step 800, loss 0.998616, acc 0.50738\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-800\n",
      "\n",
      "2019-03-27T13:11:57.954006: step 801, loss 0.887804, acc 0.5\n",
      "2019-03-27T13:11:58.014032: step 802, loss 0.750649, acc 0.625\n",
      "2019-03-27T13:11:58.074127: step 803, loss 0.75442, acc 0.609375\n",
      "2019-03-27T13:11:58.137396: step 804, loss 0.701169, acc 0.5625\n",
      "2019-03-27T13:11:58.201975: step 805, loss 0.779564, acc 0.625\n",
      "2019-03-27T13:11:58.261646: step 806, loss 0.770635, acc 0.578125\n",
      "2019-03-27T13:11:58.319716: step 807, loss 0.748183, acc 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:11:58.383278: step 808, loss 0.791967, acc 0.59375\n",
      "2019-03-27T13:11:58.442712: step 809, loss 0.910442, acc 0.59375\n",
      "2019-03-27T13:11:58.503068: step 810, loss 0.86563, acc 0.421875\n",
      "2019-03-27T13:11:58.562768: step 811, loss 0.909076, acc 0.484375\n",
      "2019-03-27T13:11:58.621117: step 812, loss 0.843213, acc 0.5625\n",
      "2019-03-27T13:11:58.685803: step 813, loss 0.694551, acc 0.6875\n",
      "2019-03-27T13:11:58.744921: step 814, loss 0.873838, acc 0.609375\n",
      "2019-03-27T13:11:58.802461: step 815, loss 0.802522, acc 0.59375\n",
      "2019-03-27T13:11:58.862582: step 816, loss 0.746045, acc 0.5625\n",
      "2019-03-27T13:11:58.924245: step 817, loss 0.864989, acc 0.515625\n",
      "2019-03-27T13:11:58.982347: step 818, loss 0.884973, acc 0.515625\n",
      "2019-03-27T13:11:59.043822: step 819, loss 0.830107, acc 0.578125\n",
      "2019-03-27T13:11:59.101777: step 820, loss 0.777444, acc 0.546875\n",
      "2019-03-27T13:11:59.159032: step 821, loss 0.706284, acc 0.671875\n",
      "2019-03-27T13:11:59.219802: step 822, loss 0.816697, acc 0.5\n",
      "2019-03-27T13:11:59.280918: step 823, loss 0.880013, acc 0.546875\n",
      "2019-03-27T13:11:59.340018: step 824, loss 0.865956, acc 0.515625\n",
      "2019-03-27T13:11:59.400035: step 825, loss 0.838386, acc 0.5625\n",
      "2019-03-27T13:11:59.457957: step 826, loss 0.741734, acc 0.625\n",
      "2019-03-27T13:11:59.517343: step 827, loss 0.741576, acc 0.625\n",
      "2019-03-27T13:11:59.577641: step 828, loss 0.701725, acc 0.703125\n",
      "2019-03-27T13:11:59.640846: step 829, loss 0.764693, acc 0.640625\n",
      "2019-03-27T13:11:59.698215: step 830, loss 0.866944, acc 0.484375\n",
      "2019-03-27T13:11:59.756134: step 831, loss 0.91915, acc 0.453125\n",
      "2019-03-27T13:11:59.817715: step 832, loss 0.859811, acc 0.515625\n",
      "2019-03-27T13:11:59.878284: step 833, loss 0.874675, acc 0.46875\n",
      "2019-03-27T13:11:59.938918: step 834, loss 0.843873, acc 0.5\n",
      "2019-03-27T13:11:59.997416: step 835, loss 0.961448, acc 0.46875\n",
      "2019-03-27T13:12:00.058089: step 836, loss 0.768115, acc 0.5625\n",
      "2019-03-27T13:12:00.118267: step 837, loss 0.78072, acc 0.578125\n",
      "2019-03-27T13:12:00.179667: step 838, loss 0.774207, acc 0.59375\n",
      "2019-03-27T13:12:00.240937: step 839, loss 0.950144, acc 0.484375\n",
      "2019-03-27T13:12:00.299092: step 840, loss 0.838723, acc 0.5625\n",
      "2019-03-27T13:12:00.362146: step 841, loss 0.82174, acc 0.53125\n",
      "2019-03-27T13:12:00.425480: step 842, loss 0.734893, acc 0.625\n",
      "2019-03-27T13:12:00.485044: step 843, loss 0.831125, acc 0.609375\n",
      "2019-03-27T13:12:00.546033: step 844, loss 0.846922, acc 0.578125\n",
      "2019-03-27T13:12:00.606184: step 845, loss 0.848809, acc 0.5625\n",
      "2019-03-27T13:12:00.665602: step 846, loss 0.711775, acc 0.6875\n",
      "2019-03-27T13:12:00.713784: step 847, loss 0.895012, acc 0.52381\n",
      "2019-03-27T13:12:00.772733: step 848, loss 0.785936, acc 0.671875\n",
      "2019-03-27T13:12:00.834325: step 849, loss 0.800275, acc 0.625\n",
      "2019-03-27T13:12:00.896301: step 850, loss 0.912768, acc 0.515625\n",
      "2019-03-27T13:12:00.955430: step 851, loss 0.749673, acc 0.578125\n",
      "2019-03-27T13:12:01.016677: step 852, loss 0.834173, acc 0.578125\n",
      "2019-03-27T13:12:01.077294: step 853, loss 0.635778, acc 0.65625\n",
      "2019-03-27T13:12:01.138203: step 854, loss 0.818023, acc 0.59375\n",
      "2019-03-27T13:12:01.199573: step 855, loss 0.856673, acc 0.59375\n",
      "2019-03-27T13:12:01.258899: step 856, loss 0.85293, acc 0.546875\n",
      "2019-03-27T13:12:01.319952: step 857, loss 0.847704, acc 0.546875\n",
      "2019-03-27T13:12:01.378680: step 858, loss 0.849055, acc 0.515625\n",
      "2019-03-27T13:12:01.439168: step 859, loss 0.744377, acc 0.578125\n",
      "2019-03-27T13:12:01.499943: step 860, loss 0.755552, acc 0.609375\n",
      "2019-03-27T13:12:01.562479: step 861, loss 0.789595, acc 0.609375\n",
      "2019-03-27T13:12:01.623402: step 862, loss 0.766636, acc 0.546875\n",
      "2019-03-27T13:12:01.684036: step 863, loss 0.708574, acc 0.65625\n",
      "2019-03-27T13:12:01.743187: step 864, loss 0.81681, acc 0.609375\n",
      "2019-03-27T13:12:01.803754: step 865, loss 0.667377, acc 0.75\n",
      "2019-03-27T13:12:01.863777: step 866, loss 0.810072, acc 0.59375\n",
      "2019-03-27T13:12:01.924160: step 867, loss 0.748311, acc 0.59375\n",
      "2019-03-27T13:12:01.989839: step 868, loss 0.75241, acc 0.640625\n",
      "2019-03-27T13:12:02.052615: step 869, loss 0.77428, acc 0.609375\n",
      "2019-03-27T13:12:02.117963: step 870, loss 0.659727, acc 0.6875\n",
      "2019-03-27T13:12:02.177231: step 871, loss 0.745801, acc 0.625\n",
      "2019-03-27T13:12:02.238754: step 872, loss 0.689168, acc 0.609375\n",
      "2019-03-27T13:12:02.301723: step 873, loss 0.858724, acc 0.5\n",
      "2019-03-27T13:12:02.360640: step 874, loss 0.681727, acc 0.609375\n",
      "2019-03-27T13:12:02.423229: step 875, loss 0.809197, acc 0.515625\n",
      "2019-03-27T13:12:02.482723: step 876, loss 0.754861, acc 0.578125\n",
      "2019-03-27T13:12:02.542577: step 877, loss 0.766124, acc 0.546875\n",
      "2019-03-27T13:12:02.603833: step 878, loss 0.642633, acc 0.625\n",
      "2019-03-27T13:12:02.665506: step 879, loss 0.810348, acc 0.53125\n",
      "2019-03-27T13:12:02.727365: step 880, loss 0.906315, acc 0.59375\n",
      "2019-03-27T13:12:02.786281: step 881, loss 0.81447, acc 0.5625\n",
      "2019-03-27T13:12:02.847993: step 882, loss 0.690986, acc 0.5625\n",
      "2019-03-27T13:12:02.908291: step 883, loss 0.838738, acc 0.578125\n",
      "2019-03-27T13:12:02.967450: step 884, loss 0.716511, acc 0.609375\n",
      "2019-03-27T13:12:03.029305: step 885, loss 0.746835, acc 0.625\n",
      "2019-03-27T13:12:03.089740: step 886, loss 0.774372, acc 0.578125\n",
      "2019-03-27T13:12:03.152237: step 887, loss 0.752619, acc 0.609375\n",
      "2019-03-27T13:12:03.212010: step 888, loss 0.781132, acc 0.65625\n",
      "2019-03-27T13:12:03.272983: step 889, loss 0.999981, acc 0.515625\n",
      "2019-03-27T13:12:03.331863: step 890, loss 0.908412, acc 0.609375\n",
      "2019-03-27T13:12:03.394763: step 891, loss 0.854413, acc 0.546875\n",
      "2019-03-27T13:12:03.455570: step 892, loss 0.705882, acc 0.671875\n",
      "2019-03-27T13:12:03.515397: step 893, loss 0.694363, acc 0.609375\n",
      "2019-03-27T13:12:03.574958: step 894, loss 0.739135, acc 0.609375\n",
      "2019-03-27T13:12:03.633481: step 895, loss 0.972259, acc 0.53125\n",
      "2019-03-27T13:12:03.694749: step 896, loss 0.779012, acc 0.578125\n",
      "2019-03-27T13:12:03.756844: step 897, loss 0.843583, acc 0.5625\n",
      "2019-03-27T13:12:03.818102: step 898, loss 0.818263, acc 0.640625\n",
      "2019-03-27T13:12:03.877800: step 899, loss 0.841176, acc 0.578125\n",
      "2019-03-27T13:12:03.937604: step 900, loss 0.756681, acc 0.59375\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:03.970435: step 900, loss 1.01257, acc 0.446494\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-900\n",
      "\n",
      "2019-03-27T13:12:04.136105: step 901, loss 0.836535, acc 0.59375\n",
      "2019-03-27T13:12:04.197305: step 902, loss 0.93162, acc 0.5625\n",
      "2019-03-27T13:12:04.259221: step 903, loss 0.87845, acc 0.578125\n",
      "2019-03-27T13:12:04.320345: step 904, loss 0.854366, acc 0.5625\n",
      "2019-03-27T13:12:04.381917: step 905, loss 0.842205, acc 0.515625\n",
      "2019-03-27T13:12:04.442633: step 906, loss 0.732024, acc 0.5625\n",
      "2019-03-27T13:12:04.502911: step 907, loss 0.86491, acc 0.5625\n",
      "2019-03-27T13:12:04.558997: step 908, loss 0.799687, acc 0.578125\n",
      "2019-03-27T13:12:04.619588: step 909, loss 0.899779, acc 0.546875\n",
      "2019-03-27T13:12:04.678741: step 910, loss 0.861544, acc 0.578125\n",
      "2019-03-27T13:12:04.738143: step 911, loss 0.794682, acc 0.640625\n",
      "2019-03-27T13:12:04.800843: step 912, loss 0.797998, acc 0.515625\n",
      "2019-03-27T13:12:04.861304: step 913, loss 0.782938, acc 0.625\n",
      "2019-03-27T13:12:04.919082: step 914, loss 0.72201, acc 0.65625\n",
      "2019-03-27T13:12:04.979562: step 915, loss 0.828639, acc 0.578125\n",
      "2019-03-27T13:12:05.039496: step 916, loss 0.915928, acc 0.46875\n",
      "2019-03-27T13:12:05.101211: step 917, loss 0.77349, acc 0.625\n",
      "2019-03-27T13:12:05.161472: step 918, loss 0.780992, acc 0.5625\n",
      "2019-03-27T13:12:05.220522: step 919, loss 0.844271, acc 0.53125\n",
      "2019-03-27T13:12:05.281139: step 920, loss 0.848325, acc 0.5625\n",
      "2019-03-27T13:12:05.343177: step 921, loss 0.769667, acc 0.515625\n",
      "2019-03-27T13:12:05.403607: step 922, loss 0.91394, acc 0.53125\n",
      "2019-03-27T13:12:05.462422: step 923, loss 0.803458, acc 0.546875\n",
      "2019-03-27T13:12:05.513105: step 924, loss 0.945713, acc 0.428571\n",
      "2019-03-27T13:12:05.575129: step 925, loss 0.654971, acc 0.625\n",
      "2019-03-27T13:12:05.636378: step 926, loss 0.802764, acc 0.609375\n",
      "2019-03-27T13:12:05.697580: step 927, loss 0.617741, acc 0.671875\n",
      "2019-03-27T13:12:05.757546: step 928, loss 0.731703, acc 0.65625\n",
      "2019-03-27T13:12:05.815283: step 929, loss 0.63178, acc 0.609375\n",
      "2019-03-27T13:12:05.873569: step 930, loss 0.651817, acc 0.640625\n",
      "2019-03-27T13:12:05.936325: step 931, loss 0.713654, acc 0.625\n",
      "2019-03-27T13:12:05.995589: step 932, loss 0.67426, acc 0.6875\n",
      "2019-03-27T13:12:06.053387: step 933, loss 0.731274, acc 0.578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:12:06.116272: step 934, loss 0.869165, acc 0.453125\n",
      "2019-03-27T13:12:06.178434: step 935, loss 0.626803, acc 0.734375\n",
      "2019-03-27T13:12:06.239422: step 936, loss 0.773693, acc 0.609375\n",
      "2019-03-27T13:12:06.298262: step 937, loss 0.753411, acc 0.578125\n",
      "2019-03-27T13:12:06.355280: step 938, loss 0.752949, acc 0.59375\n",
      "2019-03-27T13:12:06.416733: step 939, loss 0.790357, acc 0.5625\n",
      "2019-03-27T13:12:06.476791: step 940, loss 0.749023, acc 0.59375\n",
      "2019-03-27T13:12:06.536675: step 941, loss 0.765889, acc 0.65625\n",
      "2019-03-27T13:12:06.599325: step 942, loss 0.822958, acc 0.609375\n",
      "2019-03-27T13:12:06.657890: step 943, loss 0.713071, acc 0.703125\n",
      "2019-03-27T13:12:06.717374: step 944, loss 0.668254, acc 0.671875\n",
      "2019-03-27T13:12:06.778946: step 945, loss 0.719318, acc 0.53125\n",
      "2019-03-27T13:12:06.850092: step 946, loss 0.60047, acc 0.65625\n",
      "2019-03-27T13:12:06.908477: step 947, loss 0.848145, acc 0.578125\n",
      "2019-03-27T13:12:06.969219: step 948, loss 0.744105, acc 0.625\n",
      "2019-03-27T13:12:07.032833: step 949, loss 0.73814, acc 0.546875\n",
      "2019-03-27T13:12:07.091725: step 950, loss 0.763286, acc 0.5625\n",
      "2019-03-27T13:12:07.150549: step 951, loss 0.697343, acc 0.625\n",
      "2019-03-27T13:12:07.210711: step 952, loss 0.775515, acc 0.578125\n",
      "2019-03-27T13:12:07.270542: step 953, loss 0.827111, acc 0.59375\n",
      "2019-03-27T13:12:07.331150: step 954, loss 0.769191, acc 0.625\n",
      "2019-03-27T13:12:07.391094: step 955, loss 0.794543, acc 0.59375\n",
      "2019-03-27T13:12:07.454234: step 956, loss 0.55769, acc 0.71875\n",
      "2019-03-27T13:12:07.513579: step 957, loss 0.723559, acc 0.546875\n",
      "2019-03-27T13:12:07.576022: step 958, loss 0.843653, acc 0.625\n",
      "2019-03-27T13:12:07.634744: step 959, loss 0.673857, acc 0.625\n",
      "2019-03-27T13:12:07.695057: step 960, loss 0.707794, acc 0.703125\n",
      "2019-03-27T13:12:07.763499: step 961, loss 0.708714, acc 0.59375\n",
      "2019-03-27T13:12:07.827579: step 962, loss 0.820996, acc 0.5625\n",
      "2019-03-27T13:12:07.889693: step 963, loss 0.671525, acc 0.671875\n",
      "2019-03-27T13:12:07.950738: step 964, loss 0.712332, acc 0.65625\n",
      "2019-03-27T13:12:08.012119: step 965, loss 0.644147, acc 0.65625\n",
      "2019-03-27T13:12:08.073923: step 966, loss 0.699253, acc 0.609375\n",
      "2019-03-27T13:12:08.132344: step 967, loss 0.862067, acc 0.5625\n",
      "2019-03-27T13:12:08.196217: step 968, loss 0.786905, acc 0.609375\n",
      "2019-03-27T13:12:08.258837: step 969, loss 0.70021, acc 0.59375\n",
      "2019-03-27T13:12:08.318407: step 970, loss 0.703393, acc 0.625\n",
      "2019-03-27T13:12:08.379653: step 971, loss 0.808369, acc 0.609375\n",
      "2019-03-27T13:12:08.440994: step 972, loss 0.737974, acc 0.640625\n",
      "2019-03-27T13:12:08.505800: step 973, loss 0.837949, acc 0.53125\n",
      "2019-03-27T13:12:08.564862: step 974, loss 0.752746, acc 0.5625\n",
      "2019-03-27T13:12:08.628443: step 975, loss 0.818072, acc 0.546875\n",
      "2019-03-27T13:12:08.688554: step 976, loss 0.824656, acc 0.59375\n",
      "2019-03-27T13:12:08.751626: step 977, loss 0.652146, acc 0.640625\n",
      "2019-03-27T13:12:08.813319: step 978, loss 0.859789, acc 0.546875\n",
      "2019-03-27T13:12:08.871051: step 979, loss 0.776695, acc 0.53125\n",
      "2019-03-27T13:12:08.933712: step 980, loss 0.792692, acc 0.59375\n",
      "2019-03-27T13:12:08.995061: step 981, loss 0.706195, acc 0.59375\n",
      "2019-03-27T13:12:09.058922: step 982, loss 0.804255, acc 0.609375\n",
      "2019-03-27T13:12:09.116799: step 983, loss 0.796133, acc 0.5625\n",
      "2019-03-27T13:12:09.179396: step 984, loss 0.783329, acc 0.578125\n",
      "2019-03-27T13:12:09.243166: step 985, loss 0.903827, acc 0.515625\n",
      "2019-03-27T13:12:09.302578: step 986, loss 0.628979, acc 0.625\n",
      "2019-03-27T13:12:09.363398: step 987, loss 0.682099, acc 0.625\n",
      "2019-03-27T13:12:09.428298: step 988, loss 0.688486, acc 0.65625\n",
      "2019-03-27T13:12:09.489211: step 989, loss 0.858499, acc 0.59375\n",
      "2019-03-27T13:12:09.552419: step 990, loss 0.791155, acc 0.609375\n",
      "2019-03-27T13:12:09.612446: step 991, loss 0.691511, acc 0.65625\n",
      "2019-03-27T13:12:09.671899: step 992, loss 0.837323, acc 0.5625\n",
      "2019-03-27T13:12:09.728814: step 993, loss 0.806645, acc 0.5625\n",
      "2019-03-27T13:12:09.789191: step 994, loss 0.679798, acc 0.671875\n",
      "2019-03-27T13:12:09.851319: step 995, loss 0.755121, acc 0.65625\n",
      "2019-03-27T13:12:09.908736: step 996, loss 0.721628, acc 0.546875\n",
      "2019-03-27T13:12:09.969815: step 997, loss 0.762135, acc 0.59375\n",
      "2019-03-27T13:12:10.035021: step 998, loss 0.732708, acc 0.65625\n",
      "2019-03-27T13:12:10.095902: step 999, loss 0.748661, acc 0.59375\n",
      "2019-03-27T13:12:10.153868: step 1000, loss 0.64105, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:10.187638: step 1000, loss 0.917072, acc 0.392989\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1000\n",
      "\n",
      "2019-03-27T13:12:10.341946: step 1001, loss 0.8848, acc 0.47619\n",
      "2019-03-27T13:12:10.406523: step 1002, loss 0.722739, acc 0.578125\n",
      "2019-03-27T13:12:10.465080: step 1003, loss 0.695522, acc 0.703125\n",
      "2019-03-27T13:12:10.523889: step 1004, loss 0.812488, acc 0.5625\n",
      "2019-03-27T13:12:10.581587: step 1005, loss 0.698267, acc 0.625\n",
      "2019-03-27T13:12:10.642099: step 1006, loss 0.791104, acc 0.53125\n",
      "2019-03-27T13:12:10.703644: step 1007, loss 0.752029, acc 0.578125\n",
      "2019-03-27T13:12:10.765497: step 1008, loss 0.803329, acc 0.5\n",
      "2019-03-27T13:12:10.828778: step 1009, loss 0.802673, acc 0.59375\n",
      "2019-03-27T13:12:10.886712: step 1010, loss 0.78638, acc 0.59375\n",
      "2019-03-27T13:12:10.945896: step 1011, loss 0.704972, acc 0.609375\n",
      "2019-03-27T13:12:11.006627: step 1012, loss 0.625135, acc 0.6875\n",
      "2019-03-27T13:12:11.064315: step 1013, loss 0.722376, acc 0.609375\n",
      "2019-03-27T13:12:11.122409: step 1014, loss 0.776572, acc 0.5625\n",
      "2019-03-27T13:12:11.183834: step 1015, loss 0.728356, acc 0.65625\n",
      "2019-03-27T13:12:11.245643: step 1016, loss 0.815308, acc 0.546875\n",
      "2019-03-27T13:12:11.306799: step 1017, loss 0.714871, acc 0.609375\n",
      "2019-03-27T13:12:11.368893: step 1018, loss 0.759227, acc 0.609375\n",
      "2019-03-27T13:12:11.430604: step 1019, loss 0.863845, acc 0.578125\n",
      "2019-03-27T13:12:11.490666: step 1020, loss 0.714017, acc 0.640625\n",
      "2019-03-27T13:12:11.552814: step 1021, loss 0.723431, acc 0.59375\n",
      "2019-03-27T13:12:11.613738: step 1022, loss 0.727441, acc 0.515625\n",
      "2019-03-27T13:12:11.674688: step 1023, loss 0.687879, acc 0.703125\n",
      "2019-03-27T13:12:11.734082: step 1024, loss 0.76079, acc 0.625\n",
      "2019-03-27T13:12:11.794867: step 1025, loss 0.755782, acc 0.6875\n",
      "2019-03-27T13:12:11.854595: step 1026, loss 0.720638, acc 0.59375\n",
      "2019-03-27T13:12:11.915172: step 1027, loss 0.716755, acc 0.609375\n",
      "2019-03-27T13:12:11.979052: step 1028, loss 0.692807, acc 0.65625\n",
      "2019-03-27T13:12:12.040689: step 1029, loss 0.652454, acc 0.625\n",
      "2019-03-27T13:12:12.099352: step 1030, loss 0.778037, acc 0.625\n",
      "2019-03-27T13:12:12.159879: step 1031, loss 0.60753, acc 0.65625\n",
      "2019-03-27T13:12:12.221352: step 1032, loss 0.865538, acc 0.4375\n",
      "2019-03-27T13:12:12.285492: step 1033, loss 0.785013, acc 0.5625\n",
      "2019-03-27T13:12:12.343923: step 1034, loss 0.71363, acc 0.546875\n",
      "2019-03-27T13:12:12.406105: step 1035, loss 0.676695, acc 0.578125\n",
      "2019-03-27T13:12:12.465924: step 1036, loss 0.81303, acc 0.5625\n",
      "2019-03-27T13:12:12.525443: step 1037, loss 0.789313, acc 0.578125\n",
      "2019-03-27T13:12:12.588438: step 1038, loss 0.719371, acc 0.640625\n",
      "2019-03-27T13:12:12.649728: step 1039, loss 0.752575, acc 0.578125\n",
      "2019-03-27T13:12:12.711144: step 1040, loss 0.683578, acc 0.59375\n",
      "2019-03-27T13:12:12.770583: step 1041, loss 0.756191, acc 0.53125\n",
      "2019-03-27T13:12:12.829541: step 1042, loss 0.723596, acc 0.578125\n",
      "2019-03-27T13:12:12.889289: step 1043, loss 0.679453, acc 0.65625\n",
      "2019-03-27T13:12:12.950864: step 1044, loss 0.766857, acc 0.609375\n",
      "2019-03-27T13:12:13.012369: step 1045, loss 0.740861, acc 0.609375\n",
      "2019-03-27T13:12:13.072940: step 1046, loss 0.698998, acc 0.640625\n",
      "2019-03-27T13:12:13.133328: step 1047, loss 0.774227, acc 0.578125\n",
      "2019-03-27T13:12:13.191376: step 1048, loss 0.735505, acc 0.640625\n",
      "2019-03-27T13:12:13.257926: step 1049, loss 0.865361, acc 0.453125\n",
      "2019-03-27T13:12:13.319017: step 1050, loss 0.606773, acc 0.65625\n",
      "2019-03-27T13:12:13.380873: step 1051, loss 0.942092, acc 0.5\n",
      "2019-03-27T13:12:13.439780: step 1052, loss 0.65794, acc 0.625\n",
      "2019-03-27T13:12:13.499664: step 1053, loss 0.906691, acc 0.546875\n",
      "2019-03-27T13:12:13.562034: step 1054, loss 0.711218, acc 0.671875\n",
      "2019-03-27T13:12:13.619181: step 1055, loss 0.69896, acc 0.609375\n",
      "2019-03-27T13:12:13.681491: step 1056, loss 0.663331, acc 0.65625\n",
      "2019-03-27T13:12:13.743304: step 1057, loss 0.727937, acc 0.578125\n",
      "2019-03-27T13:12:13.804425: step 1058, loss 0.563536, acc 0.71875\n",
      "2019-03-27T13:12:13.862701: step 1059, loss 0.665001, acc 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:12:13.922075: step 1060, loss 0.738716, acc 0.65625\n",
      "2019-03-27T13:12:13.980339: step 1061, loss 0.60127, acc 0.75\n",
      "2019-03-27T13:12:14.043436: step 1062, loss 0.646947, acc 0.640625\n",
      "2019-03-27T13:12:14.105206: step 1063, loss 0.79224, acc 0.53125\n",
      "2019-03-27T13:12:14.167808: step 1064, loss 0.828529, acc 0.515625\n",
      "2019-03-27T13:12:14.229909: step 1065, loss 0.690605, acc 0.609375\n",
      "2019-03-27T13:12:14.290441: step 1066, loss 0.616872, acc 0.734375\n",
      "2019-03-27T13:12:14.350011: step 1067, loss 0.62808, acc 0.640625\n",
      "2019-03-27T13:12:14.411729: step 1068, loss 0.703873, acc 0.6875\n",
      "2019-03-27T13:12:14.473281: step 1069, loss 0.861048, acc 0.53125\n",
      "2019-03-27T13:12:14.533796: step 1070, loss 0.719531, acc 0.671875\n",
      "2019-03-27T13:12:14.594308: step 1071, loss 0.800514, acc 0.515625\n",
      "2019-03-27T13:12:14.654710: step 1072, loss 0.722977, acc 0.609375\n",
      "2019-03-27T13:12:14.712295: step 1073, loss 0.72552, acc 0.625\n",
      "2019-03-27T13:12:14.772374: step 1074, loss 0.678756, acc 0.5625\n",
      "2019-03-27T13:12:14.832404: step 1075, loss 0.767487, acc 0.609375\n",
      "2019-03-27T13:12:14.892149: step 1076, loss 0.738729, acc 0.65625\n",
      "2019-03-27T13:12:14.950934: step 1077, loss 0.675234, acc 0.640625\n",
      "2019-03-27T13:12:14.999275: step 1078, loss 0.67459, acc 0.52381\n",
      "2019-03-27T13:12:15.061685: step 1079, loss 0.801765, acc 0.546875\n",
      "2019-03-27T13:12:15.122773: step 1080, loss 0.676453, acc 0.546875\n",
      "2019-03-27T13:12:15.183803: step 1081, loss 0.683512, acc 0.640625\n",
      "2019-03-27T13:12:15.248576: step 1082, loss 0.653906, acc 0.640625\n",
      "2019-03-27T13:12:15.312156: step 1083, loss 0.668687, acc 0.625\n",
      "2019-03-27T13:12:15.376445: step 1084, loss 0.684146, acc 0.671875\n",
      "2019-03-27T13:12:15.437270: step 1085, loss 0.729041, acc 0.625\n",
      "2019-03-27T13:12:15.495932: step 1086, loss 0.621532, acc 0.6875\n",
      "2019-03-27T13:12:15.559298: step 1087, loss 0.615119, acc 0.609375\n",
      "2019-03-27T13:12:15.620192: step 1088, loss 0.66606, acc 0.640625\n",
      "2019-03-27T13:12:15.681532: step 1089, loss 0.660803, acc 0.71875\n",
      "2019-03-27T13:12:15.744323: step 1090, loss 0.69577, acc 0.59375\n",
      "2019-03-27T13:12:15.805035: step 1091, loss 0.66771, acc 0.5625\n",
      "2019-03-27T13:12:15.865074: step 1092, loss 0.760323, acc 0.578125\n",
      "2019-03-27T13:12:15.925629: step 1093, loss 0.758724, acc 0.625\n",
      "2019-03-27T13:12:15.985199: step 1094, loss 0.547153, acc 0.640625\n",
      "2019-03-27T13:12:16.044347: step 1095, loss 0.75945, acc 0.546875\n",
      "2019-03-27T13:12:16.105232: step 1096, loss 0.749756, acc 0.609375\n",
      "2019-03-27T13:12:16.164254: step 1097, loss 0.657767, acc 0.6875\n",
      "2019-03-27T13:12:16.229769: step 1098, loss 0.636097, acc 0.609375\n",
      "2019-03-27T13:12:16.291592: step 1099, loss 0.671631, acc 0.671875\n",
      "2019-03-27T13:12:16.353054: step 1100, loss 0.599312, acc 0.609375\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:16.387683: step 1100, loss 0.885449, acc 0.381919\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1100\n",
      "\n",
      "2019-03-27T13:12:16.549971: step 1101, loss 0.734639, acc 0.5625\n",
      "2019-03-27T13:12:16.608797: step 1102, loss 0.747213, acc 0.640625\n",
      "2019-03-27T13:12:16.669139: step 1103, loss 0.627859, acc 0.625\n",
      "2019-03-27T13:12:16.728400: step 1104, loss 0.723391, acc 0.5625\n",
      "2019-03-27T13:12:16.790895: step 1105, loss 0.674149, acc 0.625\n",
      "2019-03-27T13:12:16.853132: step 1106, loss 0.687873, acc 0.640625\n",
      "2019-03-27T13:12:16.912985: step 1107, loss 0.611344, acc 0.6875\n",
      "2019-03-27T13:12:16.973074: step 1108, loss 0.674851, acc 0.53125\n",
      "2019-03-27T13:12:17.032823: step 1109, loss 0.709091, acc 0.609375\n",
      "2019-03-27T13:12:17.092392: step 1110, loss 0.772697, acc 0.546875\n",
      "2019-03-27T13:12:17.154114: step 1111, loss 0.67288, acc 0.640625\n",
      "2019-03-27T13:12:17.213150: step 1112, loss 0.625031, acc 0.734375\n",
      "2019-03-27T13:12:17.273218: step 1113, loss 0.745992, acc 0.484375\n",
      "2019-03-27T13:12:17.334261: step 1114, loss 0.647568, acc 0.671875\n",
      "2019-03-27T13:12:17.393747: step 1115, loss 0.651, acc 0.703125\n",
      "2019-03-27T13:12:17.457080: step 1116, loss 0.650932, acc 0.65625\n",
      "2019-03-27T13:12:17.516608: step 1117, loss 0.63401, acc 0.65625\n",
      "2019-03-27T13:12:17.576489: step 1118, loss 0.722017, acc 0.640625\n",
      "2019-03-27T13:12:17.635939: step 1119, loss 0.711076, acc 0.640625\n",
      "2019-03-27T13:12:17.695675: step 1120, loss 0.63456, acc 0.625\n",
      "2019-03-27T13:12:17.756641: step 1121, loss 0.72853, acc 0.65625\n",
      "2019-03-27T13:12:17.816154: step 1122, loss 0.744671, acc 0.640625\n",
      "2019-03-27T13:12:17.876980: step 1123, loss 0.791298, acc 0.578125\n",
      "2019-03-27T13:12:17.937453: step 1124, loss 0.657113, acc 0.65625\n",
      "2019-03-27T13:12:17.996099: step 1125, loss 0.607583, acc 0.671875\n",
      "2019-03-27T13:12:18.056290: step 1126, loss 0.863331, acc 0.5625\n",
      "2019-03-27T13:12:18.119550: step 1127, loss 0.589003, acc 0.6875\n",
      "2019-03-27T13:12:18.181988: step 1128, loss 0.67408, acc 0.625\n",
      "2019-03-27T13:12:18.241995: step 1129, loss 0.745157, acc 0.546875\n",
      "2019-03-27T13:12:18.306048: step 1130, loss 0.77379, acc 0.546875\n",
      "2019-03-27T13:12:18.367707: step 1131, loss 0.722624, acc 0.609375\n",
      "2019-03-27T13:12:18.425161: step 1132, loss 0.687025, acc 0.609375\n",
      "2019-03-27T13:12:18.490142: step 1133, loss 0.663369, acc 0.609375\n",
      "2019-03-27T13:12:18.549828: step 1134, loss 0.56703, acc 0.734375\n",
      "2019-03-27T13:12:18.609979: step 1135, loss 0.695295, acc 0.625\n",
      "2019-03-27T13:12:18.670752: step 1136, loss 0.730485, acc 0.640625\n",
      "2019-03-27T13:12:18.729485: step 1137, loss 0.719106, acc 0.65625\n",
      "2019-03-27T13:12:18.791828: step 1138, loss 0.705095, acc 0.609375\n",
      "2019-03-27T13:12:18.852950: step 1139, loss 0.667993, acc 0.640625\n",
      "2019-03-27T13:12:18.913648: step 1140, loss 0.593738, acc 0.640625\n",
      "2019-03-27T13:12:18.971569: step 1141, loss 0.625657, acc 0.65625\n",
      "2019-03-27T13:12:19.033314: step 1142, loss 0.815084, acc 0.515625\n",
      "2019-03-27T13:12:19.093089: step 1143, loss 0.654199, acc 0.578125\n",
      "2019-03-27T13:12:19.151426: step 1144, loss 0.759095, acc 0.609375\n",
      "2019-03-27T13:12:19.214659: step 1145, loss 0.596678, acc 0.71875\n",
      "2019-03-27T13:12:19.278130: step 1146, loss 0.686363, acc 0.625\n",
      "2019-03-27T13:12:19.338672: step 1147, loss 0.678222, acc 0.71875\n",
      "2019-03-27T13:12:19.400336: step 1148, loss 0.72962, acc 0.6875\n",
      "2019-03-27T13:12:19.460503: step 1149, loss 0.676815, acc 0.65625\n",
      "2019-03-27T13:12:19.518548: step 1150, loss 0.778098, acc 0.59375\n",
      "2019-03-27T13:12:19.581840: step 1151, loss 0.792557, acc 0.59375\n",
      "2019-03-27T13:12:19.639809: step 1152, loss 0.688466, acc 0.53125\n",
      "2019-03-27T13:12:19.698626: step 1153, loss 0.644399, acc 0.6875\n",
      "2019-03-27T13:12:19.760855: step 1154, loss 0.685056, acc 0.609375\n",
      "2019-03-27T13:12:19.811446: step 1155, loss 0.667828, acc 0.571429\n",
      "2019-03-27T13:12:19.871214: step 1156, loss 0.677519, acc 0.625\n",
      "2019-03-27T13:12:19.932354: step 1157, loss 0.67364, acc 0.671875\n",
      "2019-03-27T13:12:19.994433: step 1158, loss 0.759689, acc 0.546875\n",
      "2019-03-27T13:12:20.053427: step 1159, loss 0.709402, acc 0.5625\n",
      "2019-03-27T13:12:20.114639: step 1160, loss 0.629507, acc 0.6875\n",
      "2019-03-27T13:12:20.176672: step 1161, loss 0.583957, acc 0.71875\n",
      "2019-03-27T13:12:20.234853: step 1162, loss 0.629933, acc 0.65625\n",
      "2019-03-27T13:12:20.295605: step 1163, loss 0.79689, acc 0.5625\n",
      "2019-03-27T13:12:20.356841: step 1164, loss 0.706381, acc 0.6875\n",
      "2019-03-27T13:12:20.416993: step 1165, loss 0.622987, acc 0.671875\n",
      "2019-03-27T13:12:20.476902: step 1166, loss 0.649315, acc 0.59375\n",
      "2019-03-27T13:12:20.537090: step 1167, loss 0.71627, acc 0.546875\n",
      "2019-03-27T13:12:20.593667: step 1168, loss 0.65379, acc 0.640625\n",
      "2019-03-27T13:12:20.655898: step 1169, loss 0.858585, acc 0.546875\n",
      "2019-03-27T13:12:20.714493: step 1170, loss 0.683848, acc 0.625\n",
      "2019-03-27T13:12:20.773929: step 1171, loss 0.827768, acc 0.546875\n",
      "2019-03-27T13:12:20.837881: step 1172, loss 0.560453, acc 0.703125\n",
      "2019-03-27T13:12:20.897986: step 1173, loss 0.580174, acc 0.625\n",
      "2019-03-27T13:12:20.956429: step 1174, loss 0.657852, acc 0.625\n",
      "2019-03-27T13:12:21.017474: step 1175, loss 0.681616, acc 0.578125\n",
      "2019-03-27T13:12:21.077893: step 1176, loss 0.568765, acc 0.703125\n",
      "2019-03-27T13:12:21.138342: step 1177, loss 0.744698, acc 0.625\n",
      "2019-03-27T13:12:21.197856: step 1178, loss 0.562864, acc 0.671875\n",
      "2019-03-27T13:12:21.255380: step 1179, loss 0.721681, acc 0.53125\n",
      "2019-03-27T13:12:21.313163: step 1180, loss 0.62956, acc 0.75\n",
      "2019-03-27T13:12:21.375028: step 1181, loss 0.641734, acc 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:12:21.438392: step 1182, loss 0.534653, acc 0.75\n",
      "2019-03-27T13:12:21.495591: step 1183, loss 0.69082, acc 0.59375\n",
      "2019-03-27T13:12:21.553918: step 1184, loss 0.61422, acc 0.625\n",
      "2019-03-27T13:12:21.614125: step 1185, loss 0.618988, acc 0.65625\n",
      "2019-03-27T13:12:21.673255: step 1186, loss 0.713688, acc 0.609375\n",
      "2019-03-27T13:12:21.736133: step 1187, loss 0.633032, acc 0.609375\n",
      "2019-03-27T13:12:21.801256: step 1188, loss 0.784382, acc 0.59375\n",
      "2019-03-27T13:12:21.864034: step 1189, loss 0.707202, acc 0.640625\n",
      "2019-03-27T13:12:21.923689: step 1190, loss 0.677434, acc 0.59375\n",
      "2019-03-27T13:12:21.980456: step 1191, loss 0.583797, acc 0.625\n",
      "2019-03-27T13:12:22.041938: step 1192, loss 0.57797, acc 0.703125\n",
      "2019-03-27T13:12:22.130577: step 1193, loss 0.544414, acc 0.765625\n",
      "2019-03-27T13:12:22.185695: step 1194, loss 0.621462, acc 0.640625\n",
      "2019-03-27T13:12:22.245320: step 1195, loss 0.850195, acc 0.546875\n",
      "2019-03-27T13:12:22.306477: step 1196, loss 0.722531, acc 0.625\n",
      "2019-03-27T13:12:22.371994: step 1197, loss 0.664634, acc 0.625\n",
      "2019-03-27T13:12:22.434402: step 1198, loss 0.57315, acc 0.71875\n",
      "2019-03-27T13:12:22.493091: step 1199, loss 0.725038, acc 0.59375\n",
      "2019-03-27T13:12:22.551872: step 1200, loss 0.741741, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:22.584328: step 1200, loss 0.910701, acc 0.498155\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1200\n",
      "\n",
      "2019-03-27T13:12:22.812342: step 1201, loss 0.636815, acc 0.65625\n",
      "2019-03-27T13:12:22.869848: step 1202, loss 0.677389, acc 0.65625\n",
      "2019-03-27T13:12:22.932312: step 1203, loss 0.670391, acc 0.6875\n",
      "2019-03-27T13:12:22.995242: step 1204, loss 0.703535, acc 0.578125\n",
      "2019-03-27T13:12:23.057379: step 1205, loss 0.585049, acc 0.75\n",
      "2019-03-27T13:12:23.120129: step 1206, loss 0.753139, acc 0.59375\n",
      "2019-03-27T13:12:23.184691: step 1207, loss 0.647565, acc 0.6875\n",
      "2019-03-27T13:12:23.250601: step 1208, loss 0.654963, acc 0.625\n",
      "2019-03-27T13:12:23.311897: step 1209, loss 0.58114, acc 0.71875\n",
      "2019-03-27T13:12:23.372245: step 1210, loss 0.587073, acc 0.671875\n",
      "2019-03-27T13:12:23.434457: step 1211, loss 0.765957, acc 0.609375\n",
      "2019-03-27T13:12:23.499261: step 1212, loss 0.672196, acc 0.59375\n",
      "2019-03-27T13:12:23.559291: step 1213, loss 0.684632, acc 0.625\n",
      "2019-03-27T13:12:23.622044: step 1214, loss 0.757765, acc 0.53125\n",
      "2019-03-27T13:12:23.682195: step 1215, loss 0.782249, acc 0.59375\n",
      "2019-03-27T13:12:23.744219: step 1216, loss 0.781281, acc 0.59375\n",
      "2019-03-27T13:12:23.806797: step 1217, loss 0.700977, acc 0.609375\n",
      "2019-03-27T13:12:23.864371: step 1218, loss 0.612172, acc 0.671875\n",
      "2019-03-27T13:12:23.928364: step 1219, loss 0.636287, acc 0.625\n",
      "2019-03-27T13:12:23.988620: step 1220, loss 0.604637, acc 0.65625\n",
      "2019-03-27T13:12:24.051669: step 1221, loss 0.630093, acc 0.640625\n",
      "2019-03-27T13:12:24.113358: step 1222, loss 0.662285, acc 0.640625\n",
      "2019-03-27T13:12:24.181990: step 1223, loss 0.822411, acc 0.59375\n",
      "2019-03-27T13:12:24.244275: step 1224, loss 0.750793, acc 0.65625\n",
      "2019-03-27T13:12:24.303965: step 1225, loss 0.650469, acc 0.609375\n",
      "2019-03-27T13:12:24.365105: step 1226, loss 0.622731, acc 0.703125\n",
      "2019-03-27T13:12:24.421014: step 1227, loss 0.657194, acc 0.59375\n",
      "2019-03-27T13:12:24.484138: step 1228, loss 0.763882, acc 0.5625\n",
      "2019-03-27T13:12:24.543734: step 1229, loss 0.737158, acc 0.5625\n",
      "2019-03-27T13:12:24.603087: step 1230, loss 0.723671, acc 0.546875\n",
      "2019-03-27T13:12:24.663312: step 1231, loss 0.715354, acc 0.625\n",
      "2019-03-27T13:12:24.707339: step 1232, loss 0.86797, acc 0.619048\n",
      "2019-03-27T13:12:24.766092: step 1233, loss 0.738427, acc 0.46875\n",
      "2019-03-27T13:12:24.827963: step 1234, loss 0.612462, acc 0.65625\n",
      "2019-03-27T13:12:24.886426: step 1235, loss 0.605659, acc 0.671875\n",
      "2019-03-27T13:12:24.953793: step 1236, loss 0.681034, acc 0.609375\n",
      "2019-03-27T13:12:25.014175: step 1237, loss 0.872404, acc 0.578125\n",
      "2019-03-27T13:12:25.079745: step 1238, loss 0.732894, acc 0.546875\n",
      "2019-03-27T13:12:25.137420: step 1239, loss 0.596717, acc 0.6875\n",
      "2019-03-27T13:12:25.199173: step 1240, loss 0.650374, acc 0.625\n",
      "2019-03-27T13:12:25.256112: step 1241, loss 0.698663, acc 0.609375\n",
      "2019-03-27T13:12:25.316824: step 1242, loss 0.612292, acc 0.703125\n",
      "2019-03-27T13:12:25.380910: step 1243, loss 0.546015, acc 0.75\n",
      "2019-03-27T13:12:25.439361: step 1244, loss 0.831601, acc 0.5\n",
      "2019-03-27T13:12:25.510816: step 1245, loss 0.651757, acc 0.734375\n",
      "2019-03-27T13:12:25.575471: step 1246, loss 0.613272, acc 0.65625\n",
      "2019-03-27T13:12:25.635432: step 1247, loss 0.618384, acc 0.65625\n",
      "2019-03-27T13:12:25.694158: step 1248, loss 0.755294, acc 0.671875\n",
      "2019-03-27T13:12:25.753265: step 1249, loss 0.54642, acc 0.671875\n",
      "2019-03-27T13:12:25.821164: step 1250, loss 0.680144, acc 0.625\n",
      "2019-03-27T13:12:25.891825: step 1251, loss 0.615638, acc 0.625\n",
      "2019-03-27T13:12:25.954022: step 1252, loss 0.45396, acc 0.765625\n",
      "2019-03-27T13:12:26.016108: step 1253, loss 0.549941, acc 0.6875\n",
      "2019-03-27T13:12:26.074129: step 1254, loss 0.732174, acc 0.625\n",
      "2019-03-27T13:12:26.135828: step 1255, loss 0.525243, acc 0.765625\n",
      "2019-03-27T13:12:26.194771: step 1256, loss 0.614005, acc 0.59375\n",
      "2019-03-27T13:12:26.253304: step 1257, loss 0.598033, acc 0.65625\n",
      "2019-03-27T13:12:26.317406: step 1258, loss 0.559945, acc 0.71875\n",
      "2019-03-27T13:12:26.375842: step 1259, loss 0.682485, acc 0.5625\n",
      "2019-03-27T13:12:26.437787: step 1260, loss 0.518487, acc 0.765625\n",
      "2019-03-27T13:12:26.496993: step 1261, loss 0.529224, acc 0.71875\n",
      "2019-03-27T13:12:26.560829: step 1262, loss 0.613157, acc 0.609375\n",
      "2019-03-27T13:12:26.622490: step 1263, loss 0.651517, acc 0.671875\n",
      "2019-03-27T13:12:26.685038: step 1264, loss 0.637747, acc 0.59375\n",
      "2019-03-27T13:12:26.747998: step 1265, loss 0.752468, acc 0.5625\n",
      "2019-03-27T13:12:26.810382: step 1266, loss 0.656611, acc 0.59375\n",
      "2019-03-27T13:12:26.872470: step 1267, loss 0.669629, acc 0.609375\n",
      "2019-03-27T13:12:26.943012: step 1268, loss 0.622591, acc 0.625\n",
      "2019-03-27T13:12:27.001621: step 1269, loss 0.743543, acc 0.5625\n",
      "2019-03-27T13:12:27.061199: step 1270, loss 0.616341, acc 0.640625\n",
      "2019-03-27T13:12:27.124039: step 1271, loss 0.533237, acc 0.6875\n",
      "2019-03-27T13:12:27.188061: step 1272, loss 0.697925, acc 0.546875\n",
      "2019-03-27T13:12:27.250750: step 1273, loss 0.634858, acc 0.625\n",
      "2019-03-27T13:12:27.309689: step 1274, loss 0.608947, acc 0.765625\n",
      "2019-03-27T13:12:27.369713: step 1275, loss 0.674004, acc 0.578125\n",
      "2019-03-27T13:12:27.431761: step 1276, loss 0.71035, acc 0.59375\n",
      "2019-03-27T13:12:27.494241: step 1277, loss 0.652558, acc 0.578125\n",
      "2019-03-27T13:12:27.554997: step 1278, loss 0.552911, acc 0.640625\n",
      "2019-03-27T13:12:27.617525: step 1279, loss 0.602023, acc 0.734375\n",
      "2019-03-27T13:12:27.679673: step 1280, loss 0.56955, acc 0.765625\n",
      "2019-03-27T13:12:27.741955: step 1281, loss 0.618165, acc 0.65625\n",
      "2019-03-27T13:12:27.804434: step 1282, loss 0.619832, acc 0.6875\n",
      "2019-03-27T13:12:27.864513: step 1283, loss 0.732663, acc 0.609375\n",
      "2019-03-27T13:12:27.924221: step 1284, loss 0.535924, acc 0.703125\n",
      "2019-03-27T13:12:27.983088: step 1285, loss 0.687574, acc 0.578125\n",
      "2019-03-27T13:12:28.042114: step 1286, loss 0.683004, acc 0.59375\n",
      "2019-03-27T13:12:28.101870: step 1287, loss 0.626835, acc 0.671875\n",
      "2019-03-27T13:12:28.161760: step 1288, loss 0.860604, acc 0.53125\n",
      "2019-03-27T13:12:28.223100: step 1289, loss 0.607436, acc 0.671875\n",
      "2019-03-27T13:12:28.282163: step 1290, loss 0.615787, acc 0.6875\n",
      "2019-03-27T13:12:28.340056: step 1291, loss 0.65431, acc 0.71875\n",
      "2019-03-27T13:12:28.401272: step 1292, loss 0.565342, acc 0.6875\n",
      "2019-03-27T13:12:28.459102: step 1293, loss 0.592198, acc 0.65625\n",
      "2019-03-27T13:12:28.516979: step 1294, loss 0.719259, acc 0.53125\n",
      "2019-03-27T13:12:28.575216: step 1295, loss 0.546238, acc 0.734375\n",
      "2019-03-27T13:12:28.634661: step 1296, loss 0.692857, acc 0.5625\n",
      "2019-03-27T13:12:28.697974: step 1297, loss 0.620437, acc 0.625\n",
      "2019-03-27T13:12:28.759923: step 1298, loss 0.635547, acc 0.671875\n",
      "2019-03-27T13:12:28.818677: step 1299, loss 0.645605, acc 0.625\n",
      "2019-03-27T13:12:28.875376: step 1300, loss 0.739206, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:28.907717: step 1300, loss 0.879383, acc 0.437269\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1300\n",
      "\n",
      "2019-03-27T13:12:29.073443: step 1301, loss 0.661679, acc 0.59375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:12:29.135421: step 1302, loss 0.723855, acc 0.671875\n",
      "2019-03-27T13:12:29.196145: step 1303, loss 0.581007, acc 0.65625\n",
      "2019-03-27T13:12:29.259047: step 1304, loss 0.603604, acc 0.640625\n",
      "2019-03-27T13:12:29.324638: step 1305, loss 0.729408, acc 0.625\n",
      "2019-03-27T13:12:29.384755: step 1306, loss 0.614713, acc 0.625\n",
      "2019-03-27T13:12:29.447403: step 1307, loss 0.700964, acc 0.609375\n",
      "2019-03-27T13:12:29.507081: step 1308, loss 0.643511, acc 0.640625\n",
      "2019-03-27T13:12:29.562954: step 1309, loss 0.468002, acc 0.809524\n",
      "2019-03-27T13:12:29.631626: step 1310, loss 0.648557, acc 0.578125\n",
      "2019-03-27T13:12:29.695410: step 1311, loss 0.588945, acc 0.609375\n",
      "2019-03-27T13:12:29.760403: step 1312, loss 0.553109, acc 0.671875\n",
      "2019-03-27T13:12:29.825846: step 1313, loss 0.471697, acc 0.796875\n",
      "2019-03-27T13:12:29.891430: step 1314, loss 0.686115, acc 0.703125\n",
      "2019-03-27T13:12:29.953765: step 1315, loss 0.625454, acc 0.640625\n",
      "2019-03-27T13:12:30.022312: step 1316, loss 0.671014, acc 0.59375\n",
      "2019-03-27T13:12:30.091972: step 1317, loss 0.677437, acc 0.625\n",
      "2019-03-27T13:12:30.157982: step 1318, loss 0.570227, acc 0.609375\n",
      "2019-03-27T13:12:30.228481: step 1319, loss 0.669148, acc 0.71875\n",
      "2019-03-27T13:12:30.292635: step 1320, loss 0.637156, acc 0.640625\n",
      "2019-03-27T13:12:30.361661: step 1321, loss 0.693897, acc 0.53125\n",
      "2019-03-27T13:12:30.428770: step 1322, loss 0.647834, acc 0.59375\n",
      "2019-03-27T13:12:30.489226: step 1323, loss 0.680689, acc 0.53125\n",
      "2019-03-27T13:12:30.551392: step 1324, loss 0.619912, acc 0.6875\n",
      "2019-03-27T13:12:30.613534: step 1325, loss 0.702473, acc 0.578125\n",
      "2019-03-27T13:12:30.671633: step 1326, loss 0.541903, acc 0.6875\n",
      "2019-03-27T13:12:30.731277: step 1327, loss 0.654704, acc 0.640625\n",
      "2019-03-27T13:12:30.796161: step 1328, loss 0.519181, acc 0.6875\n",
      "2019-03-27T13:12:30.860115: step 1329, loss 0.511281, acc 0.734375\n",
      "2019-03-27T13:12:30.922511: step 1330, loss 0.667638, acc 0.5625\n",
      "2019-03-27T13:12:30.985121: step 1331, loss 0.650512, acc 0.640625\n",
      "2019-03-27T13:12:31.047033: step 1332, loss 0.689848, acc 0.609375\n",
      "2019-03-27T13:12:31.112854: step 1333, loss 0.639262, acc 0.609375\n",
      "2019-03-27T13:12:31.172553: step 1334, loss 0.577349, acc 0.6875\n",
      "2019-03-27T13:12:31.238493: step 1335, loss 0.706902, acc 0.5625\n",
      "2019-03-27T13:12:31.300941: step 1336, loss 0.616168, acc 0.640625\n",
      "2019-03-27T13:12:31.362892: step 1337, loss 0.633008, acc 0.609375\n",
      "2019-03-27T13:12:31.426730: step 1338, loss 0.592663, acc 0.734375\n",
      "2019-03-27T13:12:31.489875: step 1339, loss 0.758124, acc 0.59375\n",
      "2019-03-27T13:12:31.550104: step 1340, loss 0.779818, acc 0.609375\n",
      "2019-03-27T13:12:31.617347: step 1341, loss 0.640454, acc 0.640625\n",
      "2019-03-27T13:12:31.680818: step 1342, loss 0.65283, acc 0.6875\n",
      "2019-03-27T13:12:31.743841: step 1343, loss 0.498368, acc 0.734375\n",
      "2019-03-27T13:12:31.808568: step 1344, loss 0.606507, acc 0.625\n",
      "2019-03-27T13:12:31.868487: step 1345, loss 0.66554, acc 0.578125\n",
      "2019-03-27T13:12:31.928905: step 1346, loss 0.757037, acc 0.515625\n",
      "2019-03-27T13:12:31.991130: step 1347, loss 0.77133, acc 0.609375\n",
      "2019-03-27T13:12:32.049343: step 1348, loss 0.545701, acc 0.71875\n",
      "2019-03-27T13:12:32.112790: step 1349, loss 0.628951, acc 0.578125\n",
      "2019-03-27T13:12:32.175621: step 1350, loss 0.608182, acc 0.671875\n",
      "2019-03-27T13:12:32.235496: step 1351, loss 0.483668, acc 0.703125\n",
      "2019-03-27T13:12:32.297218: step 1352, loss 0.67616, acc 0.578125\n",
      "2019-03-27T13:12:32.360582: step 1353, loss 0.545514, acc 0.765625\n",
      "2019-03-27T13:12:32.429512: step 1354, loss 0.636903, acc 0.59375\n",
      "2019-03-27T13:12:32.495566: step 1355, loss 0.683058, acc 0.578125\n",
      "2019-03-27T13:12:32.563186: step 1356, loss 0.704068, acc 0.546875\n",
      "2019-03-27T13:12:32.624085: step 1357, loss 0.696737, acc 0.578125\n",
      "2019-03-27T13:12:32.685403: step 1358, loss 0.65629, acc 0.625\n",
      "2019-03-27T13:12:32.749295: step 1359, loss 0.657512, acc 0.609375\n",
      "2019-03-27T13:12:32.812905: step 1360, loss 0.62482, acc 0.609375\n",
      "2019-03-27T13:12:32.877790: step 1361, loss 0.609247, acc 0.625\n",
      "2019-03-27T13:12:32.942233: step 1362, loss 0.654422, acc 0.671875\n",
      "2019-03-27T13:12:33.000322: step 1363, loss 0.549133, acc 0.65625\n",
      "2019-03-27T13:12:33.064240: step 1364, loss 0.719902, acc 0.53125\n",
      "2019-03-27T13:12:33.125119: step 1365, loss 0.596576, acc 0.6875\n",
      "2019-03-27T13:12:33.185512: step 1366, loss 0.563227, acc 0.765625\n",
      "2019-03-27T13:12:33.248022: step 1367, loss 0.624435, acc 0.625\n",
      "2019-03-27T13:12:33.309485: step 1368, loss 0.60082, acc 0.625\n",
      "2019-03-27T13:12:33.370307: step 1369, loss 0.723417, acc 0.640625\n",
      "2019-03-27T13:12:33.431071: step 1370, loss 0.635696, acc 0.6875\n",
      "2019-03-27T13:12:33.492963: step 1371, loss 0.724687, acc 0.546875\n",
      "2019-03-27T13:12:33.552283: step 1372, loss 0.59758, acc 0.65625\n",
      "2019-03-27T13:12:33.613675: step 1373, loss 0.644661, acc 0.625\n",
      "2019-03-27T13:12:33.671493: step 1374, loss 0.527185, acc 0.75\n",
      "2019-03-27T13:12:33.733111: step 1375, loss 0.506679, acc 0.625\n",
      "2019-03-27T13:12:33.792751: step 1376, loss 0.6436, acc 0.59375\n",
      "2019-03-27T13:12:33.853872: step 1377, loss 0.546491, acc 0.671875\n",
      "2019-03-27T13:12:33.915275: step 1378, loss 0.598268, acc 0.65625\n",
      "2019-03-27T13:12:33.978730: step 1379, loss 0.626173, acc 0.578125\n",
      "2019-03-27T13:12:34.039751: step 1380, loss 0.649145, acc 0.625\n",
      "2019-03-27T13:12:34.097023: step 1381, loss 0.553025, acc 0.65625\n",
      "2019-03-27T13:12:34.157460: step 1382, loss 0.467276, acc 0.734375\n",
      "2019-03-27T13:12:34.218484: step 1383, loss 0.641664, acc 0.640625\n",
      "2019-03-27T13:12:34.279551: step 1384, loss 0.62183, acc 0.671875\n",
      "2019-03-27T13:12:34.343035: step 1385, loss 0.500414, acc 0.640625\n",
      "2019-03-27T13:12:34.392678: step 1386, loss 0.759056, acc 0.52381\n",
      "2019-03-27T13:12:34.453732: step 1387, loss 0.589179, acc 0.65625\n",
      "2019-03-27T13:12:34.515506: step 1388, loss 0.56549, acc 0.671875\n",
      "2019-03-27T13:12:34.574958: step 1389, loss 0.664422, acc 0.625\n",
      "2019-03-27T13:12:34.633581: step 1390, loss 0.599177, acc 0.671875\n",
      "2019-03-27T13:12:34.693268: step 1391, loss 0.538803, acc 0.71875\n",
      "2019-03-27T13:12:34.751138: step 1392, loss 0.666013, acc 0.515625\n",
      "2019-03-27T13:12:34.809705: step 1393, loss 0.625322, acc 0.625\n",
      "2019-03-27T13:12:34.871287: step 1394, loss 0.639734, acc 0.640625\n",
      "2019-03-27T13:12:34.933814: step 1395, loss 0.553974, acc 0.734375\n",
      "2019-03-27T13:12:34.992689: step 1396, loss 0.647402, acc 0.640625\n",
      "2019-03-27T13:12:35.053613: step 1397, loss 0.572346, acc 0.65625\n",
      "2019-03-27T13:12:35.113630: step 1398, loss 0.485923, acc 0.71875\n",
      "2019-03-27T13:12:35.173888: step 1399, loss 0.660653, acc 0.5625\n",
      "2019-03-27T13:12:35.231513: step 1400, loss 0.757429, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:35.264083: step 1400, loss 0.876943, acc 0.549815\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1400\n",
      "\n",
      "2019-03-27T13:12:35.434120: step 1401, loss 0.710205, acc 0.578125\n",
      "2019-03-27T13:12:35.494606: step 1402, loss 0.638397, acc 0.609375\n",
      "2019-03-27T13:12:35.557602: step 1403, loss 0.676394, acc 0.578125\n",
      "2019-03-27T13:12:35.619518: step 1404, loss 0.501278, acc 0.78125\n",
      "2019-03-27T13:12:35.678227: step 1405, loss 0.571449, acc 0.6875\n",
      "2019-03-27T13:12:35.739947: step 1406, loss 0.611912, acc 0.609375\n",
      "2019-03-27T13:12:35.800368: step 1407, loss 0.583981, acc 0.671875\n",
      "2019-03-27T13:12:35.862383: step 1408, loss 0.483509, acc 0.734375\n",
      "2019-03-27T13:12:35.920204: step 1409, loss 0.564689, acc 0.640625\n",
      "2019-03-27T13:12:35.981928: step 1410, loss 0.64745, acc 0.625\n",
      "2019-03-27T13:12:36.043203: step 1411, loss 0.616603, acc 0.5625\n",
      "2019-03-27T13:12:36.100772: step 1412, loss 0.663221, acc 0.5625\n",
      "2019-03-27T13:12:36.160229: step 1413, loss 0.579371, acc 0.625\n",
      "2019-03-27T13:12:36.225344: step 1414, loss 0.684911, acc 0.578125\n",
      "2019-03-27T13:12:36.288564: step 1415, loss 0.584972, acc 0.671875\n",
      "2019-03-27T13:12:36.349137: step 1416, loss 0.590148, acc 0.65625\n",
      "2019-03-27T13:12:36.411147: step 1417, loss 0.566655, acc 0.65625\n",
      "2019-03-27T13:12:36.469192: step 1418, loss 0.610539, acc 0.65625\n",
      "2019-03-27T13:12:36.529742: step 1419, loss 0.577614, acc 0.65625\n",
      "2019-03-27T13:12:36.595198: step 1420, loss 0.66532, acc 0.640625\n",
      "2019-03-27T13:12:36.656539: step 1421, loss 0.557082, acc 0.609375\n",
      "2019-03-27T13:12:36.721594: step 1422, loss 0.736097, acc 0.5\n",
      "2019-03-27T13:12:36.782028: step 1423, loss 0.581192, acc 0.703125\n",
      "2019-03-27T13:12:36.840703: step 1424, loss 0.520724, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:12:36.896962: step 1425, loss 0.521038, acc 0.6875\n",
      "2019-03-27T13:12:36.957562: step 1426, loss 0.693655, acc 0.625\n",
      "2019-03-27T13:12:37.018741: step 1427, loss 0.590993, acc 0.640625\n",
      "2019-03-27T13:12:37.077265: step 1428, loss 0.634474, acc 0.578125\n",
      "2019-03-27T13:12:37.140984: step 1429, loss 0.625484, acc 0.578125\n",
      "2019-03-27T13:12:37.200984: step 1430, loss 0.648216, acc 0.578125\n",
      "2019-03-27T13:12:37.259912: step 1431, loss 0.452548, acc 0.75\n",
      "2019-03-27T13:12:37.320762: step 1432, loss 0.515324, acc 0.625\n",
      "2019-03-27T13:12:37.383700: step 1433, loss 0.557138, acc 0.59375\n",
      "2019-03-27T13:12:37.443929: step 1434, loss 0.480222, acc 0.765625\n",
      "2019-03-27T13:12:37.504066: step 1435, loss 0.528674, acc 0.734375\n",
      "2019-03-27T13:12:37.564555: step 1436, loss 0.719605, acc 0.609375\n",
      "2019-03-27T13:12:37.624984: step 1437, loss 0.566123, acc 0.703125\n",
      "2019-03-27T13:12:37.684464: step 1438, loss 0.655706, acc 0.578125\n",
      "2019-03-27T13:12:37.745377: step 1439, loss 0.551716, acc 0.65625\n",
      "2019-03-27T13:12:37.809382: step 1440, loss 0.628577, acc 0.640625\n",
      "2019-03-27T13:12:37.876982: step 1441, loss 0.603892, acc 0.640625\n",
      "2019-03-27T13:12:37.939620: step 1442, loss 0.643802, acc 0.6875\n",
      "2019-03-27T13:12:38.000109: step 1443, loss 0.529207, acc 0.71875\n",
      "2019-03-27T13:12:38.061002: step 1444, loss 0.527444, acc 0.796875\n",
      "2019-03-27T13:12:38.120476: step 1445, loss 0.642774, acc 0.578125\n",
      "2019-03-27T13:12:38.182215: step 1446, loss 0.696612, acc 0.546875\n",
      "2019-03-27T13:12:38.249468: step 1447, loss 0.616534, acc 0.59375\n",
      "2019-03-27T13:12:38.313073: step 1448, loss 0.595962, acc 0.671875\n",
      "2019-03-27T13:12:38.374708: step 1449, loss 0.575533, acc 0.625\n",
      "2019-03-27T13:12:38.439471: step 1450, loss 0.523678, acc 0.71875\n",
      "2019-03-27T13:12:38.502659: step 1451, loss 0.679212, acc 0.625\n",
      "2019-03-27T13:12:38.565172: step 1452, loss 0.693236, acc 0.578125\n",
      "2019-03-27T13:12:38.628948: step 1453, loss 0.647478, acc 0.65625\n",
      "2019-03-27T13:12:38.691533: step 1454, loss 0.585953, acc 0.6875\n",
      "2019-03-27T13:12:38.750604: step 1455, loss 0.528547, acc 0.6875\n",
      "2019-03-27T13:12:38.810176: step 1456, loss 0.615498, acc 0.65625\n",
      "2019-03-27T13:12:38.872130: step 1457, loss 0.609359, acc 0.578125\n",
      "2019-03-27T13:12:38.932592: step 1458, loss 0.480559, acc 0.71875\n",
      "2019-03-27T13:12:39.001061: step 1459, loss 0.732898, acc 0.53125\n",
      "2019-03-27T13:12:39.063824: step 1460, loss 0.633474, acc 0.703125\n",
      "2019-03-27T13:12:39.125341: step 1461, loss 0.601825, acc 0.625\n",
      "2019-03-27T13:12:39.193034: step 1462, loss 0.735422, acc 0.46875\n",
      "2019-03-27T13:12:39.242146: step 1463, loss 0.698535, acc 0.666667\n",
      "2019-03-27T13:12:39.310806: step 1464, loss 0.703781, acc 0.578125\n",
      "2019-03-27T13:12:39.372978: step 1465, loss 0.492269, acc 0.75\n",
      "2019-03-27T13:12:39.437743: step 1466, loss 0.773951, acc 0.53125\n",
      "2019-03-27T13:12:39.499325: step 1467, loss 0.590917, acc 0.71875\n",
      "2019-03-27T13:12:39.559959: step 1468, loss 0.604982, acc 0.71875\n",
      "2019-03-27T13:12:39.619739: step 1469, loss 0.478871, acc 0.6875\n",
      "2019-03-27T13:12:39.680258: step 1470, loss 0.665361, acc 0.53125\n",
      "2019-03-27T13:12:39.742455: step 1471, loss 0.586733, acc 0.625\n",
      "2019-03-27T13:12:39.803876: step 1472, loss 0.692825, acc 0.578125\n",
      "2019-03-27T13:12:39.867236: step 1473, loss 0.652603, acc 0.703125\n",
      "2019-03-27T13:12:39.931560: step 1474, loss 0.575035, acc 0.6875\n",
      "2019-03-27T13:12:39.992665: step 1475, loss 0.698817, acc 0.578125\n",
      "2019-03-27T13:12:40.051987: step 1476, loss 0.661386, acc 0.59375\n",
      "2019-03-27T13:12:40.112831: step 1477, loss 0.507032, acc 0.71875\n",
      "2019-03-27T13:12:40.172818: step 1478, loss 0.555888, acc 0.609375\n",
      "2019-03-27T13:12:40.233373: step 1479, loss 0.592754, acc 0.625\n",
      "2019-03-27T13:12:40.294471: step 1480, loss 0.694182, acc 0.578125\n",
      "2019-03-27T13:12:40.357046: step 1481, loss 0.593618, acc 0.578125\n",
      "2019-03-27T13:12:40.420126: step 1482, loss 0.480816, acc 0.765625\n",
      "2019-03-27T13:12:40.482386: step 1483, loss 0.572122, acc 0.65625\n",
      "2019-03-27T13:12:40.543742: step 1484, loss 0.629257, acc 0.59375\n",
      "2019-03-27T13:12:40.603899: step 1485, loss 0.620494, acc 0.5625\n",
      "2019-03-27T13:12:40.663843: step 1486, loss 0.503478, acc 0.703125\n",
      "2019-03-27T13:12:40.730697: step 1487, loss 0.572418, acc 0.65625\n",
      "2019-03-27T13:12:40.788782: step 1488, loss 0.530361, acc 0.671875\n",
      "2019-03-27T13:12:40.851444: step 1489, loss 0.572482, acc 0.65625\n",
      "2019-03-27T13:12:40.912134: step 1490, loss 0.696006, acc 0.5625\n",
      "2019-03-27T13:12:40.970861: step 1491, loss 0.54973, acc 0.71875\n",
      "2019-03-27T13:12:41.032546: step 1492, loss 0.686781, acc 0.546875\n",
      "2019-03-27T13:12:41.093047: step 1493, loss 0.58468, acc 0.65625\n",
      "2019-03-27T13:12:41.155415: step 1494, loss 0.538184, acc 0.6875\n",
      "2019-03-27T13:12:41.215163: step 1495, loss 0.639934, acc 0.640625\n",
      "2019-03-27T13:12:41.278273: step 1496, loss 0.625382, acc 0.65625\n",
      "2019-03-27T13:12:41.339665: step 1497, loss 0.660941, acc 0.578125\n",
      "2019-03-27T13:12:41.402737: step 1498, loss 0.485623, acc 0.71875\n",
      "2019-03-27T13:12:41.462711: step 1499, loss 0.56286, acc 0.671875\n",
      "2019-03-27T13:12:41.524555: step 1500, loss 0.612802, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:41.556643: step 1500, loss 0.854706, acc 0.566421\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1500\n",
      "\n",
      "2019-03-27T13:12:41.726928: step 1501, loss 0.550409, acc 0.734375\n",
      "2019-03-27T13:12:41.787713: step 1502, loss 0.533451, acc 0.71875\n",
      "2019-03-27T13:12:41.849800: step 1503, loss 0.654707, acc 0.625\n",
      "2019-03-27T13:12:41.911169: step 1504, loss 0.549922, acc 0.765625\n",
      "2019-03-27T13:12:41.970997: step 1505, loss 0.651755, acc 0.640625\n",
      "2019-03-27T13:12:42.028538: step 1506, loss 0.532623, acc 0.703125\n",
      "2019-03-27T13:12:42.088253: step 1507, loss 0.637748, acc 0.609375\n",
      "2019-03-27T13:12:42.146058: step 1508, loss 0.474334, acc 0.765625\n",
      "2019-03-27T13:12:42.207167: step 1509, loss 0.571483, acc 0.625\n",
      "2019-03-27T13:12:42.267635: step 1510, loss 0.537441, acc 0.703125\n",
      "2019-03-27T13:12:42.327968: step 1511, loss 0.443523, acc 0.765625\n",
      "2019-03-27T13:12:42.393116: step 1512, loss 0.535701, acc 0.671875\n",
      "2019-03-27T13:12:42.451615: step 1513, loss 0.575617, acc 0.59375\n",
      "2019-03-27T13:12:42.511353: step 1514, loss 0.654298, acc 0.578125\n",
      "2019-03-27T13:12:42.576125: step 1515, loss 0.637716, acc 0.671875\n",
      "2019-03-27T13:12:42.636228: step 1516, loss 0.550802, acc 0.6875\n",
      "2019-03-27T13:12:42.698842: step 1517, loss 0.757161, acc 0.546875\n",
      "2019-03-27T13:12:42.760686: step 1518, loss 0.549721, acc 0.71875\n",
      "2019-03-27T13:12:42.818677: step 1519, loss 0.564573, acc 0.71875\n",
      "2019-03-27T13:12:42.879921: step 1520, loss 0.72137, acc 0.65625\n",
      "2019-03-27T13:12:42.940711: step 1521, loss 0.664967, acc 0.546875\n",
      "2019-03-27T13:12:43.004174: step 1522, loss 0.577745, acc 0.640625\n",
      "2019-03-27T13:12:43.066795: step 1523, loss 0.684604, acc 0.546875\n",
      "2019-03-27T13:12:43.127575: step 1524, loss 0.665998, acc 0.65625\n",
      "2019-03-27T13:12:43.190408: step 1525, loss 0.540166, acc 0.65625\n",
      "2019-03-27T13:12:43.252406: step 1526, loss 0.635949, acc 0.59375\n",
      "2019-03-27T13:12:43.314805: step 1527, loss 0.583865, acc 0.59375\n",
      "2019-03-27T13:12:43.373825: step 1528, loss 0.471653, acc 0.75\n",
      "2019-03-27T13:12:43.435093: step 1529, loss 0.701072, acc 0.625\n",
      "2019-03-27T13:12:43.501760: step 1530, loss 0.634976, acc 0.53125\n",
      "2019-03-27T13:12:43.562728: step 1531, loss 0.559399, acc 0.59375\n",
      "2019-03-27T13:12:43.623616: step 1532, loss 0.554167, acc 0.65625\n",
      "2019-03-27T13:12:43.684057: step 1533, loss 0.631268, acc 0.640625\n",
      "2019-03-27T13:12:43.744901: step 1534, loss 0.523637, acc 0.703125\n",
      "2019-03-27T13:12:43.805881: step 1535, loss 0.567153, acc 0.640625\n",
      "2019-03-27T13:12:43.867717: step 1536, loss 0.565228, acc 0.6875\n",
      "2019-03-27T13:12:43.930216: step 1537, loss 0.692616, acc 0.53125\n",
      "2019-03-27T13:12:43.991651: step 1538, loss 0.688863, acc 0.609375\n",
      "2019-03-27T13:12:44.051462: step 1539, loss 0.55299, acc 0.671875\n",
      "2019-03-27T13:12:44.101391: step 1540, loss 0.597571, acc 0.619048\n",
      "2019-03-27T13:12:44.163022: step 1541, loss 0.571394, acc 0.6875\n",
      "2019-03-27T13:12:44.224045: step 1542, loss 0.60903, acc 0.71875\n",
      "2019-03-27T13:12:44.282478: step 1543, loss 0.585684, acc 0.65625\n",
      "2019-03-27T13:12:44.345632: step 1544, loss 0.652356, acc 0.609375\n",
      "2019-03-27T13:12:44.408120: step 1545, loss 0.452172, acc 0.65625\n",
      "2019-03-27T13:12:44.470011: step 1546, loss 0.48019, acc 0.71875\n",
      "2019-03-27T13:12:44.529870: step 1547, loss 0.665742, acc 0.59375\n",
      "2019-03-27T13:12:44.593777: step 1548, loss 0.588985, acc 0.609375\n",
      "2019-03-27T13:12:44.655270: step 1549, loss 0.5764, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:12:44.718370: step 1550, loss 0.487113, acc 0.6875\n",
      "2019-03-27T13:12:44.777974: step 1551, loss 0.627478, acc 0.640625\n",
      "2019-03-27T13:12:44.839921: step 1552, loss 0.56113, acc 0.625\n",
      "2019-03-27T13:12:44.898526: step 1553, loss 0.652809, acc 0.578125\n",
      "2019-03-27T13:12:44.958316: step 1554, loss 0.560123, acc 0.640625\n",
      "2019-03-27T13:12:45.018707: step 1555, loss 0.585994, acc 0.71875\n",
      "2019-03-27T13:12:45.078335: step 1556, loss 0.544124, acc 0.671875\n",
      "2019-03-27T13:12:45.142253: step 1557, loss 0.57511, acc 0.625\n",
      "2019-03-27T13:12:45.200524: step 1558, loss 0.562893, acc 0.671875\n",
      "2019-03-27T13:12:45.258908: step 1559, loss 0.521316, acc 0.640625\n",
      "2019-03-27T13:12:45.318844: step 1560, loss 0.618259, acc 0.609375\n",
      "2019-03-27T13:12:45.376288: step 1561, loss 0.531389, acc 0.65625\n",
      "2019-03-27T13:12:45.435059: step 1562, loss 0.708163, acc 0.5625\n",
      "2019-03-27T13:12:45.494008: step 1563, loss 0.535519, acc 0.75\n",
      "2019-03-27T13:12:45.552984: step 1564, loss 0.60995, acc 0.640625\n",
      "2019-03-27T13:12:45.613292: step 1565, loss 0.556572, acc 0.671875\n",
      "2019-03-27T13:12:45.672382: step 1566, loss 0.538815, acc 0.75\n",
      "2019-03-27T13:12:45.731915: step 1567, loss 0.644224, acc 0.578125\n",
      "2019-03-27T13:12:45.792582: step 1568, loss 0.536767, acc 0.671875\n",
      "2019-03-27T13:12:45.855602: step 1569, loss 0.484706, acc 0.6875\n",
      "2019-03-27T13:12:45.919872: step 1570, loss 0.614856, acc 0.640625\n",
      "2019-03-27T13:12:45.978901: step 1571, loss 0.533687, acc 0.625\n",
      "2019-03-27T13:12:46.047217: step 1572, loss 0.673374, acc 0.53125\n",
      "2019-03-27T13:12:46.114818: step 1573, loss 0.552736, acc 0.6875\n",
      "2019-03-27T13:12:46.178185: step 1574, loss 0.599348, acc 0.671875\n",
      "2019-03-27T13:12:46.239105: step 1575, loss 0.525113, acc 0.640625\n",
      "2019-03-27T13:12:46.303893: step 1576, loss 0.531293, acc 0.625\n",
      "2019-03-27T13:12:46.364908: step 1577, loss 0.652376, acc 0.59375\n",
      "2019-03-27T13:12:46.429804: step 1578, loss 0.575822, acc 0.75\n",
      "2019-03-27T13:12:46.491758: step 1579, loss 0.663033, acc 0.625\n",
      "2019-03-27T13:12:46.555753: step 1580, loss 0.534228, acc 0.671875\n",
      "2019-03-27T13:12:46.615097: step 1581, loss 0.633277, acc 0.5625\n",
      "2019-03-27T13:12:46.677294: step 1582, loss 0.607034, acc 0.6875\n",
      "2019-03-27T13:12:46.738676: step 1583, loss 0.502313, acc 0.65625\n",
      "2019-03-27T13:12:46.801702: step 1584, loss 0.494449, acc 0.734375\n",
      "2019-03-27T13:12:46.864489: step 1585, loss 0.618327, acc 0.59375\n",
      "2019-03-27T13:12:46.926571: step 1586, loss 0.491767, acc 0.703125\n",
      "2019-03-27T13:12:46.988901: step 1587, loss 0.485652, acc 0.71875\n",
      "2019-03-27T13:12:47.047989: step 1588, loss 0.51085, acc 0.671875\n",
      "2019-03-27T13:12:47.108261: step 1589, loss 0.601639, acc 0.578125\n",
      "2019-03-27T13:12:47.171701: step 1590, loss 0.640071, acc 0.65625\n",
      "2019-03-27T13:12:47.231893: step 1591, loss 0.529824, acc 0.65625\n",
      "2019-03-27T13:12:47.290048: step 1592, loss 0.613154, acc 0.625\n",
      "2019-03-27T13:12:47.348885: step 1593, loss 0.600017, acc 0.578125\n",
      "2019-03-27T13:12:47.410624: step 1594, loss 0.705083, acc 0.515625\n",
      "2019-03-27T13:12:47.473998: step 1595, loss 0.569753, acc 0.640625\n",
      "2019-03-27T13:12:47.533858: step 1596, loss 0.575798, acc 0.609375\n",
      "2019-03-27T13:12:47.596759: step 1597, loss 0.687616, acc 0.578125\n",
      "2019-03-27T13:12:47.656342: step 1598, loss 0.585339, acc 0.546875\n",
      "2019-03-27T13:12:47.715912: step 1599, loss 0.529772, acc 0.71875\n",
      "2019-03-27T13:12:47.775426: step 1600, loss 0.529423, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:47.808150: step 1600, loss 0.942941, acc 0.5\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1600\n",
      "\n",
      "2019-03-27T13:12:47.975644: step 1601, loss 0.560762, acc 0.671875\n",
      "2019-03-27T13:12:48.043479: step 1602, loss 0.655259, acc 0.625\n",
      "2019-03-27T13:12:48.102318: step 1603, loss 0.646111, acc 0.65625\n",
      "2019-03-27T13:12:48.164518: step 1604, loss 0.577242, acc 0.71875\n",
      "2019-03-27T13:12:48.226218: step 1605, loss 0.478706, acc 0.703125\n",
      "2019-03-27T13:12:48.286886: step 1606, loss 0.574131, acc 0.640625\n",
      "2019-03-27T13:12:48.344937: step 1607, loss 0.551151, acc 0.65625\n",
      "2019-03-27T13:12:48.408485: step 1608, loss 0.534766, acc 0.671875\n",
      "2019-03-27T13:12:48.472733: step 1609, loss 0.543525, acc 0.671875\n",
      "2019-03-27T13:12:48.532019: step 1610, loss 0.555305, acc 0.65625\n",
      "2019-03-27T13:12:48.596310: step 1611, loss 0.534171, acc 0.703125\n",
      "2019-03-27T13:12:48.658888: step 1612, loss 0.603357, acc 0.625\n",
      "2019-03-27T13:12:48.717927: step 1613, loss 0.694216, acc 0.5\n",
      "2019-03-27T13:12:48.777788: step 1614, loss 0.575026, acc 0.640625\n",
      "2019-03-27T13:12:48.839539: step 1615, loss 0.534886, acc 0.640625\n",
      "2019-03-27T13:12:48.901580: step 1616, loss 0.555683, acc 0.625\n",
      "2019-03-27T13:12:48.949361: step 1617, loss 0.480683, acc 0.666667\n",
      "2019-03-27T13:12:49.012940: step 1618, loss 0.591521, acc 0.6875\n",
      "2019-03-27T13:12:49.073259: step 1619, loss 0.61083, acc 0.59375\n",
      "2019-03-27T13:12:49.133049: step 1620, loss 0.563314, acc 0.65625\n",
      "2019-03-27T13:12:49.194121: step 1621, loss 0.545196, acc 0.65625\n",
      "2019-03-27T13:12:49.255219: step 1622, loss 0.541465, acc 0.671875\n",
      "2019-03-27T13:12:49.317614: step 1623, loss 0.555063, acc 0.671875\n",
      "2019-03-27T13:12:49.382319: step 1624, loss 0.51094, acc 0.703125\n",
      "2019-03-27T13:12:49.444366: step 1625, loss 0.591285, acc 0.671875\n",
      "2019-03-27T13:12:49.509065: step 1626, loss 0.439569, acc 0.8125\n",
      "2019-03-27T13:12:49.570975: step 1627, loss 0.530935, acc 0.6875\n",
      "2019-03-27T13:12:49.632758: step 1628, loss 0.561822, acc 0.625\n",
      "2019-03-27T13:12:49.698071: step 1629, loss 0.597928, acc 0.734375\n",
      "2019-03-27T13:12:49.757310: step 1630, loss 0.598712, acc 0.5625\n",
      "2019-03-27T13:12:49.819794: step 1631, loss 0.485301, acc 0.71875\n",
      "2019-03-27T13:12:49.882623: step 1632, loss 0.599224, acc 0.671875\n",
      "2019-03-27T13:12:49.942182: step 1633, loss 0.550836, acc 0.6875\n",
      "2019-03-27T13:12:50.005126: step 1634, loss 0.436046, acc 0.734375\n",
      "2019-03-27T13:12:50.071297: step 1635, loss 0.554209, acc 0.671875\n",
      "2019-03-27T13:12:50.131833: step 1636, loss 0.568969, acc 0.65625\n",
      "2019-03-27T13:12:50.196094: step 1637, loss 0.619277, acc 0.59375\n",
      "2019-03-27T13:12:50.257907: step 1638, loss 0.535361, acc 0.609375\n",
      "2019-03-27T13:12:50.320183: step 1639, loss 0.538435, acc 0.640625\n",
      "2019-03-27T13:12:50.385952: step 1640, loss 0.533351, acc 0.59375\n",
      "2019-03-27T13:12:50.446087: step 1641, loss 0.554681, acc 0.65625\n",
      "2019-03-27T13:12:50.516362: step 1642, loss 0.705979, acc 0.5625\n",
      "2019-03-27T13:12:50.575410: step 1643, loss 0.520531, acc 0.6875\n",
      "2019-03-27T13:12:50.638461: step 1644, loss 0.564375, acc 0.609375\n",
      "2019-03-27T13:12:50.703091: step 1645, loss 0.517042, acc 0.625\n",
      "2019-03-27T13:12:50.763085: step 1646, loss 0.481976, acc 0.65625\n",
      "2019-03-27T13:12:50.822667: step 1647, loss 0.519974, acc 0.71875\n",
      "2019-03-27T13:12:50.883993: step 1648, loss 0.558429, acc 0.640625\n",
      "2019-03-27T13:12:50.946638: step 1649, loss 0.580217, acc 0.671875\n",
      "2019-03-27T13:12:51.010056: step 1650, loss 0.562953, acc 0.578125\n",
      "2019-03-27T13:12:51.074800: step 1651, loss 0.687223, acc 0.625\n",
      "2019-03-27T13:12:51.137260: step 1652, loss 0.610243, acc 0.59375\n",
      "2019-03-27T13:12:51.198345: step 1653, loss 0.457809, acc 0.71875\n",
      "2019-03-27T13:12:51.258460: step 1654, loss 0.511634, acc 0.6875\n",
      "2019-03-27T13:12:51.321902: step 1655, loss 0.566128, acc 0.625\n",
      "2019-03-27T13:12:51.378891: step 1656, loss 0.499359, acc 0.71875\n",
      "2019-03-27T13:12:51.442534: step 1657, loss 0.617036, acc 0.59375\n",
      "2019-03-27T13:12:51.505259: step 1658, loss 0.445308, acc 0.71875\n",
      "2019-03-27T13:12:51.567108: step 1659, loss 0.525621, acc 0.734375\n",
      "2019-03-27T13:12:51.629098: step 1660, loss 0.511593, acc 0.59375\n",
      "2019-03-27T13:12:51.694296: step 1661, loss 0.722294, acc 0.609375\n",
      "2019-03-27T13:12:51.756456: step 1662, loss 0.51903, acc 0.71875\n",
      "2019-03-27T13:12:51.826359: step 1663, loss 0.561523, acc 0.71875\n",
      "2019-03-27T13:12:51.895789: step 1664, loss 0.560635, acc 0.703125\n",
      "2019-03-27T13:12:51.957787: step 1665, loss 0.518434, acc 0.6875\n",
      "2019-03-27T13:12:52.021277: step 1666, loss 0.517766, acc 0.75\n",
      "2019-03-27T13:12:52.088693: step 1667, loss 0.547087, acc 0.71875\n",
      "2019-03-27T13:12:52.151521: step 1668, loss 0.501659, acc 0.71875\n",
      "2019-03-27T13:12:52.218551: step 1669, loss 0.659545, acc 0.59375\n",
      "2019-03-27T13:12:52.283060: step 1670, loss 0.495346, acc 0.71875\n",
      "2019-03-27T13:12:52.344021: step 1671, loss 0.663002, acc 0.59375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:12:52.409859: step 1672, loss 0.602422, acc 0.640625\n",
      "2019-03-27T13:12:52.471815: step 1673, loss 0.661314, acc 0.5\n",
      "2019-03-27T13:12:52.540538: step 1674, loss 0.476311, acc 0.734375\n",
      "2019-03-27T13:12:52.605119: step 1675, loss 0.63617, acc 0.609375\n",
      "2019-03-27T13:12:52.664364: step 1676, loss 0.540364, acc 0.609375\n",
      "2019-03-27T13:12:52.733423: step 1677, loss 0.467462, acc 0.734375\n",
      "2019-03-27T13:12:52.794129: step 1678, loss 0.583325, acc 0.546875\n",
      "2019-03-27T13:12:52.860401: step 1679, loss 0.50177, acc 0.671875\n",
      "2019-03-27T13:12:52.924259: step 1680, loss 0.550969, acc 0.75\n",
      "2019-03-27T13:12:52.984542: step 1681, loss 0.535303, acc 0.671875\n",
      "2019-03-27T13:12:53.044825: step 1682, loss 0.621856, acc 0.6875\n",
      "2019-03-27T13:12:53.108334: step 1683, loss 0.563391, acc 0.6875\n",
      "2019-03-27T13:12:53.169149: step 1684, loss 0.556787, acc 0.671875\n",
      "2019-03-27T13:12:53.230398: step 1685, loss 0.609734, acc 0.671875\n",
      "2019-03-27T13:12:53.293951: step 1686, loss 0.541363, acc 0.671875\n",
      "2019-03-27T13:12:53.354721: step 1687, loss 0.594536, acc 0.609375\n",
      "2019-03-27T13:12:53.418488: step 1688, loss 0.709884, acc 0.625\n",
      "2019-03-27T13:12:53.480015: step 1689, loss 0.541943, acc 0.671875\n",
      "2019-03-27T13:12:53.538039: step 1690, loss 0.583052, acc 0.59375\n",
      "2019-03-27T13:12:53.602100: step 1691, loss 0.57899, acc 0.6875\n",
      "2019-03-27T13:12:53.662244: step 1692, loss 0.498353, acc 0.640625\n",
      "2019-03-27T13:12:53.722466: step 1693, loss 0.491011, acc 0.75\n",
      "2019-03-27T13:12:53.772548: step 1694, loss 0.487115, acc 0.761905\n",
      "2019-03-27T13:12:53.834758: step 1695, loss 0.577234, acc 0.625\n",
      "2019-03-27T13:12:53.896864: step 1696, loss 0.47526, acc 0.71875\n",
      "2019-03-27T13:12:53.960960: step 1697, loss 0.491318, acc 0.671875\n",
      "2019-03-27T13:12:54.023674: step 1698, loss 0.564556, acc 0.65625\n",
      "2019-03-27T13:12:54.082250: step 1699, loss 0.579174, acc 0.671875\n",
      "2019-03-27T13:12:54.142961: step 1700, loss 0.534425, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:12:54.174818: step 1700, loss 0.859875, acc 0.538745\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1700\n",
      "\n",
      "2019-03-27T13:12:54.342512: step 1701, loss 0.463836, acc 0.71875\n",
      "2019-03-27T13:12:54.406204: step 1702, loss 0.522003, acc 0.65625\n",
      "2019-03-27T13:12:54.469516: step 1703, loss 0.628881, acc 0.578125\n",
      "2019-03-27T13:12:54.531456: step 1704, loss 0.634668, acc 0.65625\n",
      "2019-03-27T13:12:54.591204: step 1705, loss 0.564589, acc 0.609375\n",
      "2019-03-27T13:12:54.651996: step 1706, loss 0.444118, acc 0.734375\n",
      "2019-03-27T13:12:54.712829: step 1707, loss 0.583503, acc 0.609375\n",
      "2019-03-27T13:12:54.772582: step 1708, loss 0.548774, acc 0.65625\n",
      "2019-03-27T13:12:54.834617: step 1709, loss 0.592247, acc 0.578125\n",
      "2019-03-27T13:12:54.893624: step 1710, loss 0.531224, acc 0.71875\n",
      "2019-03-27T13:12:54.951783: step 1711, loss 0.653904, acc 0.578125\n",
      "2019-03-27T13:12:55.011215: step 1712, loss 0.593535, acc 0.59375\n",
      "2019-03-27T13:12:55.070396: step 1713, loss 0.516416, acc 0.765625\n",
      "2019-03-27T13:12:55.131472: step 1714, loss 0.586996, acc 0.59375\n",
      "2019-03-27T13:12:55.192240: step 1715, loss 0.501671, acc 0.71875\n",
      "2019-03-27T13:12:55.256237: step 1716, loss 0.586756, acc 0.625\n",
      "2019-03-27T13:12:55.315254: step 1717, loss 0.559392, acc 0.59375\n",
      "2019-03-27T13:12:55.377196: step 1718, loss 0.519637, acc 0.671875\n",
      "2019-03-27T13:12:55.437117: step 1719, loss 0.506175, acc 0.6875\n",
      "2019-03-27T13:12:55.498972: step 1720, loss 0.521336, acc 0.65625\n",
      "2019-03-27T13:12:55.557964: step 1721, loss 0.509924, acc 0.6875\n",
      "2019-03-27T13:12:55.621595: step 1722, loss 0.523834, acc 0.671875\n",
      "2019-03-27T13:12:55.684818: step 1723, loss 0.523512, acc 0.65625\n",
      "2019-03-27T13:12:55.744756: step 1724, loss 0.445544, acc 0.75\n",
      "2019-03-27T13:12:55.807151: step 1725, loss 0.551569, acc 0.671875\n",
      "2019-03-27T13:12:55.866458: step 1726, loss 0.598119, acc 0.625\n",
      "2019-03-27T13:12:55.928364: step 1727, loss 0.497054, acc 0.71875\n",
      "2019-03-27T13:12:55.990060: step 1728, loss 0.519438, acc 0.71875\n",
      "2019-03-27T13:12:56.051539: step 1729, loss 0.70135, acc 0.65625\n",
      "2019-03-27T13:12:56.110249: step 1730, loss 0.536786, acc 0.71875\n",
      "2019-03-27T13:12:56.171281: step 1731, loss 0.618996, acc 0.578125\n",
      "2019-03-27T13:12:56.234528: step 1732, loss 0.507324, acc 0.65625\n",
      "2019-03-27T13:12:56.297713: step 1733, loss 0.501997, acc 0.703125\n",
      "2019-03-27T13:12:56.363495: step 1734, loss 0.539732, acc 0.671875\n",
      "2019-03-27T13:12:56.423607: step 1735, loss 0.56803, acc 0.609375\n",
      "2019-03-27T13:12:56.482878: step 1736, loss 0.543802, acc 0.65625\n",
      "2019-03-27T13:12:56.544518: step 1737, loss 0.638396, acc 0.65625\n",
      "2019-03-27T13:12:56.605477: step 1738, loss 0.564343, acc 0.609375\n",
      "2019-03-27T13:12:56.665259: step 1739, loss 0.461765, acc 0.734375\n",
      "2019-03-27T13:12:56.724466: step 1740, loss 0.594035, acc 0.65625\n",
      "2019-03-27T13:12:56.781849: step 1741, loss 0.580534, acc 0.640625\n",
      "2019-03-27T13:12:56.842190: step 1742, loss 0.583859, acc 0.65625\n",
      "2019-03-27T13:12:56.901300: step 1743, loss 0.592611, acc 0.59375\n",
      "2019-03-27T13:12:56.963859: step 1744, loss 0.632056, acc 0.65625\n",
      "2019-03-27T13:12:57.022067: step 1745, loss 0.577438, acc 0.625\n",
      "2019-03-27T13:12:57.078589: step 1746, loss 0.511284, acc 0.65625\n",
      "2019-03-27T13:12:57.138034: step 1747, loss 0.484334, acc 0.71875\n",
      "2019-03-27T13:12:57.196187: step 1748, loss 0.577978, acc 0.59375\n",
      "2019-03-27T13:12:57.255084: step 1749, loss 0.657221, acc 0.5625\n",
      "2019-03-27T13:12:57.312929: step 1750, loss 0.625096, acc 0.640625\n",
      "2019-03-27T13:12:57.372092: step 1751, loss 0.549665, acc 0.671875\n",
      "2019-03-27T13:12:57.428457: step 1752, loss 0.600811, acc 0.546875\n",
      "2019-03-27T13:12:57.487997: step 1753, loss 0.461115, acc 0.703125\n",
      "2019-03-27T13:12:57.548084: step 1754, loss 0.553339, acc 0.609375\n",
      "2019-03-27T13:12:57.608150: step 1755, loss 0.573295, acc 0.578125\n",
      "2019-03-27T13:12:57.666214: step 1756, loss 0.515208, acc 0.65625\n",
      "2019-03-27T13:12:57.730437: step 1757, loss 0.530912, acc 0.65625\n",
      "2019-03-27T13:12:57.791530: step 1758, loss 0.547326, acc 0.6875\n",
      "2019-03-27T13:12:57.851068: step 1759, loss 0.580835, acc 0.671875\n",
      "2019-03-27T13:12:57.913155: step 1760, loss 0.639728, acc 0.671875\n",
      "2019-03-27T13:12:57.974883: step 1761, loss 0.463224, acc 0.75\n",
      "2019-03-27T13:12:58.036012: step 1762, loss 0.521477, acc 0.71875\n",
      "2019-03-27T13:12:58.096134: step 1763, loss 0.589056, acc 0.640625\n",
      "2019-03-27T13:12:58.154902: step 1764, loss 0.536464, acc 0.671875\n",
      "2019-03-27T13:12:58.215925: step 1765, loss 0.564244, acc 0.65625\n",
      "2019-03-27T13:12:58.281390: step 1766, loss 0.591088, acc 0.609375\n",
      "2019-03-27T13:12:58.344496: step 1767, loss 0.523234, acc 0.71875\n",
      "2019-03-27T13:12:58.403636: step 1768, loss 0.488291, acc 0.671875\n",
      "2019-03-27T13:12:58.465397: step 1769, loss 0.565206, acc 0.546875\n",
      "2019-03-27T13:12:58.526479: step 1770, loss 0.560072, acc 0.59375\n",
      "2019-03-27T13:12:58.574878: step 1771, loss 0.483164, acc 0.666667\n",
      "2019-03-27T13:12:58.636916: step 1772, loss 0.591463, acc 0.625\n",
      "2019-03-27T13:12:58.694726: step 1773, loss 0.445443, acc 0.703125\n",
      "2019-03-27T13:12:58.756316: step 1774, loss 0.542622, acc 0.671875\n",
      "2019-03-27T13:12:58.815639: step 1775, loss 0.537639, acc 0.640625\n",
      "2019-03-27T13:12:58.877335: step 1776, loss 0.516335, acc 0.75\n",
      "2019-03-27T13:12:58.938645: step 1777, loss 0.518597, acc 0.703125\n",
      "2019-03-27T13:12:59.004241: step 1778, loss 0.460386, acc 0.71875\n",
      "2019-03-27T13:12:59.066137: step 1779, loss 0.606591, acc 0.609375\n",
      "2019-03-27T13:12:59.125387: step 1780, loss 0.671077, acc 0.6875\n",
      "2019-03-27T13:12:59.184545: step 1781, loss 0.564552, acc 0.6875\n",
      "2019-03-27T13:12:59.246071: step 1782, loss 0.53415, acc 0.625\n",
      "2019-03-27T13:12:59.307418: step 1783, loss 0.455793, acc 0.78125\n",
      "2019-03-27T13:12:59.366538: step 1784, loss 0.517058, acc 0.65625\n",
      "2019-03-27T13:12:59.426075: step 1785, loss 0.642699, acc 0.625\n",
      "2019-03-27T13:12:59.488014: step 1786, loss 0.565748, acc 0.5625\n",
      "2019-03-27T13:12:59.547921: step 1787, loss 0.529835, acc 0.6875\n",
      "2019-03-27T13:12:59.613601: step 1788, loss 0.53917, acc 0.65625\n",
      "2019-03-27T13:12:59.673537: step 1789, loss 0.595659, acc 0.671875\n",
      "2019-03-27T13:12:59.733401: step 1790, loss 0.529267, acc 0.671875\n",
      "2019-03-27T13:12:59.792838: step 1791, loss 0.558869, acc 0.59375\n",
      "2019-03-27T13:12:59.852553: step 1792, loss 0.479964, acc 0.734375\n",
      "2019-03-27T13:12:59.915289: step 1793, loss 0.654248, acc 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:12:59.977954: step 1794, loss 0.530991, acc 0.65625\n",
      "2019-03-27T13:13:00.044431: step 1795, loss 0.579018, acc 0.578125\n",
      "2019-03-27T13:13:00.110490: step 1796, loss 0.464885, acc 0.671875\n",
      "2019-03-27T13:13:00.170751: step 1797, loss 0.525481, acc 0.703125\n",
      "2019-03-27T13:13:00.232017: step 1798, loss 0.48987, acc 0.734375\n",
      "2019-03-27T13:13:00.293595: step 1799, loss 0.547371, acc 0.671875\n",
      "2019-03-27T13:13:00.352689: step 1800, loss 0.71203, acc 0.46875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:00.385987: step 1800, loss 0.8802, acc 0.531365\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1800\n",
      "\n",
      "2019-03-27T13:13:00.557399: step 1801, loss 0.54446, acc 0.640625\n",
      "2019-03-27T13:13:00.625866: step 1802, loss 0.484764, acc 0.78125\n",
      "2019-03-27T13:13:00.687201: step 1803, loss 0.503023, acc 0.59375\n",
      "2019-03-27T13:13:00.746829: step 1804, loss 0.466234, acc 0.75\n",
      "2019-03-27T13:13:00.807496: step 1805, loss 0.548719, acc 0.640625\n",
      "2019-03-27T13:13:00.868470: step 1806, loss 0.492397, acc 0.71875\n",
      "2019-03-27T13:13:00.928934: step 1807, loss 0.43491, acc 0.71875\n",
      "2019-03-27T13:13:00.987658: step 1808, loss 0.555407, acc 0.65625\n",
      "2019-03-27T13:13:01.050496: step 1809, loss 0.543113, acc 0.640625\n",
      "2019-03-27T13:13:01.109110: step 1810, loss 0.494574, acc 0.65625\n",
      "2019-03-27T13:13:01.169469: step 1811, loss 0.540162, acc 0.625\n",
      "2019-03-27T13:13:01.230190: step 1812, loss 0.629052, acc 0.59375\n",
      "2019-03-27T13:13:01.291431: step 1813, loss 0.55469, acc 0.671875\n",
      "2019-03-27T13:13:01.353070: step 1814, loss 0.526795, acc 0.6875\n",
      "2019-03-27T13:13:01.412777: step 1815, loss 0.598448, acc 0.625\n",
      "2019-03-27T13:13:01.471079: step 1816, loss 0.518387, acc 0.65625\n",
      "2019-03-27T13:13:01.535726: step 1817, loss 0.590608, acc 0.5625\n",
      "2019-03-27T13:13:01.603212: step 1818, loss 0.479191, acc 0.703125\n",
      "2019-03-27T13:13:01.665408: step 1819, loss 0.589575, acc 0.53125\n",
      "2019-03-27T13:13:01.729264: step 1820, loss 0.562316, acc 0.609375\n",
      "2019-03-27T13:13:01.793356: step 1821, loss 0.566047, acc 0.609375\n",
      "2019-03-27T13:13:01.858758: step 1822, loss 0.459072, acc 0.609375\n",
      "2019-03-27T13:13:01.925294: step 1823, loss 0.409595, acc 0.796875\n",
      "2019-03-27T13:13:01.986856: step 1824, loss 0.548198, acc 0.640625\n",
      "2019-03-27T13:13:02.050407: step 1825, loss 0.564995, acc 0.609375\n",
      "2019-03-27T13:13:02.112975: step 1826, loss 0.552006, acc 0.609375\n",
      "2019-03-27T13:13:02.172998: step 1827, loss 0.525662, acc 0.703125\n",
      "2019-03-27T13:13:02.233824: step 1828, loss 0.556761, acc 0.640625\n",
      "2019-03-27T13:13:02.298507: step 1829, loss 0.542219, acc 0.609375\n",
      "2019-03-27T13:13:02.356835: step 1830, loss 0.53333, acc 0.734375\n",
      "2019-03-27T13:13:02.416754: step 1831, loss 0.552254, acc 0.5625\n",
      "2019-03-27T13:13:02.477312: step 1832, loss 0.532458, acc 0.65625\n",
      "2019-03-27T13:13:02.539672: step 1833, loss 0.524614, acc 0.640625\n",
      "2019-03-27T13:13:02.599363: step 1834, loss 0.457709, acc 0.65625\n",
      "2019-03-27T13:13:02.661732: step 1835, loss 0.428056, acc 0.75\n",
      "2019-03-27T13:13:02.722847: step 1836, loss 0.48072, acc 0.703125\n",
      "2019-03-27T13:13:02.782920: step 1837, loss 0.528766, acc 0.640625\n",
      "2019-03-27T13:13:02.844121: step 1838, loss 0.53294, acc 0.609375\n",
      "2019-03-27T13:13:02.905187: step 1839, loss 0.57093, acc 0.6875\n",
      "2019-03-27T13:13:02.967432: step 1840, loss 0.604133, acc 0.609375\n",
      "2019-03-27T13:13:03.030022: step 1841, loss 0.442931, acc 0.765625\n",
      "2019-03-27T13:13:03.095113: step 1842, loss 0.510198, acc 0.640625\n",
      "2019-03-27T13:13:03.157559: step 1843, loss 0.513387, acc 0.703125\n",
      "2019-03-27T13:13:03.222125: step 1844, loss 0.608062, acc 0.625\n",
      "2019-03-27T13:13:03.282838: step 1845, loss 0.501284, acc 0.65625\n",
      "2019-03-27T13:13:03.343909: step 1846, loss 0.59792, acc 0.65625\n",
      "2019-03-27T13:13:03.402532: step 1847, loss 0.457694, acc 0.703125\n",
      "2019-03-27T13:13:03.452269: step 1848, loss 0.450856, acc 0.761905\n",
      "2019-03-27T13:13:03.514465: step 1849, loss 0.50426, acc 0.671875\n",
      "2019-03-27T13:13:03.573612: step 1850, loss 0.510014, acc 0.703125\n",
      "2019-03-27T13:13:03.633360: step 1851, loss 0.596125, acc 0.640625\n",
      "2019-03-27T13:13:03.697206: step 1852, loss 0.588384, acc 0.53125\n",
      "2019-03-27T13:13:03.755448: step 1853, loss 0.568283, acc 0.671875\n",
      "2019-03-27T13:13:03.818278: step 1854, loss 0.455104, acc 0.75\n",
      "2019-03-27T13:13:03.878544: step 1855, loss 0.593097, acc 0.625\n",
      "2019-03-27T13:13:03.941496: step 1856, loss 0.672164, acc 0.671875\n",
      "2019-03-27T13:13:04.001359: step 1857, loss 0.703768, acc 0.5\n",
      "2019-03-27T13:13:04.062928: step 1858, loss 0.457556, acc 0.78125\n",
      "2019-03-27T13:13:04.121129: step 1859, loss 0.462802, acc 0.78125\n",
      "2019-03-27T13:13:04.180625: step 1860, loss 0.540594, acc 0.59375\n",
      "2019-03-27T13:13:04.240455: step 1861, loss 0.542469, acc 0.65625\n",
      "2019-03-27T13:13:04.306179: step 1862, loss 0.538222, acc 0.65625\n",
      "2019-03-27T13:13:04.373629: step 1863, loss 0.63278, acc 0.5625\n",
      "2019-03-27T13:13:04.430338: step 1864, loss 0.550791, acc 0.671875\n",
      "2019-03-27T13:13:04.489700: step 1865, loss 0.485839, acc 0.703125\n",
      "2019-03-27T13:13:04.550549: step 1866, loss 0.605038, acc 0.5\n",
      "2019-03-27T13:13:04.610826: step 1867, loss 0.637854, acc 0.65625\n",
      "2019-03-27T13:13:04.669240: step 1868, loss 0.591564, acc 0.609375\n",
      "2019-03-27T13:13:04.731711: step 1869, loss 0.612139, acc 0.59375\n",
      "2019-03-27T13:13:04.791245: step 1870, loss 0.520144, acc 0.65625\n",
      "2019-03-27T13:13:04.853251: step 1871, loss 0.504242, acc 0.71875\n",
      "2019-03-27T13:13:04.912955: step 1872, loss 0.497114, acc 0.6875\n",
      "2019-03-27T13:13:04.975540: step 1873, loss 0.493157, acc 0.75\n",
      "2019-03-27T13:13:05.034422: step 1874, loss 0.489167, acc 0.6875\n",
      "2019-03-27T13:13:05.092560: step 1875, loss 0.614307, acc 0.578125\n",
      "2019-03-27T13:13:05.153316: step 1876, loss 0.549323, acc 0.71875\n",
      "2019-03-27T13:13:05.213915: step 1877, loss 0.560182, acc 0.71875\n",
      "2019-03-27T13:13:05.277908: step 1878, loss 0.55896, acc 0.671875\n",
      "2019-03-27T13:13:05.340087: step 1879, loss 0.50506, acc 0.703125\n",
      "2019-03-27T13:13:05.401466: step 1880, loss 0.475343, acc 0.703125\n",
      "2019-03-27T13:13:05.463964: step 1881, loss 0.425612, acc 0.796875\n",
      "2019-03-27T13:13:05.526865: step 1882, loss 0.578706, acc 0.640625\n",
      "2019-03-27T13:13:05.586013: step 1883, loss 0.532326, acc 0.625\n",
      "2019-03-27T13:13:05.648196: step 1884, loss 0.492285, acc 0.734375\n",
      "2019-03-27T13:13:05.710196: step 1885, loss 0.453056, acc 0.71875\n",
      "2019-03-27T13:13:05.774533: step 1886, loss 0.603002, acc 0.609375\n",
      "2019-03-27T13:13:05.837960: step 1887, loss 0.556215, acc 0.640625\n",
      "2019-03-27T13:13:05.896979: step 1888, loss 0.593632, acc 0.5625\n",
      "2019-03-27T13:13:05.960258: step 1889, loss 0.483943, acc 0.625\n",
      "2019-03-27T13:13:06.023164: step 1890, loss 0.553691, acc 0.578125\n",
      "2019-03-27T13:13:06.088143: step 1891, loss 0.65877, acc 0.71875\n",
      "2019-03-27T13:13:06.156096: step 1892, loss 0.548562, acc 0.734375\n",
      "2019-03-27T13:13:06.218846: step 1893, loss 0.579707, acc 0.671875\n",
      "2019-03-27T13:13:06.282247: step 1894, loss 0.544906, acc 0.703125\n",
      "2019-03-27T13:13:06.342034: step 1895, loss 0.600141, acc 0.65625\n",
      "2019-03-27T13:13:06.407253: step 1896, loss 0.544162, acc 0.71875\n",
      "2019-03-27T13:13:06.471476: step 1897, loss 0.522385, acc 0.6875\n",
      "2019-03-27T13:13:06.531994: step 1898, loss 0.433843, acc 0.734375\n",
      "2019-03-27T13:13:06.592107: step 1899, loss 0.513643, acc 0.671875\n",
      "2019-03-27T13:13:06.656156: step 1900, loss 0.50333, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:06.688021: step 1900, loss 0.967816, acc 0.52583\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-1900\n",
      "\n",
      "2019-03-27T13:13:06.862902: step 1901, loss 0.518018, acc 0.703125\n",
      "2019-03-27T13:13:06.925790: step 1902, loss 0.53624, acc 0.703125\n",
      "2019-03-27T13:13:06.990561: step 1903, loss 0.490105, acc 0.671875\n",
      "2019-03-27T13:13:07.054288: step 1904, loss 0.416055, acc 0.78125\n",
      "2019-03-27T13:13:07.113527: step 1905, loss 0.506942, acc 0.71875\n",
      "2019-03-27T13:13:07.175274: step 1906, loss 0.582314, acc 0.625\n",
      "2019-03-27T13:13:07.238666: step 1907, loss 0.405169, acc 0.71875\n",
      "2019-03-27T13:13:07.297655: step 1908, loss 0.533204, acc 0.59375\n",
      "2019-03-27T13:13:07.357533: step 1909, loss 0.611409, acc 0.625\n",
      "2019-03-27T13:13:07.423186: step 1910, loss 0.573255, acc 0.625\n",
      "2019-03-27T13:13:07.484930: step 1911, loss 0.540879, acc 0.59375\n",
      "2019-03-27T13:13:07.545126: step 1912, loss 0.527571, acc 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:13:07.606933: step 1913, loss 0.556317, acc 0.734375\n",
      "2019-03-27T13:13:07.667575: step 1914, loss 0.4692, acc 0.6875\n",
      "2019-03-27T13:13:07.728353: step 1915, loss 0.548804, acc 0.703125\n",
      "2019-03-27T13:13:07.796083: step 1916, loss 0.476846, acc 0.703125\n",
      "2019-03-27T13:13:07.857041: step 1917, loss 0.628344, acc 0.625\n",
      "2019-03-27T13:13:07.922078: step 1918, loss 0.5343, acc 0.640625\n",
      "2019-03-27T13:13:07.980492: step 1919, loss 0.555553, acc 0.6875\n",
      "2019-03-27T13:13:08.043383: step 1920, loss 0.608371, acc 0.59375\n",
      "2019-03-27T13:13:08.106657: step 1921, loss 0.508659, acc 0.65625\n",
      "2019-03-27T13:13:08.168783: step 1922, loss 0.570149, acc 0.625\n",
      "2019-03-27T13:13:08.231720: step 1923, loss 0.503887, acc 0.734375\n",
      "2019-03-27T13:13:08.290305: step 1924, loss 0.542197, acc 0.703125\n",
      "2019-03-27T13:13:08.340418: step 1925, loss 0.46885, acc 0.666667\n",
      "2019-03-27T13:13:08.402956: step 1926, loss 0.628148, acc 0.59375\n",
      "2019-03-27T13:13:08.460099: step 1927, loss 0.617953, acc 0.5625\n",
      "2019-03-27T13:13:08.520117: step 1928, loss 0.549823, acc 0.5625\n",
      "2019-03-27T13:13:08.581572: step 1929, loss 0.520269, acc 0.71875\n",
      "2019-03-27T13:13:08.641683: step 1930, loss 0.729187, acc 0.546875\n",
      "2019-03-27T13:13:08.699738: step 1931, loss 0.512914, acc 0.671875\n",
      "2019-03-27T13:13:08.761855: step 1932, loss 0.51967, acc 0.65625\n",
      "2019-03-27T13:13:08.824805: step 1933, loss 0.502458, acc 0.625\n",
      "2019-03-27T13:13:08.886059: step 1934, loss 0.578249, acc 0.6875\n",
      "2019-03-27T13:13:08.946588: step 1935, loss 0.509806, acc 0.65625\n",
      "2019-03-27T13:13:09.006212: step 1936, loss 0.603808, acc 0.53125\n",
      "2019-03-27T13:13:09.070761: step 1937, loss 0.47003, acc 0.71875\n",
      "2019-03-27T13:13:09.130480: step 1938, loss 0.467338, acc 0.734375\n",
      "2019-03-27T13:13:09.193471: step 1939, loss 0.494417, acc 0.6875\n",
      "2019-03-27T13:13:09.253365: step 1940, loss 0.539699, acc 0.625\n",
      "2019-03-27T13:13:09.314127: step 1941, loss 0.448171, acc 0.78125\n",
      "2019-03-27T13:13:09.374870: step 1942, loss 0.585237, acc 0.5625\n",
      "2019-03-27T13:13:09.435127: step 1943, loss 0.530831, acc 0.671875\n",
      "2019-03-27T13:13:09.494839: step 1944, loss 0.482362, acc 0.75\n",
      "2019-03-27T13:13:09.555508: step 1945, loss 0.519273, acc 0.703125\n",
      "2019-03-27T13:13:09.616687: step 1946, loss 0.505236, acc 0.71875\n",
      "2019-03-27T13:13:09.677094: step 1947, loss 0.526814, acc 0.6875\n",
      "2019-03-27T13:13:09.741175: step 1948, loss 0.463871, acc 0.703125\n",
      "2019-03-27T13:13:09.804356: step 1949, loss 0.567165, acc 0.671875\n",
      "2019-03-27T13:13:09.865994: step 1950, loss 0.589316, acc 0.59375\n",
      "2019-03-27T13:13:09.929512: step 1951, loss 0.639457, acc 0.671875\n",
      "2019-03-27T13:13:09.992800: step 1952, loss 0.502936, acc 0.671875\n",
      "2019-03-27T13:13:10.058122: step 1953, loss 0.555807, acc 0.640625\n",
      "2019-03-27T13:13:10.117730: step 1954, loss 0.614281, acc 0.625\n",
      "2019-03-27T13:13:10.176673: step 1955, loss 0.46358, acc 0.765625\n",
      "2019-03-27T13:13:10.239604: step 1956, loss 0.577444, acc 0.640625\n",
      "2019-03-27T13:13:10.305287: step 1957, loss 0.511637, acc 0.6875\n",
      "2019-03-27T13:13:10.368121: step 1958, loss 0.44333, acc 0.734375\n",
      "2019-03-27T13:13:10.433254: step 1959, loss 0.506828, acc 0.765625\n",
      "2019-03-27T13:13:10.498684: step 1960, loss 0.452765, acc 0.8125\n",
      "2019-03-27T13:13:10.561432: step 1961, loss 0.537957, acc 0.640625\n",
      "2019-03-27T13:13:10.620374: step 1962, loss 0.522372, acc 0.65625\n",
      "2019-03-27T13:13:10.682129: step 1963, loss 0.438385, acc 0.640625\n",
      "2019-03-27T13:13:10.744314: step 1964, loss 0.569968, acc 0.625\n",
      "2019-03-27T13:13:10.803578: step 1965, loss 0.479421, acc 0.71875\n",
      "2019-03-27T13:13:10.863722: step 1966, loss 0.459835, acc 0.703125\n",
      "2019-03-27T13:13:10.923309: step 1967, loss 0.600889, acc 0.671875\n",
      "2019-03-27T13:13:10.982207: step 1968, loss 0.525758, acc 0.6875\n",
      "2019-03-27T13:13:11.044645: step 1969, loss 0.483712, acc 0.59375\n",
      "2019-03-27T13:13:11.103636: step 1970, loss 0.573787, acc 0.578125\n",
      "2019-03-27T13:13:11.166317: step 1971, loss 0.494007, acc 0.734375\n",
      "2019-03-27T13:13:11.228463: step 1972, loss 0.451161, acc 0.671875\n",
      "2019-03-27T13:13:11.290073: step 1973, loss 0.592281, acc 0.59375\n",
      "2019-03-27T13:13:11.352978: step 1974, loss 0.576657, acc 0.609375\n",
      "2019-03-27T13:13:11.417347: step 1975, loss 0.634898, acc 0.625\n",
      "2019-03-27T13:13:11.477908: step 1976, loss 0.546183, acc 0.65625\n",
      "2019-03-27T13:13:11.541381: step 1977, loss 0.443734, acc 0.71875\n",
      "2019-03-27T13:13:11.602610: step 1978, loss 0.428571, acc 0.765625\n",
      "2019-03-27T13:13:11.663939: step 1979, loss 0.564302, acc 0.625\n",
      "2019-03-27T13:13:11.728175: step 1980, loss 0.479187, acc 0.75\n",
      "2019-03-27T13:13:11.792976: step 1981, loss 0.471696, acc 0.671875\n",
      "2019-03-27T13:13:11.853738: step 1982, loss 0.582307, acc 0.625\n",
      "2019-03-27T13:13:11.919102: step 1983, loss 0.419109, acc 0.6875\n",
      "2019-03-27T13:13:11.983292: step 1984, loss 0.567623, acc 0.609375\n",
      "2019-03-27T13:13:12.046704: step 1985, loss 0.538234, acc 0.6875\n",
      "2019-03-27T13:13:12.109112: step 1986, loss 0.508983, acc 0.765625\n",
      "2019-03-27T13:13:12.172436: step 1987, loss 0.526565, acc 0.703125\n",
      "2019-03-27T13:13:12.233125: step 1988, loss 0.489566, acc 0.65625\n",
      "2019-03-27T13:13:12.294417: step 1989, loss 0.491068, acc 0.703125\n",
      "2019-03-27T13:13:12.358078: step 1990, loss 0.558563, acc 0.671875\n",
      "2019-03-27T13:13:12.417236: step 1991, loss 0.491886, acc 0.671875\n",
      "2019-03-27T13:13:12.476719: step 1992, loss 0.576845, acc 0.625\n",
      "2019-03-27T13:13:12.539768: step 1993, loss 0.558151, acc 0.65625\n",
      "2019-03-27T13:13:12.602863: step 1994, loss 0.676459, acc 0.578125\n",
      "2019-03-27T13:13:12.664643: step 1995, loss 0.544149, acc 0.625\n",
      "2019-03-27T13:13:12.724952: step 1996, loss 0.500474, acc 0.59375\n",
      "2019-03-27T13:13:12.788483: step 1997, loss 0.537534, acc 0.703125\n",
      "2019-03-27T13:13:12.849054: step 1998, loss 0.574308, acc 0.671875\n",
      "2019-03-27T13:13:12.910246: step 1999, loss 0.500717, acc 0.6875\n",
      "2019-03-27T13:13:12.974801: step 2000, loss 0.549276, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:13.008964: step 2000, loss 0.897075, acc 0.557196\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2000\n",
      "\n",
      "2019-03-27T13:13:13.185570: step 2001, loss 0.469174, acc 0.765625\n",
      "2019-03-27T13:13:13.236919: step 2002, loss 0.540518, acc 0.666667\n",
      "2019-03-27T13:13:13.297716: step 2003, loss 0.452357, acc 0.71875\n",
      "2019-03-27T13:13:13.357677: step 2004, loss 0.53156, acc 0.640625\n",
      "2019-03-27T13:13:13.418696: step 2005, loss 0.503098, acc 0.609375\n",
      "2019-03-27T13:13:13.482040: step 2006, loss 0.455017, acc 0.703125\n",
      "2019-03-27T13:13:13.542872: step 2007, loss 0.452335, acc 0.75\n",
      "2019-03-27T13:13:13.605616: step 2008, loss 0.524122, acc 0.734375\n",
      "2019-03-27T13:13:13.665223: step 2009, loss 0.458549, acc 0.6875\n",
      "2019-03-27T13:13:13.725634: step 2010, loss 0.478678, acc 0.6875\n",
      "2019-03-27T13:13:13.785947: step 2011, loss 0.477415, acc 0.71875\n",
      "2019-03-27T13:13:13.847334: step 2012, loss 0.59419, acc 0.640625\n",
      "2019-03-27T13:13:13.907932: step 2013, loss 0.552423, acc 0.59375\n",
      "2019-03-27T13:13:13.970306: step 2014, loss 0.549055, acc 0.6875\n",
      "2019-03-27T13:13:14.031761: step 2015, loss 0.434859, acc 0.6875\n",
      "2019-03-27T13:13:14.094168: step 2016, loss 0.570411, acc 0.59375\n",
      "2019-03-27T13:13:14.154894: step 2017, loss 0.438301, acc 0.6875\n",
      "2019-03-27T13:13:14.215159: step 2018, loss 0.534309, acc 0.671875\n",
      "2019-03-27T13:13:14.278478: step 2019, loss 0.551383, acc 0.6875\n",
      "2019-03-27T13:13:14.338436: step 2020, loss 0.473688, acc 0.6875\n",
      "2019-03-27T13:13:14.398103: step 2021, loss 0.487515, acc 0.703125\n",
      "2019-03-27T13:13:14.458700: step 2022, loss 0.553465, acc 0.609375\n",
      "2019-03-27T13:13:14.520168: step 2023, loss 0.515609, acc 0.65625\n",
      "2019-03-27T13:13:14.580198: step 2024, loss 0.498937, acc 0.640625\n",
      "2019-03-27T13:13:14.637044: step 2025, loss 0.577754, acc 0.578125\n",
      "2019-03-27T13:13:14.697422: step 2026, loss 0.490516, acc 0.71875\n",
      "2019-03-27T13:13:14.760732: step 2027, loss 0.566476, acc 0.640625\n",
      "2019-03-27T13:13:14.824259: step 2028, loss 0.528035, acc 0.671875\n",
      "2019-03-27T13:13:14.886958: step 2029, loss 0.56542, acc 0.625\n",
      "2019-03-27T13:13:14.946122: step 2030, loss 0.557213, acc 0.546875\n",
      "2019-03-27T13:13:15.007484: step 2031, loss 0.60059, acc 0.71875\n",
      "2019-03-27T13:13:15.068930: step 2032, loss 0.464846, acc 0.71875\n",
      "2019-03-27T13:13:15.128900: step 2033, loss 0.495229, acc 0.640625\n",
      "2019-03-27T13:13:15.190358: step 2034, loss 0.527136, acc 0.59375\n",
      "2019-03-27T13:13:15.251240: step 2035, loss 0.412038, acc 0.671875\n",
      "2019-03-27T13:13:15.311158: step 2036, loss 0.544609, acc 0.65625\n",
      "2019-03-27T13:13:15.373113: step 2037, loss 0.510161, acc 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:13:15.438938: step 2038, loss 0.461642, acc 0.65625\n",
      "2019-03-27T13:13:15.500858: step 2039, loss 0.619208, acc 0.671875\n",
      "2019-03-27T13:13:15.557914: step 2040, loss 0.500121, acc 0.6875\n",
      "2019-03-27T13:13:15.620665: step 2041, loss 0.486747, acc 0.640625\n",
      "2019-03-27T13:13:15.682372: step 2042, loss 0.48464, acc 0.734375\n",
      "2019-03-27T13:13:15.741372: step 2043, loss 0.625216, acc 0.5625\n",
      "2019-03-27T13:13:15.801089: step 2044, loss 0.506497, acc 0.71875\n",
      "2019-03-27T13:13:15.860512: step 2045, loss 0.615635, acc 0.578125\n",
      "2019-03-27T13:13:15.922121: step 2046, loss 0.499268, acc 0.703125\n",
      "2019-03-27T13:13:15.982388: step 2047, loss 0.504802, acc 0.671875\n",
      "2019-03-27T13:13:16.043019: step 2048, loss 0.395633, acc 0.765625\n",
      "2019-03-27T13:13:16.103170: step 2049, loss 0.597147, acc 0.59375\n",
      "2019-03-27T13:13:16.168485: step 2050, loss 0.516402, acc 0.640625\n",
      "2019-03-27T13:13:16.229222: step 2051, loss 0.554034, acc 0.625\n",
      "2019-03-27T13:13:16.288491: step 2052, loss 0.48829, acc 0.65625\n",
      "2019-03-27T13:13:16.353413: step 2053, loss 0.462997, acc 0.671875\n",
      "2019-03-27T13:13:16.416603: step 2054, loss 0.582719, acc 0.546875\n",
      "2019-03-27T13:13:16.478877: step 2055, loss 0.471892, acc 0.765625\n",
      "2019-03-27T13:13:16.537397: step 2056, loss 0.561959, acc 0.609375\n",
      "2019-03-27T13:13:16.601863: step 2057, loss 0.485117, acc 0.6875\n",
      "2019-03-27T13:13:16.664897: step 2058, loss 0.473389, acc 0.6875\n",
      "2019-03-27T13:13:16.728505: step 2059, loss 0.603006, acc 0.671875\n",
      "2019-03-27T13:13:16.788575: step 2060, loss 0.482188, acc 0.65625\n",
      "2019-03-27T13:13:16.850276: step 2061, loss 0.472969, acc 0.609375\n",
      "2019-03-27T13:13:16.908882: step 2062, loss 0.426911, acc 0.71875\n",
      "2019-03-27T13:13:16.971402: step 2063, loss 0.583042, acc 0.59375\n",
      "2019-03-27T13:13:17.034009: step 2064, loss 0.48709, acc 0.671875\n",
      "2019-03-27T13:13:17.096482: step 2065, loss 0.494842, acc 0.71875\n",
      "2019-03-27T13:13:17.157031: step 2066, loss 0.459341, acc 0.703125\n",
      "2019-03-27T13:13:17.223908: step 2067, loss 0.503396, acc 0.734375\n",
      "2019-03-27T13:13:17.287173: step 2068, loss 0.497381, acc 0.6875\n",
      "2019-03-27T13:13:17.352738: step 2069, loss 0.472635, acc 0.703125\n",
      "2019-03-27T13:13:17.416897: step 2070, loss 0.504687, acc 0.59375\n",
      "2019-03-27T13:13:17.478843: step 2071, loss 0.506162, acc 0.546875\n",
      "2019-03-27T13:13:17.541262: step 2072, loss 0.427235, acc 0.71875\n",
      "2019-03-27T13:13:17.603242: step 2073, loss 0.417924, acc 0.703125\n",
      "2019-03-27T13:13:17.663891: step 2074, loss 0.575114, acc 0.546875\n",
      "2019-03-27T13:13:17.726594: step 2075, loss 0.509955, acc 0.703125\n",
      "2019-03-27T13:13:17.790677: step 2076, loss 0.503179, acc 0.703125\n",
      "2019-03-27T13:13:17.854142: step 2077, loss 0.630712, acc 0.5625\n",
      "2019-03-27T13:13:17.918500: step 2078, loss 0.549388, acc 0.6875\n",
      "2019-03-27T13:13:17.969143: step 2079, loss 0.529689, acc 0.761905\n",
      "2019-03-27T13:13:18.029623: step 2080, loss 0.438994, acc 0.734375\n",
      "2019-03-27T13:13:18.094894: step 2081, loss 0.401013, acc 0.75\n",
      "2019-03-27T13:13:18.158022: step 2082, loss 0.415635, acc 0.734375\n",
      "2019-03-27T13:13:18.219221: step 2083, loss 0.507709, acc 0.75\n",
      "2019-03-27T13:13:18.277127: step 2084, loss 0.473497, acc 0.703125\n",
      "2019-03-27T13:13:18.337029: step 2085, loss 0.586813, acc 0.59375\n",
      "2019-03-27T13:13:18.397294: step 2086, loss 0.530633, acc 0.6875\n",
      "2019-03-27T13:13:18.455801: step 2087, loss 0.414377, acc 0.71875\n",
      "2019-03-27T13:13:18.516405: step 2088, loss 0.536245, acc 0.65625\n",
      "2019-03-27T13:13:18.578010: step 2089, loss 0.537492, acc 0.671875\n",
      "2019-03-27T13:13:18.640150: step 2090, loss 0.479771, acc 0.703125\n",
      "2019-03-27T13:13:18.706832: step 2091, loss 0.465199, acc 0.6875\n",
      "2019-03-27T13:13:18.767899: step 2092, loss 0.485064, acc 0.75\n",
      "2019-03-27T13:13:18.830983: step 2093, loss 0.54472, acc 0.625\n",
      "2019-03-27T13:13:18.889356: step 2094, loss 0.436683, acc 0.71875\n",
      "2019-03-27T13:13:18.949684: step 2095, loss 0.41397, acc 0.71875\n",
      "2019-03-27T13:13:19.015123: step 2096, loss 0.376572, acc 0.703125\n",
      "2019-03-27T13:13:19.074529: step 2097, loss 0.410596, acc 0.71875\n",
      "2019-03-27T13:13:19.134707: step 2098, loss 0.606784, acc 0.625\n",
      "2019-03-27T13:13:19.194821: step 2099, loss 0.477049, acc 0.703125\n",
      "2019-03-27T13:13:19.257153: step 2100, loss 0.62773, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:19.291633: step 2100, loss 1.00715, acc 0.335793\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2100\n",
      "\n",
      "2019-03-27T13:13:19.459329: step 2101, loss 0.599204, acc 0.546875\n",
      "2019-03-27T13:13:19.521551: step 2102, loss 0.572413, acc 0.59375\n",
      "2019-03-27T13:13:19.583429: step 2103, loss 0.576047, acc 0.625\n",
      "2019-03-27T13:13:19.643140: step 2104, loss 0.415409, acc 0.703125\n",
      "2019-03-27T13:13:19.705283: step 2105, loss 0.429319, acc 0.71875\n",
      "2019-03-27T13:13:19.765578: step 2106, loss 0.648486, acc 0.5625\n",
      "2019-03-27T13:13:19.825223: step 2107, loss 0.430172, acc 0.78125\n",
      "2019-03-27T13:13:19.885572: step 2108, loss 0.542222, acc 0.625\n",
      "2019-03-27T13:13:19.945714: step 2109, loss 0.614836, acc 0.5625\n",
      "2019-03-27T13:13:20.010117: step 2110, loss 0.472704, acc 0.703125\n",
      "2019-03-27T13:13:20.072538: step 2111, loss 0.419721, acc 0.78125\n",
      "2019-03-27T13:13:20.137633: step 2112, loss 0.466589, acc 0.78125\n",
      "2019-03-27T13:13:20.196640: step 2113, loss 0.47679, acc 0.6875\n",
      "2019-03-27T13:13:20.266148: step 2114, loss 0.521581, acc 0.78125\n",
      "2019-03-27T13:13:20.328820: step 2115, loss 0.51765, acc 0.625\n",
      "2019-03-27T13:13:20.388133: step 2116, loss 0.550462, acc 0.625\n",
      "2019-03-27T13:13:20.448534: step 2117, loss 0.462808, acc 0.671875\n",
      "2019-03-27T13:13:20.508201: step 2118, loss 0.561668, acc 0.71875\n",
      "2019-03-27T13:13:20.569334: step 2119, loss 0.508828, acc 0.671875\n",
      "2019-03-27T13:13:20.633712: step 2120, loss 0.677086, acc 0.6875\n",
      "2019-03-27T13:13:20.696982: step 2121, loss 0.576798, acc 0.671875\n",
      "2019-03-27T13:13:20.756943: step 2122, loss 0.524495, acc 0.640625\n",
      "2019-03-27T13:13:20.816849: step 2123, loss 0.434492, acc 0.765625\n",
      "2019-03-27T13:13:20.878356: step 2124, loss 0.524634, acc 0.640625\n",
      "2019-03-27T13:13:20.938480: step 2125, loss 0.521671, acc 0.71875\n",
      "2019-03-27T13:13:20.999058: step 2126, loss 0.460305, acc 0.65625\n",
      "2019-03-27T13:13:21.060608: step 2127, loss 0.482386, acc 0.734375\n",
      "2019-03-27T13:13:21.125862: step 2128, loss 0.468599, acc 0.71875\n",
      "2019-03-27T13:13:21.187359: step 2129, loss 0.438864, acc 0.734375\n",
      "2019-03-27T13:13:21.251955: step 2130, loss 0.528159, acc 0.6875\n",
      "2019-03-27T13:13:21.310010: step 2131, loss 0.696714, acc 0.5625\n",
      "2019-03-27T13:13:21.372673: step 2132, loss 0.555581, acc 0.65625\n",
      "2019-03-27T13:13:21.429450: step 2133, loss 0.388731, acc 0.75\n",
      "2019-03-27T13:13:21.492184: step 2134, loss 0.485049, acc 0.625\n",
      "2019-03-27T13:13:21.555286: step 2135, loss 0.460229, acc 0.703125\n",
      "2019-03-27T13:13:21.617328: step 2136, loss 0.57241, acc 0.671875\n",
      "2019-03-27T13:13:21.679312: step 2137, loss 0.412935, acc 0.796875\n",
      "2019-03-27T13:13:21.746365: step 2138, loss 0.601843, acc 0.625\n",
      "2019-03-27T13:13:21.807400: step 2139, loss 0.548438, acc 0.71875\n",
      "2019-03-27T13:13:21.868171: step 2140, loss 0.579781, acc 0.65625\n",
      "2019-03-27T13:13:21.928510: step 2141, loss 0.494581, acc 0.625\n",
      "2019-03-27T13:13:21.993931: step 2142, loss 0.53862, acc 0.671875\n",
      "2019-03-27T13:13:22.055164: step 2143, loss 0.497683, acc 0.609375\n",
      "2019-03-27T13:13:22.118025: step 2144, loss 0.600371, acc 0.609375\n",
      "2019-03-27T13:13:22.178630: step 2145, loss 0.480522, acc 0.71875\n",
      "2019-03-27T13:13:22.242623: step 2146, loss 0.648521, acc 0.625\n",
      "2019-03-27T13:13:22.303297: step 2147, loss 0.563897, acc 0.65625\n",
      "2019-03-27T13:13:22.363843: step 2148, loss 0.622672, acc 0.5625\n",
      "2019-03-27T13:13:22.420152: step 2149, loss 0.566249, acc 0.609375\n",
      "2019-03-27T13:13:22.478989: step 2150, loss 0.483751, acc 0.671875\n",
      "2019-03-27T13:13:22.540271: step 2151, loss 0.420657, acc 0.765625\n",
      "2019-03-27T13:13:22.600898: step 2152, loss 0.433353, acc 0.75\n",
      "2019-03-27T13:13:22.663077: step 2153, loss 0.444486, acc 0.796875\n",
      "2019-03-27T13:13:22.723512: step 2154, loss 0.627048, acc 0.6875\n",
      "2019-03-27T13:13:22.783869: step 2155, loss 0.632182, acc 0.625\n",
      "2019-03-27T13:13:22.831997: step 2156, loss 0.60809, acc 0.666667\n",
      "2019-03-27T13:13:22.895169: step 2157, loss 0.492564, acc 0.703125\n",
      "2019-03-27T13:13:22.956614: step 2158, loss 0.470893, acc 0.734375\n",
      "2019-03-27T13:13:23.019438: step 2159, loss 0.463481, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:13:23.080885: step 2160, loss 0.55756, acc 0.59375\n",
      "2019-03-27T13:13:23.150388: step 2161, loss 0.534227, acc 0.65625\n",
      "2019-03-27T13:13:23.213211: step 2162, loss 0.567592, acc 0.6875\n",
      "2019-03-27T13:13:23.276621: step 2163, loss 0.589815, acc 0.671875\n",
      "2019-03-27T13:13:23.337759: step 2164, loss 0.448961, acc 0.703125\n",
      "2019-03-27T13:13:23.399102: step 2165, loss 0.349229, acc 0.796875\n",
      "2019-03-27T13:13:23.456439: step 2166, loss 0.443685, acc 0.734375\n",
      "2019-03-27T13:13:23.516790: step 2167, loss 0.400363, acc 0.765625\n",
      "2019-03-27T13:13:23.580340: step 2168, loss 0.494079, acc 0.609375\n",
      "2019-03-27T13:13:23.642131: step 2169, loss 0.629674, acc 0.515625\n",
      "2019-03-27T13:13:23.704035: step 2170, loss 0.489818, acc 0.640625\n",
      "2019-03-27T13:13:23.767698: step 2171, loss 0.442471, acc 0.65625\n",
      "2019-03-27T13:13:23.828613: step 2172, loss 0.426318, acc 0.71875\n",
      "2019-03-27T13:13:23.890712: step 2173, loss 0.44847, acc 0.71875\n",
      "2019-03-27T13:13:23.953693: step 2174, loss 0.52364, acc 0.671875\n",
      "2019-03-27T13:13:24.015749: step 2175, loss 0.605857, acc 0.609375\n",
      "2019-03-27T13:13:24.077996: step 2176, loss 0.581012, acc 0.59375\n",
      "2019-03-27T13:13:24.138626: step 2177, loss 0.582158, acc 0.640625\n",
      "2019-03-27T13:13:24.198990: step 2178, loss 0.573905, acc 0.6875\n",
      "2019-03-27T13:13:24.262929: step 2179, loss 0.532822, acc 0.578125\n",
      "2019-03-27T13:13:24.324272: step 2180, loss 0.446813, acc 0.71875\n",
      "2019-03-27T13:13:24.384758: step 2181, loss 0.386254, acc 0.796875\n",
      "2019-03-27T13:13:24.449129: step 2182, loss 0.483438, acc 0.6875\n",
      "2019-03-27T13:13:24.510509: step 2183, loss 0.633918, acc 0.5625\n",
      "2019-03-27T13:13:24.574178: step 2184, loss 0.529321, acc 0.734375\n",
      "2019-03-27T13:13:24.633820: step 2185, loss 0.450008, acc 0.71875\n",
      "2019-03-27T13:13:24.698183: step 2186, loss 0.411243, acc 0.71875\n",
      "2019-03-27T13:13:24.760408: step 2187, loss 0.437606, acc 0.796875\n",
      "2019-03-27T13:13:24.819919: step 2188, loss 0.606137, acc 0.640625\n",
      "2019-03-27T13:13:24.882288: step 2189, loss 0.513107, acc 0.609375\n",
      "2019-03-27T13:13:24.943501: step 2190, loss 0.562301, acc 0.640625\n",
      "2019-03-27T13:13:25.004802: step 2191, loss 0.528254, acc 0.6875\n",
      "2019-03-27T13:13:25.067827: step 2192, loss 0.547507, acc 0.59375\n",
      "2019-03-27T13:13:25.128603: step 2193, loss 0.453471, acc 0.75\n",
      "2019-03-27T13:13:25.189980: step 2194, loss 0.458855, acc 0.671875\n",
      "2019-03-27T13:13:25.253067: step 2195, loss 0.489669, acc 0.6875\n",
      "2019-03-27T13:13:25.316018: step 2196, loss 0.49006, acc 0.71875\n",
      "2019-03-27T13:13:25.379279: step 2197, loss 0.50198, acc 0.6875\n",
      "2019-03-27T13:13:25.438594: step 2198, loss 0.53183, acc 0.734375\n",
      "2019-03-27T13:13:25.503021: step 2199, loss 0.448174, acc 0.71875\n",
      "2019-03-27T13:13:25.567849: step 2200, loss 0.522841, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:25.602401: step 2200, loss 0.861951, acc 0.356089\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2200\n",
      "\n",
      "2019-03-27T13:13:25.767300: step 2201, loss 0.518611, acc 0.609375\n",
      "2019-03-27T13:13:25.829177: step 2202, loss 0.471275, acc 0.71875\n",
      "2019-03-27T13:13:25.892841: step 2203, loss 0.476871, acc 0.640625\n",
      "2019-03-27T13:13:25.950274: step 2204, loss 0.456323, acc 0.6875\n",
      "2019-03-27T13:13:26.010539: step 2205, loss 0.436795, acc 0.703125\n",
      "2019-03-27T13:13:26.073208: step 2206, loss 0.552217, acc 0.65625\n",
      "2019-03-27T13:13:26.131559: step 2207, loss 0.434424, acc 0.703125\n",
      "2019-03-27T13:13:26.192261: step 2208, loss 0.470306, acc 0.609375\n",
      "2019-03-27T13:13:26.252413: step 2209, loss 0.463195, acc 0.734375\n",
      "2019-03-27T13:13:26.314254: step 2210, loss 0.55212, acc 0.6875\n",
      "2019-03-27T13:13:26.375187: step 2211, loss 0.424308, acc 0.765625\n",
      "2019-03-27T13:13:26.436002: step 2212, loss 0.461001, acc 0.703125\n",
      "2019-03-27T13:13:26.498884: step 2213, loss 0.508855, acc 0.65625\n",
      "2019-03-27T13:13:26.556111: step 2214, loss 0.501641, acc 0.671875\n",
      "2019-03-27T13:13:26.617552: step 2215, loss 0.496046, acc 0.71875\n",
      "2019-03-27T13:13:26.678050: step 2216, loss 0.500294, acc 0.671875\n",
      "2019-03-27T13:13:26.737194: step 2217, loss 0.452793, acc 0.703125\n",
      "2019-03-27T13:13:26.799573: step 2218, loss 0.448938, acc 0.734375\n",
      "2019-03-27T13:13:26.862918: step 2219, loss 0.505007, acc 0.6875\n",
      "2019-03-27T13:13:26.924321: step 2220, loss 0.523149, acc 0.703125\n",
      "2019-03-27T13:13:26.983157: step 2221, loss 0.447019, acc 0.8125\n",
      "2019-03-27T13:13:27.041857: step 2222, loss 0.54069, acc 0.65625\n",
      "2019-03-27T13:13:27.103757: step 2223, loss 0.425138, acc 0.703125\n",
      "2019-03-27T13:13:27.165271: step 2224, loss 0.597125, acc 0.59375\n",
      "2019-03-27T13:13:27.226967: step 2225, loss 0.525382, acc 0.71875\n",
      "2019-03-27T13:13:27.286685: step 2226, loss 0.464089, acc 0.703125\n",
      "2019-03-27T13:13:27.345618: step 2227, loss 0.501127, acc 0.640625\n",
      "2019-03-27T13:13:27.407349: step 2228, loss 0.484207, acc 0.6875\n",
      "2019-03-27T13:13:27.468157: step 2229, loss 0.514298, acc 0.671875\n",
      "2019-03-27T13:13:27.529424: step 2230, loss 0.503877, acc 0.703125\n",
      "2019-03-27T13:13:27.591757: step 2231, loss 0.575716, acc 0.65625\n",
      "2019-03-27T13:13:27.656321: step 2232, loss 0.617316, acc 0.609375\n",
      "2019-03-27T13:13:27.708675: step 2233, loss 0.508118, acc 0.571429\n",
      "2019-03-27T13:13:27.773837: step 2234, loss 0.41309, acc 0.765625\n",
      "2019-03-27T13:13:27.835925: step 2235, loss 0.542439, acc 0.625\n",
      "2019-03-27T13:13:27.896767: step 2236, loss 0.547278, acc 0.65625\n",
      "2019-03-27T13:13:27.956680: step 2237, loss 0.517977, acc 0.703125\n",
      "2019-03-27T13:13:28.015214: step 2238, loss 0.585995, acc 0.625\n",
      "2019-03-27T13:13:28.078782: step 2239, loss 0.643792, acc 0.5\n",
      "2019-03-27T13:13:28.138890: step 2240, loss 0.508539, acc 0.734375\n",
      "2019-03-27T13:13:28.200871: step 2241, loss 0.565636, acc 0.5625\n",
      "2019-03-27T13:13:28.259921: step 2242, loss 0.455994, acc 0.625\n",
      "2019-03-27T13:13:28.322223: step 2243, loss 0.522491, acc 0.640625\n",
      "2019-03-27T13:13:28.384581: step 2244, loss 0.396534, acc 0.71875\n",
      "2019-03-27T13:13:28.443687: step 2245, loss 0.570677, acc 0.65625\n",
      "2019-03-27T13:13:28.503517: step 2246, loss 0.497256, acc 0.671875\n",
      "2019-03-27T13:13:28.564717: step 2247, loss 0.401742, acc 0.765625\n",
      "2019-03-27T13:13:28.624373: step 2248, loss 0.458537, acc 0.78125\n",
      "2019-03-27T13:13:28.684218: step 2249, loss 0.665226, acc 0.515625\n",
      "2019-03-27T13:13:28.745213: step 2250, loss 0.467187, acc 0.71875\n",
      "2019-03-27T13:13:28.804167: step 2251, loss 0.469096, acc 0.71875\n",
      "2019-03-27T13:13:28.863083: step 2252, loss 0.545065, acc 0.546875\n",
      "2019-03-27T13:13:28.927413: step 2253, loss 0.510386, acc 0.65625\n",
      "2019-03-27T13:13:28.988384: step 2254, loss 0.461744, acc 0.65625\n",
      "2019-03-27T13:13:29.051546: step 2255, loss 0.471557, acc 0.6875\n",
      "2019-03-27T13:13:29.111565: step 2256, loss 0.481387, acc 0.640625\n",
      "2019-03-27T13:13:29.172335: step 2257, loss 0.489558, acc 0.65625\n",
      "2019-03-27T13:13:29.229681: step 2258, loss 0.48944, acc 0.59375\n",
      "2019-03-27T13:13:29.289499: step 2259, loss 0.4892, acc 0.671875\n",
      "2019-03-27T13:13:29.350962: step 2260, loss 0.544405, acc 0.609375\n",
      "2019-03-27T13:13:29.413295: step 2261, loss 0.36587, acc 0.78125\n",
      "2019-03-27T13:13:29.473359: step 2262, loss 0.548462, acc 0.671875\n",
      "2019-03-27T13:13:29.536246: step 2263, loss 0.464656, acc 0.71875\n",
      "2019-03-27T13:13:29.597341: step 2264, loss 0.399445, acc 0.75\n",
      "2019-03-27T13:13:29.660607: step 2265, loss 0.441592, acc 0.703125\n",
      "2019-03-27T13:13:29.722167: step 2266, loss 0.407975, acc 0.8125\n",
      "2019-03-27T13:13:29.783153: step 2267, loss 0.560895, acc 0.703125\n",
      "2019-03-27T13:13:29.841935: step 2268, loss 0.451347, acc 0.75\n",
      "2019-03-27T13:13:29.901551: step 2269, loss 0.475704, acc 0.703125\n",
      "2019-03-27T13:13:29.960443: step 2270, loss 0.531575, acc 0.703125\n",
      "2019-03-27T13:13:30.018003: step 2271, loss 0.463517, acc 0.65625\n",
      "2019-03-27T13:13:30.077114: step 2272, loss 0.698233, acc 0.453125\n",
      "2019-03-27T13:13:30.138617: step 2273, loss 0.619381, acc 0.578125\n",
      "2019-03-27T13:13:30.200832: step 2274, loss 0.463316, acc 0.765625\n",
      "2019-03-27T13:13:30.265072: step 2275, loss 0.527721, acc 0.640625\n",
      "2019-03-27T13:13:30.328520: step 2276, loss 0.570017, acc 0.65625\n",
      "2019-03-27T13:13:30.388272: step 2277, loss 0.516006, acc 0.6875\n",
      "2019-03-27T13:13:30.449576: step 2278, loss 0.498942, acc 0.703125\n",
      "2019-03-27T13:13:30.507270: step 2279, loss 0.488196, acc 0.671875\n",
      "2019-03-27T13:13:30.569100: step 2280, loss 0.450941, acc 0.703125\n",
      "2019-03-27T13:13:30.630015: step 2281, loss 0.440095, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:13:30.689311: step 2282, loss 0.45526, acc 0.703125\n",
      "2019-03-27T13:13:30.751123: step 2283, loss 0.510295, acc 0.671875\n",
      "2019-03-27T13:13:30.816754: step 2284, loss 0.52301, acc 0.578125\n",
      "2019-03-27T13:13:30.881655: step 2285, loss 0.485275, acc 0.671875\n",
      "2019-03-27T13:13:30.943279: step 2286, loss 0.504544, acc 0.5\n",
      "2019-03-27T13:13:31.004227: step 2287, loss 0.504991, acc 0.640625\n",
      "2019-03-27T13:13:31.064496: step 2288, loss 0.46664, acc 0.734375\n",
      "2019-03-27T13:13:31.125798: step 2289, loss 0.467849, acc 0.625\n",
      "2019-03-27T13:13:31.185418: step 2290, loss 0.481338, acc 0.71875\n",
      "2019-03-27T13:13:31.244769: step 2291, loss 0.456915, acc 0.671875\n",
      "2019-03-27T13:13:31.305062: step 2292, loss 0.581262, acc 0.59375\n",
      "2019-03-27T13:13:31.365207: step 2293, loss 0.47948, acc 0.703125\n",
      "2019-03-27T13:13:31.424821: step 2294, loss 0.491414, acc 0.640625\n",
      "2019-03-27T13:13:31.488801: step 2295, loss 0.526933, acc 0.671875\n",
      "2019-03-27T13:13:31.549091: step 2296, loss 0.451172, acc 0.78125\n",
      "2019-03-27T13:13:31.608467: step 2297, loss 0.588608, acc 0.578125\n",
      "2019-03-27T13:13:31.668404: step 2298, loss 0.54477, acc 0.703125\n",
      "2019-03-27T13:13:31.727670: step 2299, loss 0.576757, acc 0.671875\n",
      "2019-03-27T13:13:31.788375: step 2300, loss 0.651742, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:31.820187: step 2300, loss 1.013, acc 0.52214\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2300\n",
      "\n",
      "2019-03-27T13:13:31.990955: step 2301, loss 0.447839, acc 0.703125\n",
      "2019-03-27T13:13:32.052073: step 2302, loss 0.485716, acc 0.703125\n",
      "2019-03-27T13:13:32.118017: step 2303, loss 0.535626, acc 0.609375\n",
      "2019-03-27T13:13:32.180074: step 2304, loss 0.515521, acc 0.65625\n",
      "2019-03-27T13:13:32.240738: step 2305, loss 0.645045, acc 0.578125\n",
      "2019-03-27T13:13:32.302582: step 2306, loss 0.45706, acc 0.734375\n",
      "2019-03-27T13:13:32.363071: step 2307, loss 0.439682, acc 0.78125\n",
      "2019-03-27T13:13:32.422534: step 2308, loss 0.470409, acc 0.71875\n",
      "2019-03-27T13:13:32.481235: step 2309, loss 0.601522, acc 0.640625\n",
      "2019-03-27T13:13:32.532384: step 2310, loss 0.5479, acc 0.666667\n",
      "2019-03-27T13:13:32.591494: step 2311, loss 0.64719, acc 0.578125\n",
      "2019-03-27T13:13:32.654704: step 2312, loss 0.561032, acc 0.640625\n",
      "2019-03-27T13:13:32.717284: step 2313, loss 0.486831, acc 0.6875\n",
      "2019-03-27T13:13:32.781488: step 2314, loss 0.403043, acc 0.703125\n",
      "2019-03-27T13:13:32.844769: step 2315, loss 0.501063, acc 0.6875\n",
      "2019-03-27T13:13:32.905366: step 2316, loss 0.504785, acc 0.703125\n",
      "2019-03-27T13:13:32.968642: step 2317, loss 0.49259, acc 0.71875\n",
      "2019-03-27T13:13:33.029699: step 2318, loss 0.523832, acc 0.671875\n",
      "2019-03-27T13:13:33.094851: step 2319, loss 0.48782, acc 0.75\n",
      "2019-03-27T13:13:33.157447: step 2320, loss 0.591724, acc 0.59375\n",
      "2019-03-27T13:13:33.228469: step 2321, loss 0.471809, acc 0.671875\n",
      "2019-03-27T13:13:33.286945: step 2322, loss 0.444551, acc 0.6875\n",
      "2019-03-27T13:13:33.346730: step 2323, loss 0.488903, acc 0.703125\n",
      "2019-03-27T13:13:33.407535: step 2324, loss 0.472032, acc 0.734375\n",
      "2019-03-27T13:13:33.470938: step 2325, loss 0.538774, acc 0.625\n",
      "2019-03-27T13:13:33.535110: step 2326, loss 0.467343, acc 0.71875\n",
      "2019-03-27T13:13:33.595309: step 2327, loss 0.504592, acc 0.609375\n",
      "2019-03-27T13:13:33.658204: step 2328, loss 0.495166, acc 0.6875\n",
      "2019-03-27T13:13:33.719679: step 2329, loss 0.499165, acc 0.703125\n",
      "2019-03-27T13:13:33.782312: step 2330, loss 0.492783, acc 0.625\n",
      "2019-03-27T13:13:33.846182: step 2331, loss 0.457037, acc 0.6875\n",
      "2019-03-27T13:13:33.906497: step 2332, loss 0.497946, acc 0.65625\n",
      "2019-03-27T13:13:33.969765: step 2333, loss 0.449808, acc 0.703125\n",
      "2019-03-27T13:13:34.029766: step 2334, loss 0.456827, acc 0.71875\n",
      "2019-03-27T13:13:34.089822: step 2335, loss 0.474255, acc 0.71875\n",
      "2019-03-27T13:13:34.150770: step 2336, loss 0.550113, acc 0.625\n",
      "2019-03-27T13:13:34.214545: step 2337, loss 0.552033, acc 0.609375\n",
      "2019-03-27T13:13:34.275989: step 2338, loss 0.38916, acc 0.828125\n",
      "2019-03-27T13:13:34.336197: step 2339, loss 0.532413, acc 0.609375\n",
      "2019-03-27T13:13:34.395648: step 2340, loss 0.523031, acc 0.765625\n",
      "2019-03-27T13:13:34.461575: step 2341, loss 0.486291, acc 0.734375\n",
      "2019-03-27T13:13:34.526423: step 2342, loss 0.449125, acc 0.71875\n",
      "2019-03-27T13:13:34.586395: step 2343, loss 0.469983, acc 0.703125\n",
      "2019-03-27T13:13:34.646723: step 2344, loss 0.452605, acc 0.71875\n",
      "2019-03-27T13:13:34.710655: step 2345, loss 0.377514, acc 0.75\n",
      "2019-03-27T13:13:34.769902: step 2346, loss 0.507912, acc 0.671875\n",
      "2019-03-27T13:13:34.832539: step 2347, loss 0.485462, acc 0.703125\n",
      "2019-03-27T13:13:34.893988: step 2348, loss 0.400173, acc 0.71875\n",
      "2019-03-27T13:13:34.955690: step 2349, loss 0.542917, acc 0.6875\n",
      "2019-03-27T13:13:35.014717: step 2350, loss 0.494112, acc 0.671875\n",
      "2019-03-27T13:13:35.073569: step 2351, loss 0.446924, acc 0.734375\n",
      "2019-03-27T13:13:35.140988: step 2352, loss 0.379282, acc 0.78125\n",
      "2019-03-27T13:13:35.202189: step 2353, loss 0.450486, acc 0.703125\n",
      "2019-03-27T13:13:35.260330: step 2354, loss 0.512788, acc 0.625\n",
      "2019-03-27T13:13:35.320957: step 2355, loss 0.455767, acc 0.65625\n",
      "2019-03-27T13:13:35.383721: step 2356, loss 0.54013, acc 0.640625\n",
      "2019-03-27T13:13:35.444365: step 2357, loss 0.486915, acc 0.734375\n",
      "2019-03-27T13:13:35.505236: step 2358, loss 0.511852, acc 0.5625\n",
      "2019-03-27T13:13:35.565679: step 2359, loss 0.553161, acc 0.609375\n",
      "2019-03-27T13:13:35.628995: step 2360, loss 0.475281, acc 0.6875\n",
      "2019-03-27T13:13:35.688792: step 2361, loss 0.467623, acc 0.65625\n",
      "2019-03-27T13:13:35.748974: step 2362, loss 0.594039, acc 0.609375\n",
      "2019-03-27T13:13:35.808954: step 2363, loss 0.482925, acc 0.734375\n",
      "2019-03-27T13:13:35.869211: step 2364, loss 0.491856, acc 0.6875\n",
      "2019-03-27T13:13:35.931538: step 2365, loss 0.482851, acc 0.75\n",
      "2019-03-27T13:13:35.997076: step 2366, loss 0.575165, acc 0.609375\n",
      "2019-03-27T13:13:36.061507: step 2367, loss 0.473374, acc 0.71875\n",
      "2019-03-27T13:13:36.130475: step 2368, loss 0.459173, acc 0.703125\n",
      "2019-03-27T13:13:36.189816: step 2369, loss 0.596052, acc 0.578125\n",
      "2019-03-27T13:13:36.251776: step 2370, loss 0.508942, acc 0.734375\n",
      "2019-03-27T13:13:36.315079: step 2371, loss 0.403747, acc 0.78125\n",
      "2019-03-27T13:13:36.374783: step 2372, loss 0.541763, acc 0.609375\n",
      "2019-03-27T13:13:36.440515: step 2373, loss 0.463929, acc 0.6875\n",
      "2019-03-27T13:13:36.502403: step 2374, loss 0.371461, acc 0.8125\n",
      "2019-03-27T13:13:36.566271: step 2375, loss 0.576458, acc 0.6875\n",
      "2019-03-27T13:13:36.630783: step 2376, loss 0.470308, acc 0.65625\n",
      "2019-03-27T13:13:36.690146: step 2377, loss 0.463677, acc 0.625\n",
      "2019-03-27T13:13:36.752342: step 2378, loss 0.524772, acc 0.78125\n",
      "2019-03-27T13:13:36.814923: step 2379, loss 0.506708, acc 0.609375\n",
      "2019-03-27T13:13:36.874007: step 2380, loss 0.539756, acc 0.671875\n",
      "2019-03-27T13:13:36.934813: step 2381, loss 0.413363, acc 0.75\n",
      "2019-03-27T13:13:36.999072: step 2382, loss 0.510043, acc 0.640625\n",
      "2019-03-27T13:13:37.062197: step 2383, loss 0.381225, acc 0.703125\n",
      "2019-03-27T13:13:37.127335: step 2384, loss 0.449884, acc 0.671875\n",
      "2019-03-27T13:13:37.195035: step 2385, loss 0.381161, acc 0.734375\n",
      "2019-03-27T13:13:37.260353: step 2386, loss 0.514956, acc 0.671875\n",
      "2019-03-27T13:13:37.312650: step 2387, loss 0.478479, acc 0.761905\n",
      "2019-03-27T13:13:37.376675: step 2388, loss 0.411473, acc 0.8125\n",
      "2019-03-27T13:13:37.438780: step 2389, loss 0.482, acc 0.6875\n",
      "2019-03-27T13:13:37.498354: step 2390, loss 0.364446, acc 0.8125\n",
      "2019-03-27T13:13:37.557547: step 2391, loss 0.414067, acc 0.75\n",
      "2019-03-27T13:13:37.616101: step 2392, loss 0.451915, acc 0.765625\n",
      "2019-03-27T13:13:37.675685: step 2393, loss 0.435304, acc 0.71875\n",
      "2019-03-27T13:13:37.737754: step 2394, loss 0.505423, acc 0.671875\n",
      "2019-03-27T13:13:37.797227: step 2395, loss 0.519612, acc 0.578125\n",
      "2019-03-27T13:13:37.858544: step 2396, loss 0.476934, acc 0.671875\n",
      "2019-03-27T13:13:37.919179: step 2397, loss 0.478974, acc 0.59375\n",
      "2019-03-27T13:13:37.979041: step 2398, loss 0.450955, acc 0.671875\n",
      "2019-03-27T13:13:38.038898: step 2399, loss 0.537639, acc 0.59375\n",
      "2019-03-27T13:13:38.101462: step 2400, loss 0.501375, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:38.134231: step 2400, loss 0.94532, acc 0.297048\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2400\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:13:38.304879: step 2401, loss 0.441932, acc 0.75\n",
      "2019-03-27T13:13:38.366077: step 2402, loss 0.523156, acc 0.71875\n",
      "2019-03-27T13:13:38.425379: step 2403, loss 0.571893, acc 0.65625\n",
      "2019-03-27T13:13:38.485308: step 2404, loss 0.377633, acc 0.75\n",
      "2019-03-27T13:13:38.545805: step 2405, loss 0.489805, acc 0.6875\n",
      "2019-03-27T13:13:38.607497: step 2406, loss 0.531117, acc 0.703125\n",
      "2019-03-27T13:13:38.668037: step 2407, loss 0.450911, acc 0.734375\n",
      "2019-03-27T13:13:38.728254: step 2408, loss 0.47887, acc 0.6875\n",
      "2019-03-27T13:13:38.789156: step 2409, loss 0.466328, acc 0.640625\n",
      "2019-03-27T13:13:38.850621: step 2410, loss 0.414664, acc 0.734375\n",
      "2019-03-27T13:13:38.913044: step 2411, loss 0.480254, acc 0.671875\n",
      "2019-03-27T13:13:38.976407: step 2412, loss 0.518815, acc 0.578125\n",
      "2019-03-27T13:13:39.039224: step 2413, loss 0.439036, acc 0.71875\n",
      "2019-03-27T13:13:39.099720: step 2414, loss 0.5183, acc 0.671875\n",
      "2019-03-27T13:13:39.158975: step 2415, loss 0.522395, acc 0.671875\n",
      "2019-03-27T13:13:39.218017: step 2416, loss 0.509355, acc 0.671875\n",
      "2019-03-27T13:13:39.278958: step 2417, loss 0.475718, acc 0.75\n",
      "2019-03-27T13:13:39.340887: step 2418, loss 0.508568, acc 0.625\n",
      "2019-03-27T13:13:39.400190: step 2419, loss 0.468441, acc 0.6875\n",
      "2019-03-27T13:13:39.460545: step 2420, loss 0.358269, acc 0.75\n",
      "2019-03-27T13:13:39.520207: step 2421, loss 0.494786, acc 0.703125\n",
      "2019-03-27T13:13:39.583070: step 2422, loss 0.479292, acc 0.640625\n",
      "2019-03-27T13:13:39.640378: step 2423, loss 0.531916, acc 0.59375\n",
      "2019-03-27T13:13:39.699993: step 2424, loss 0.487705, acc 0.671875\n",
      "2019-03-27T13:13:39.760114: step 2425, loss 0.601744, acc 0.671875\n",
      "2019-03-27T13:13:39.823603: step 2426, loss 0.58429, acc 0.640625\n",
      "2019-03-27T13:13:39.883679: step 2427, loss 0.515324, acc 0.78125\n",
      "2019-03-27T13:13:39.942148: step 2428, loss 0.439795, acc 0.71875\n",
      "2019-03-27T13:13:40.005438: step 2429, loss 0.487104, acc 0.71875\n",
      "2019-03-27T13:13:40.064160: step 2430, loss 0.542465, acc 0.640625\n",
      "2019-03-27T13:13:40.124321: step 2431, loss 0.462172, acc 0.703125\n",
      "2019-03-27T13:13:40.183444: step 2432, loss 0.465387, acc 0.671875\n",
      "2019-03-27T13:13:40.244240: step 2433, loss 0.427244, acc 0.765625\n",
      "2019-03-27T13:13:40.303791: step 2434, loss 0.447431, acc 0.671875\n",
      "2019-03-27T13:13:40.364416: step 2435, loss 0.388768, acc 0.75\n",
      "2019-03-27T13:13:40.422699: step 2436, loss 0.529368, acc 0.640625\n",
      "2019-03-27T13:13:40.483432: step 2437, loss 0.53227, acc 0.65625\n",
      "2019-03-27T13:13:40.546404: step 2438, loss 0.492055, acc 0.65625\n",
      "2019-03-27T13:13:40.606515: step 2439, loss 0.514648, acc 0.671875\n",
      "2019-03-27T13:13:40.669599: step 2440, loss 0.443478, acc 0.671875\n",
      "2019-03-27T13:13:40.729762: step 2441, loss 0.620639, acc 0.625\n",
      "2019-03-27T13:13:40.794894: step 2442, loss 0.51171, acc 0.6875\n",
      "2019-03-27T13:13:40.855476: step 2443, loss 0.540117, acc 0.71875\n",
      "2019-03-27T13:13:40.921319: step 2444, loss 0.484976, acc 0.703125\n",
      "2019-03-27T13:13:40.978928: step 2445, loss 0.668623, acc 0.640625\n",
      "2019-03-27T13:13:41.041747: step 2446, loss 0.505892, acc 0.6875\n",
      "2019-03-27T13:13:41.106604: step 2447, loss 0.408219, acc 0.78125\n",
      "2019-03-27T13:13:41.166916: step 2448, loss 0.587694, acc 0.625\n",
      "2019-03-27T13:13:41.227208: step 2449, loss 0.513202, acc 0.703125\n",
      "2019-03-27T13:13:41.283507: step 2450, loss 0.503511, acc 0.640625\n",
      "2019-03-27T13:13:41.344047: step 2451, loss 0.341699, acc 0.828125\n",
      "2019-03-27T13:13:41.407577: step 2452, loss 0.47335, acc 0.703125\n",
      "2019-03-27T13:13:41.470592: step 2453, loss 0.546081, acc 0.640625\n",
      "2019-03-27T13:13:41.530862: step 2454, loss 0.492908, acc 0.640625\n",
      "2019-03-27T13:13:41.591542: step 2455, loss 0.458404, acc 0.71875\n",
      "2019-03-27T13:13:41.652585: step 2456, loss 0.448381, acc 0.71875\n",
      "2019-03-27T13:13:41.712772: step 2457, loss 0.414775, acc 0.8125\n",
      "2019-03-27T13:13:41.775100: step 2458, loss 0.475293, acc 0.734375\n",
      "2019-03-27T13:13:41.835797: step 2459, loss 0.483619, acc 0.703125\n",
      "2019-03-27T13:13:41.896479: step 2460, loss 0.620543, acc 0.625\n",
      "2019-03-27T13:13:41.955702: step 2461, loss 0.5124, acc 0.640625\n",
      "2019-03-27T13:13:42.017888: step 2462, loss 0.545307, acc 0.609375\n",
      "2019-03-27T13:13:42.080269: step 2463, loss 0.532991, acc 0.609375\n",
      "2019-03-27T13:13:42.129090: step 2464, loss 0.490574, acc 0.666667\n",
      "2019-03-27T13:13:42.190035: step 2465, loss 0.367832, acc 0.8125\n",
      "2019-03-27T13:13:42.252777: step 2466, loss 0.498594, acc 0.65625\n",
      "2019-03-27T13:13:42.316834: step 2467, loss 0.417115, acc 0.765625\n",
      "2019-03-27T13:13:42.376302: step 2468, loss 0.478651, acc 0.6875\n",
      "2019-03-27T13:13:42.438470: step 2469, loss 0.597572, acc 0.703125\n",
      "2019-03-27T13:13:42.499377: step 2470, loss 0.52278, acc 0.71875\n",
      "2019-03-27T13:13:42.560202: step 2471, loss 0.402101, acc 0.75\n",
      "2019-03-27T13:13:42.619857: step 2472, loss 0.407198, acc 0.75\n",
      "2019-03-27T13:13:42.683659: step 2473, loss 0.499634, acc 0.6875\n",
      "2019-03-27T13:13:42.744076: step 2474, loss 0.446494, acc 0.734375\n",
      "2019-03-27T13:13:42.804991: step 2475, loss 0.522516, acc 0.6875\n",
      "2019-03-27T13:13:42.864442: step 2476, loss 0.462145, acc 0.6875\n",
      "2019-03-27T13:13:42.923462: step 2477, loss 0.452698, acc 0.71875\n",
      "2019-03-27T13:13:42.983460: step 2478, loss 0.397577, acc 0.765625\n",
      "2019-03-27T13:13:43.044425: step 2479, loss 0.552425, acc 0.640625\n",
      "2019-03-27T13:13:43.104199: step 2480, loss 0.564746, acc 0.703125\n",
      "2019-03-27T13:13:43.164783: step 2481, loss 0.391256, acc 0.71875\n",
      "2019-03-27T13:13:43.231497: step 2482, loss 0.466045, acc 0.6875\n",
      "2019-03-27T13:13:43.293247: step 2483, loss 0.450559, acc 0.734375\n",
      "2019-03-27T13:13:43.354007: step 2484, loss 0.572555, acc 0.625\n",
      "2019-03-27T13:13:43.416274: step 2485, loss 0.493831, acc 0.6875\n",
      "2019-03-27T13:13:43.477448: step 2486, loss 0.480624, acc 0.703125\n",
      "2019-03-27T13:13:43.537483: step 2487, loss 0.539413, acc 0.625\n",
      "2019-03-27T13:13:43.597661: step 2488, loss 0.51953, acc 0.640625\n",
      "2019-03-27T13:13:43.657674: step 2489, loss 0.514492, acc 0.65625\n",
      "2019-03-27T13:13:43.722135: step 2490, loss 0.47893, acc 0.71875\n",
      "2019-03-27T13:13:43.786499: step 2491, loss 0.399542, acc 0.828125\n",
      "2019-03-27T13:13:43.852515: step 2492, loss 0.449935, acc 0.703125\n",
      "2019-03-27T13:13:43.913218: step 2493, loss 0.477696, acc 0.625\n",
      "2019-03-27T13:13:43.976307: step 2494, loss 0.410649, acc 0.71875\n",
      "2019-03-27T13:13:44.039009: step 2495, loss 0.36853, acc 0.765625\n",
      "2019-03-27T13:13:44.098886: step 2496, loss 0.447306, acc 0.671875\n",
      "2019-03-27T13:13:44.161681: step 2497, loss 0.638522, acc 0.578125\n",
      "2019-03-27T13:13:44.221519: step 2498, loss 0.52687, acc 0.640625\n",
      "2019-03-27T13:13:44.284155: step 2499, loss 0.490962, acc 0.65625\n",
      "2019-03-27T13:13:44.345821: step 2500, loss 0.538658, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:44.381876: step 2500, loss 0.913524, acc 0.396679\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2500\n",
      "\n",
      "2019-03-27T13:13:44.552381: step 2501, loss 0.398358, acc 0.78125\n",
      "2019-03-27T13:13:44.611875: step 2502, loss 0.49633, acc 0.65625\n",
      "2019-03-27T13:13:44.672837: step 2503, loss 0.481779, acc 0.640625\n",
      "2019-03-27T13:13:44.738267: step 2504, loss 0.478446, acc 0.734375\n",
      "2019-03-27T13:13:44.800799: step 2505, loss 0.442669, acc 0.703125\n",
      "2019-03-27T13:13:44.863811: step 2506, loss 0.491969, acc 0.6875\n",
      "2019-03-27T13:13:44.925975: step 2507, loss 0.444709, acc 0.765625\n",
      "2019-03-27T13:13:44.989796: step 2508, loss 0.42975, acc 0.625\n",
      "2019-03-27T13:13:45.051131: step 2509, loss 0.409506, acc 0.71875\n",
      "2019-03-27T13:13:45.110213: step 2510, loss 0.46684, acc 0.703125\n",
      "2019-03-27T13:13:45.169044: step 2511, loss 0.492018, acc 0.640625\n",
      "2019-03-27T13:13:45.229447: step 2512, loss 0.495812, acc 0.625\n",
      "2019-03-27T13:13:45.293517: step 2513, loss 0.462607, acc 0.65625\n",
      "2019-03-27T13:13:45.355417: step 2514, loss 0.542285, acc 0.671875\n",
      "2019-03-27T13:13:45.416859: step 2515, loss 0.491223, acc 0.65625\n",
      "2019-03-27T13:13:45.479690: step 2516, loss 0.450289, acc 0.671875\n",
      "2019-03-27T13:13:45.539313: step 2517, loss 0.64518, acc 0.671875\n",
      "2019-03-27T13:13:45.596226: step 2518, loss 0.531564, acc 0.625\n",
      "2019-03-27T13:13:45.659022: step 2519, loss 0.433994, acc 0.734375\n",
      "2019-03-27T13:13:45.724828: step 2520, loss 0.538456, acc 0.6875\n",
      "2019-03-27T13:13:45.784957: step 2521, loss 0.503765, acc 0.671875\n",
      "2019-03-27T13:13:45.845339: step 2522, loss 0.481052, acc 0.703125\n",
      "2019-03-27T13:13:45.908898: step 2523, loss 0.593591, acc 0.703125\n",
      "2019-03-27T13:13:45.967536: step 2524, loss 0.467825, acc 0.765625\n",
      "2019-03-27T13:13:46.024907: step 2525, loss 0.52338, acc 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:13:46.087084: step 2526, loss 0.472454, acc 0.734375\n",
      "2019-03-27T13:13:46.149274: step 2527, loss 0.461126, acc 0.71875\n",
      "2019-03-27T13:13:46.209168: step 2528, loss 0.452999, acc 0.6875\n",
      "2019-03-27T13:13:46.268504: step 2529, loss 0.529193, acc 0.65625\n",
      "2019-03-27T13:13:46.331625: step 2530, loss 0.519682, acc 0.6875\n",
      "2019-03-27T13:13:46.392624: step 2531, loss 0.537614, acc 0.640625\n",
      "2019-03-27T13:13:46.454182: step 2532, loss 0.498431, acc 0.65625\n",
      "2019-03-27T13:13:46.516333: step 2533, loss 0.419296, acc 0.703125\n",
      "2019-03-27T13:13:46.576086: step 2534, loss 0.483361, acc 0.703125\n",
      "2019-03-27T13:13:46.637416: step 2535, loss 0.478499, acc 0.6875\n",
      "2019-03-27T13:13:46.701514: step 2536, loss 0.41647, acc 0.703125\n",
      "2019-03-27T13:13:46.762975: step 2537, loss 0.520422, acc 0.609375\n",
      "2019-03-27T13:13:46.823914: step 2538, loss 0.47997, acc 0.625\n",
      "2019-03-27T13:13:46.885795: step 2539, loss 0.465825, acc 0.703125\n",
      "2019-03-27T13:13:46.946446: step 2540, loss 0.478423, acc 0.703125\n",
      "2019-03-27T13:13:46.998417: step 2541, loss 0.642066, acc 0.428571\n",
      "2019-03-27T13:13:47.059012: step 2542, loss 0.411387, acc 0.734375\n",
      "2019-03-27T13:13:47.121109: step 2543, loss 0.476973, acc 0.671875\n",
      "2019-03-27T13:13:47.183737: step 2544, loss 0.475582, acc 0.65625\n",
      "2019-03-27T13:13:47.244482: step 2545, loss 0.51452, acc 0.640625\n",
      "2019-03-27T13:13:47.305056: step 2546, loss 0.435863, acc 0.65625\n",
      "2019-03-27T13:13:47.367706: step 2547, loss 0.545088, acc 0.65625\n",
      "2019-03-27T13:13:47.431964: step 2548, loss 0.385864, acc 0.734375\n",
      "2019-03-27T13:13:47.495962: step 2549, loss 0.50127, acc 0.609375\n",
      "2019-03-27T13:13:47.558042: step 2550, loss 0.475958, acc 0.6875\n",
      "2019-03-27T13:13:47.618398: step 2551, loss 0.364803, acc 0.78125\n",
      "2019-03-27T13:13:47.682015: step 2552, loss 0.367621, acc 0.796875\n",
      "2019-03-27T13:13:47.741111: step 2553, loss 0.458764, acc 0.671875\n",
      "2019-03-27T13:13:47.798775: step 2554, loss 0.436171, acc 0.75\n",
      "2019-03-27T13:13:47.860429: step 2555, loss 0.47091, acc 0.6875\n",
      "2019-03-27T13:13:47.921714: step 2556, loss 0.460685, acc 0.640625\n",
      "2019-03-27T13:13:47.983333: step 2557, loss 0.483803, acc 0.65625\n",
      "2019-03-27T13:13:48.043603: step 2558, loss 0.446124, acc 0.75\n",
      "2019-03-27T13:13:48.106958: step 2559, loss 0.444035, acc 0.671875\n",
      "2019-03-27T13:13:48.167125: step 2560, loss 0.457393, acc 0.71875\n",
      "2019-03-27T13:13:48.226562: step 2561, loss 0.560754, acc 0.609375\n",
      "2019-03-27T13:13:48.290929: step 2562, loss 0.487246, acc 0.671875\n",
      "2019-03-27T13:13:48.351193: step 2563, loss 0.458188, acc 0.71875\n",
      "2019-03-27T13:13:48.412961: step 2564, loss 0.507845, acc 0.671875\n",
      "2019-03-27T13:13:48.472845: step 2565, loss 0.515199, acc 0.734375\n",
      "2019-03-27T13:13:48.533836: step 2566, loss 0.480876, acc 0.609375\n",
      "2019-03-27T13:13:48.592307: step 2567, loss 0.500201, acc 0.65625\n",
      "2019-03-27T13:13:48.654559: step 2568, loss 0.501861, acc 0.625\n",
      "2019-03-27T13:13:48.716564: step 2569, loss 0.54013, acc 0.625\n",
      "2019-03-27T13:13:48.776627: step 2570, loss 0.546285, acc 0.5625\n",
      "2019-03-27T13:13:48.838211: step 2571, loss 0.459757, acc 0.734375\n",
      "2019-03-27T13:13:48.899341: step 2572, loss 0.57953, acc 0.609375\n",
      "2019-03-27T13:13:48.960382: step 2573, loss 0.577367, acc 0.59375\n",
      "2019-03-27T13:13:49.021632: step 2574, loss 0.427657, acc 0.734375\n",
      "2019-03-27T13:13:49.085236: step 2575, loss 0.365271, acc 0.765625\n",
      "2019-03-27T13:13:49.147426: step 2576, loss 0.392061, acc 0.703125\n",
      "2019-03-27T13:13:49.208905: step 2577, loss 0.596326, acc 0.578125\n",
      "2019-03-27T13:13:49.269161: step 2578, loss 0.566412, acc 0.625\n",
      "2019-03-27T13:13:49.330009: step 2579, loss 0.534139, acc 0.671875\n",
      "2019-03-27T13:13:49.389940: step 2580, loss 0.523813, acc 0.65625\n",
      "2019-03-27T13:13:49.449487: step 2581, loss 0.438327, acc 0.71875\n",
      "2019-03-27T13:13:49.510617: step 2582, loss 0.60175, acc 0.65625\n",
      "2019-03-27T13:13:49.570003: step 2583, loss 0.614235, acc 0.71875\n",
      "2019-03-27T13:13:49.630974: step 2584, loss 0.51728, acc 0.625\n",
      "2019-03-27T13:13:49.693795: step 2585, loss 0.37678, acc 0.796875\n",
      "2019-03-27T13:13:49.754496: step 2586, loss 0.60079, acc 0.59375\n",
      "2019-03-27T13:13:49.817143: step 2587, loss 0.505929, acc 0.6875\n",
      "2019-03-27T13:13:49.879130: step 2588, loss 0.53714, acc 0.640625\n",
      "2019-03-27T13:13:49.942032: step 2589, loss 0.478991, acc 0.625\n",
      "2019-03-27T13:13:50.001487: step 2590, loss 0.571131, acc 0.609375\n",
      "2019-03-27T13:13:50.063483: step 2591, loss 0.485168, acc 0.671875\n",
      "2019-03-27T13:13:50.125089: step 2592, loss 0.630619, acc 0.65625\n",
      "2019-03-27T13:13:50.184337: step 2593, loss 0.560304, acc 0.59375\n",
      "2019-03-27T13:13:50.245584: step 2594, loss 0.650169, acc 0.609375\n",
      "2019-03-27T13:13:50.308961: step 2595, loss 0.544054, acc 0.609375\n",
      "2019-03-27T13:13:50.374254: step 2596, loss 0.423511, acc 0.734375\n",
      "2019-03-27T13:13:50.434761: step 2597, loss 0.535805, acc 0.5625\n",
      "2019-03-27T13:13:50.494869: step 2598, loss 0.407787, acc 0.765625\n",
      "2019-03-27T13:13:50.554280: step 2599, loss 0.474217, acc 0.6875\n",
      "2019-03-27T13:13:50.619997: step 2600, loss 0.469939, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:50.653420: step 2600, loss 1.04859, acc 0.516605\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2600\n",
      "\n",
      "2019-03-27T13:13:50.818341: step 2601, loss 0.43346, acc 0.734375\n",
      "2019-03-27T13:13:50.883570: step 2602, loss 0.554226, acc 0.671875\n",
      "2019-03-27T13:13:50.945933: step 2603, loss 0.450608, acc 0.734375\n",
      "2019-03-27T13:13:51.007381: step 2604, loss 0.377011, acc 0.796875\n",
      "2019-03-27T13:13:51.068010: step 2605, loss 0.380337, acc 0.734375\n",
      "2019-03-27T13:13:51.127991: step 2606, loss 0.460768, acc 0.765625\n",
      "2019-03-27T13:13:51.193386: step 2607, loss 0.518914, acc 0.640625\n",
      "2019-03-27T13:13:51.255750: step 2608, loss 0.592715, acc 0.6875\n",
      "2019-03-27T13:13:51.316581: step 2609, loss 0.522051, acc 0.6875\n",
      "2019-03-27T13:13:51.375971: step 2610, loss 0.483811, acc 0.671875\n",
      "2019-03-27T13:13:51.434929: step 2611, loss 0.484593, acc 0.71875\n",
      "2019-03-27T13:13:51.498767: step 2612, loss 0.526749, acc 0.65625\n",
      "2019-03-27T13:13:51.560485: step 2613, loss 0.51506, acc 0.71875\n",
      "2019-03-27T13:13:51.620972: step 2614, loss 0.509581, acc 0.625\n",
      "2019-03-27T13:13:51.680785: step 2615, loss 0.483389, acc 0.671875\n",
      "2019-03-27T13:13:51.739424: step 2616, loss 0.484742, acc 0.65625\n",
      "2019-03-27T13:13:51.799354: step 2617, loss 0.421773, acc 0.703125\n",
      "2019-03-27T13:13:51.849954: step 2618, loss 0.376405, acc 0.666667\n",
      "2019-03-27T13:13:51.910346: step 2619, loss 0.420017, acc 0.75\n",
      "2019-03-27T13:13:51.974964: step 2620, loss 0.492083, acc 0.6875\n",
      "2019-03-27T13:13:52.034151: step 2621, loss 0.518746, acc 0.65625\n",
      "2019-03-27T13:13:52.095108: step 2622, loss 0.387487, acc 0.75\n",
      "2019-03-27T13:13:52.156766: step 2623, loss 0.450996, acc 0.78125\n",
      "2019-03-27T13:13:52.217019: step 2624, loss 0.469232, acc 0.796875\n",
      "2019-03-27T13:13:52.281605: step 2625, loss 0.353734, acc 0.875\n",
      "2019-03-27T13:13:52.342802: step 2626, loss 0.508353, acc 0.625\n",
      "2019-03-27T13:13:52.402252: step 2627, loss 0.609397, acc 0.59375\n",
      "2019-03-27T13:13:52.460119: step 2628, loss 0.54637, acc 0.625\n",
      "2019-03-27T13:13:52.521031: step 2629, loss 0.507703, acc 0.609375\n",
      "2019-03-27T13:13:52.581343: step 2630, loss 0.431208, acc 0.734375\n",
      "2019-03-27T13:13:52.648427: step 2631, loss 0.498079, acc 0.671875\n",
      "2019-03-27T13:13:52.713998: step 2632, loss 0.507401, acc 0.609375\n",
      "2019-03-27T13:13:52.774558: step 2633, loss 0.489482, acc 0.703125\n",
      "2019-03-27T13:13:52.836080: step 2634, loss 0.470562, acc 0.640625\n",
      "2019-03-27T13:13:52.897354: step 2635, loss 0.511051, acc 0.6875\n",
      "2019-03-27T13:13:52.954403: step 2636, loss 0.404859, acc 0.734375\n",
      "2019-03-27T13:13:53.011823: step 2637, loss 0.4903, acc 0.671875\n",
      "2019-03-27T13:13:53.076353: step 2638, loss 0.437096, acc 0.6875\n",
      "2019-03-27T13:13:53.137037: step 2639, loss 0.448357, acc 0.71875\n",
      "2019-03-27T13:13:53.201176: step 2640, loss 0.429169, acc 0.734375\n",
      "2019-03-27T13:13:53.260417: step 2641, loss 0.515367, acc 0.640625\n",
      "2019-03-27T13:13:53.319636: step 2642, loss 0.489982, acc 0.765625\n",
      "2019-03-27T13:13:53.384021: step 2643, loss 0.508829, acc 0.6875\n",
      "2019-03-27T13:13:53.443398: step 2644, loss 0.424269, acc 0.703125\n",
      "2019-03-27T13:13:53.502698: step 2645, loss 0.432108, acc 0.671875\n",
      "2019-03-27T13:13:53.565105: step 2646, loss 0.499999, acc 0.625\n",
      "2019-03-27T13:13:53.623379: step 2647, loss 0.463307, acc 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:13:53.682987: step 2648, loss 0.46699, acc 0.6875\n",
      "2019-03-27T13:13:53.739359: step 2649, loss 0.455118, acc 0.640625\n",
      "2019-03-27T13:13:53.798622: step 2650, loss 0.475058, acc 0.640625\n",
      "2019-03-27T13:13:53.861254: step 2651, loss 0.651472, acc 0.6875\n",
      "2019-03-27T13:13:53.924776: step 2652, loss 0.532004, acc 0.640625\n",
      "2019-03-27T13:13:53.985287: step 2653, loss 0.496118, acc 0.609375\n",
      "2019-03-27T13:13:54.042658: step 2654, loss 0.523108, acc 0.65625\n",
      "2019-03-27T13:13:54.108003: step 2655, loss 0.440414, acc 0.6875\n",
      "2019-03-27T13:13:54.170819: step 2656, loss 0.583026, acc 0.609375\n",
      "2019-03-27T13:13:54.231813: step 2657, loss 0.502947, acc 0.671875\n",
      "2019-03-27T13:13:54.300208: step 2658, loss 0.449034, acc 0.75\n",
      "2019-03-27T13:13:54.363165: step 2659, loss 0.484651, acc 0.609375\n",
      "2019-03-27T13:13:54.422473: step 2660, loss 0.404757, acc 0.703125\n",
      "2019-03-27T13:13:54.481632: step 2661, loss 0.391776, acc 0.8125\n",
      "2019-03-27T13:13:54.546299: step 2662, loss 0.495271, acc 0.6875\n",
      "2019-03-27T13:13:54.604064: step 2663, loss 0.425559, acc 0.78125\n",
      "2019-03-27T13:13:54.663260: step 2664, loss 0.49725, acc 0.65625\n",
      "2019-03-27T13:13:54.722090: step 2665, loss 0.440638, acc 0.734375\n",
      "2019-03-27T13:13:54.782335: step 2666, loss 0.411244, acc 0.765625\n",
      "2019-03-27T13:13:54.845449: step 2667, loss 0.493781, acc 0.65625\n",
      "2019-03-27T13:13:54.908602: step 2668, loss 0.523863, acc 0.765625\n",
      "2019-03-27T13:13:54.970720: step 2669, loss 0.486986, acc 0.6875\n",
      "2019-03-27T13:13:55.031353: step 2670, loss 0.463077, acc 0.703125\n",
      "2019-03-27T13:13:55.090712: step 2671, loss 0.428769, acc 0.625\n",
      "2019-03-27T13:13:55.152325: step 2672, loss 0.560095, acc 0.703125\n",
      "2019-03-27T13:13:55.212992: step 2673, loss 0.497215, acc 0.640625\n",
      "2019-03-27T13:13:55.273176: step 2674, loss 0.507681, acc 0.6875\n",
      "2019-03-27T13:13:55.333868: step 2675, loss 0.422756, acc 0.6875\n",
      "2019-03-27T13:13:55.395138: step 2676, loss 0.376188, acc 0.75\n",
      "2019-03-27T13:13:55.455938: step 2677, loss 0.454387, acc 0.734375\n",
      "2019-03-27T13:13:55.517127: step 2678, loss 0.380842, acc 0.703125\n",
      "2019-03-27T13:13:55.578582: step 2679, loss 0.439137, acc 0.65625\n",
      "2019-03-27T13:13:55.641397: step 2680, loss 0.417844, acc 0.65625\n",
      "2019-03-27T13:13:55.703288: step 2681, loss 0.516392, acc 0.71875\n",
      "2019-03-27T13:13:55.762700: step 2682, loss 0.4894, acc 0.671875\n",
      "2019-03-27T13:13:55.823098: step 2683, loss 0.412866, acc 0.75\n",
      "2019-03-27T13:13:55.883102: step 2684, loss 0.492855, acc 0.671875\n",
      "2019-03-27T13:13:55.941923: step 2685, loss 0.411337, acc 0.6875\n",
      "2019-03-27T13:13:56.001354: step 2686, loss 0.550824, acc 0.59375\n",
      "2019-03-27T13:13:56.064024: step 2687, loss 0.537355, acc 0.734375\n",
      "2019-03-27T13:13:56.129456: step 2688, loss 0.449478, acc 0.734375\n",
      "2019-03-27T13:13:56.188858: step 2689, loss 0.507283, acc 0.6875\n",
      "2019-03-27T13:13:56.247410: step 2690, loss 0.688054, acc 0.671875\n",
      "2019-03-27T13:13:56.310715: step 2691, loss 0.665426, acc 0.5625\n",
      "2019-03-27T13:13:56.369644: step 2692, loss 0.536529, acc 0.59375\n",
      "2019-03-27T13:13:56.426544: step 2693, loss 0.509606, acc 0.671875\n",
      "2019-03-27T13:13:56.488821: step 2694, loss 0.600834, acc 0.515625\n",
      "2019-03-27T13:13:56.536618: step 2695, loss 0.386986, acc 0.761905\n",
      "2019-03-27T13:13:56.598630: step 2696, loss 0.488129, acc 0.65625\n",
      "2019-03-27T13:13:56.661097: step 2697, loss 0.576813, acc 0.609375\n",
      "2019-03-27T13:13:56.721241: step 2698, loss 0.484716, acc 0.6875\n",
      "2019-03-27T13:13:56.780881: step 2699, loss 0.406573, acc 0.78125\n",
      "2019-03-27T13:13:56.840944: step 2700, loss 0.483001, acc 0.703125\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:13:56.876313: step 2700, loss 1.00646, acc 0.416974\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2700\n",
      "\n",
      "2019-03-27T13:13:57.110304: step 2701, loss 0.541662, acc 0.6875\n",
      "2019-03-27T13:13:57.170525: step 2702, loss 0.486743, acc 0.59375\n",
      "2019-03-27T13:13:57.229736: step 2703, loss 0.538733, acc 0.671875\n",
      "2019-03-27T13:13:57.289770: step 2704, loss 0.561616, acc 0.625\n",
      "2019-03-27T13:13:57.353427: step 2705, loss 0.487706, acc 0.671875\n",
      "2019-03-27T13:13:57.420632: step 2706, loss 0.433708, acc 0.703125\n",
      "2019-03-27T13:13:57.486082: step 2707, loss 0.454018, acc 0.703125\n",
      "2019-03-27T13:13:57.545688: step 2708, loss 0.500208, acc 0.625\n",
      "2019-03-27T13:13:57.604953: step 2709, loss 0.447289, acc 0.71875\n",
      "2019-03-27T13:13:57.674950: step 2710, loss 0.450232, acc 0.640625\n",
      "2019-03-27T13:13:57.738974: step 2711, loss 0.393481, acc 0.71875\n",
      "2019-03-27T13:13:57.800948: step 2712, loss 0.481033, acc 0.578125\n",
      "2019-03-27T13:13:57.862593: step 2713, loss 0.482049, acc 0.671875\n",
      "2019-03-27T13:13:57.925901: step 2714, loss 0.486289, acc 0.625\n",
      "2019-03-27T13:13:57.988033: step 2715, loss 0.477787, acc 0.640625\n",
      "2019-03-27T13:13:58.049916: step 2716, loss 0.478484, acc 0.65625\n",
      "2019-03-27T13:13:58.111297: step 2717, loss 0.537445, acc 0.59375\n",
      "2019-03-27T13:13:58.170873: step 2718, loss 0.558218, acc 0.5625\n",
      "2019-03-27T13:13:58.234181: step 2719, loss 0.41795, acc 0.703125\n",
      "2019-03-27T13:13:58.295532: step 2720, loss 0.42891, acc 0.703125\n",
      "2019-03-27T13:13:58.355728: step 2721, loss 0.484957, acc 0.765625\n",
      "2019-03-27T13:13:58.416693: step 2722, loss 0.400098, acc 0.765625\n",
      "2019-03-27T13:13:58.479293: step 2723, loss 0.555959, acc 0.625\n",
      "2019-03-27T13:13:58.541820: step 2724, loss 0.497499, acc 0.609375\n",
      "2019-03-27T13:13:58.603498: step 2725, loss 0.503939, acc 0.734375\n",
      "2019-03-27T13:13:58.666610: step 2726, loss 0.551961, acc 0.578125\n",
      "2019-03-27T13:13:58.730559: step 2727, loss 0.466202, acc 0.71875\n",
      "2019-03-27T13:13:58.790162: step 2728, loss 0.42924, acc 0.703125\n",
      "2019-03-27T13:13:58.849625: step 2729, loss 0.372556, acc 0.796875\n",
      "2019-03-27T13:13:58.911596: step 2730, loss 0.559789, acc 0.59375\n",
      "2019-03-27T13:13:58.975353: step 2731, loss 0.347306, acc 0.796875\n",
      "2019-03-27T13:13:59.038933: step 2732, loss 0.383159, acc 0.75\n",
      "2019-03-27T13:13:59.102233: step 2733, loss 0.432216, acc 0.6875\n",
      "2019-03-27T13:13:59.165416: step 2734, loss 0.532372, acc 0.609375\n",
      "2019-03-27T13:13:59.224608: step 2735, loss 0.441528, acc 0.65625\n",
      "2019-03-27T13:13:59.284178: step 2736, loss 0.512374, acc 0.71875\n",
      "2019-03-27T13:13:59.345226: step 2737, loss 0.463618, acc 0.6875\n",
      "2019-03-27T13:13:59.408162: step 2738, loss 0.459372, acc 0.703125\n",
      "2019-03-27T13:13:59.468294: step 2739, loss 0.595183, acc 0.515625\n",
      "2019-03-27T13:13:59.530001: step 2740, loss 0.409015, acc 0.703125\n",
      "2019-03-27T13:13:59.595505: step 2741, loss 0.572817, acc 0.578125\n",
      "2019-03-27T13:13:59.656201: step 2742, loss 0.433528, acc 0.734375\n",
      "2019-03-27T13:13:59.715701: step 2743, loss 0.518042, acc 0.734375\n",
      "2019-03-27T13:13:59.781737: step 2744, loss 0.399496, acc 0.78125\n",
      "2019-03-27T13:13:59.841068: step 2745, loss 0.648122, acc 0.609375\n",
      "2019-03-27T13:13:59.900182: step 2746, loss 0.47922, acc 0.59375\n",
      "2019-03-27T13:13:59.964367: step 2747, loss 0.476387, acc 0.640625\n",
      "2019-03-27T13:14:00.025117: step 2748, loss 0.450441, acc 0.703125\n",
      "2019-03-27T13:14:00.088836: step 2749, loss 0.538552, acc 0.65625\n",
      "2019-03-27T13:14:00.150875: step 2750, loss 0.462689, acc 0.625\n",
      "2019-03-27T13:14:00.212661: step 2751, loss 0.494699, acc 0.59375\n",
      "2019-03-27T13:14:00.276488: step 2752, loss 0.57171, acc 0.640625\n",
      "2019-03-27T13:14:00.339280: step 2753, loss 0.443864, acc 0.734375\n",
      "2019-03-27T13:14:00.399668: step 2754, loss 0.494814, acc 0.65625\n",
      "2019-03-27T13:14:00.457541: step 2755, loss 0.536083, acc 0.71875\n",
      "2019-03-27T13:14:00.516691: step 2756, loss 0.443831, acc 0.75\n",
      "2019-03-27T13:14:00.580311: step 2757, loss 0.608617, acc 0.609375\n",
      "2019-03-27T13:14:00.643238: step 2758, loss 0.499531, acc 0.65625\n",
      "2019-03-27T13:14:00.704602: step 2759, loss 0.531424, acc 0.65625\n",
      "2019-03-27T13:14:00.763010: step 2760, loss 0.530903, acc 0.703125\n",
      "2019-03-27T13:14:00.822730: step 2761, loss 0.426515, acc 0.65625\n",
      "2019-03-27T13:14:00.888977: step 2762, loss 0.464637, acc 0.703125\n",
      "2019-03-27T13:14:00.953682: step 2763, loss 0.491893, acc 0.59375\n",
      "2019-03-27T13:14:01.016257: step 2764, loss 0.586292, acc 0.6875\n",
      "2019-03-27T13:14:01.080382: step 2765, loss 0.496551, acc 0.640625\n",
      "2019-03-27T13:14:01.141098: step 2766, loss 0.381402, acc 0.75\n",
      "2019-03-27T13:14:01.203184: step 2767, loss 0.516592, acc 0.640625\n",
      "2019-03-27T13:14:01.262846: step 2768, loss 0.541773, acc 0.578125\n",
      "2019-03-27T13:14:01.322982: step 2769, loss 0.495842, acc 0.734375\n",
      "2019-03-27T13:14:01.383167: step 2770, loss 0.495957, acc 0.625\n",
      "2019-03-27T13:14:01.445503: step 2771, loss 0.468159, acc 0.671875\n",
      "2019-03-27T13:14:01.492925: step 2772, loss 0.497524, acc 0.666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:14:01.556575: step 2773, loss 0.425323, acc 0.796875\n",
      "2019-03-27T13:14:01.618066: step 2774, loss 0.475139, acc 0.734375\n",
      "2019-03-27T13:14:01.678131: step 2775, loss 0.470624, acc 0.703125\n",
      "2019-03-27T13:14:01.737545: step 2776, loss 0.519132, acc 0.734375\n",
      "2019-03-27T13:14:01.796675: step 2777, loss 0.416496, acc 0.765625\n",
      "2019-03-27T13:14:01.856542: step 2778, loss 0.43189, acc 0.78125\n",
      "2019-03-27T13:14:01.918374: step 2779, loss 0.420056, acc 0.796875\n",
      "2019-03-27T13:14:01.977648: step 2780, loss 0.432768, acc 0.71875\n",
      "2019-03-27T13:14:02.037740: step 2781, loss 0.443675, acc 0.703125\n",
      "2019-03-27T13:14:02.098057: step 2782, loss 0.474921, acc 0.6875\n",
      "2019-03-27T13:14:02.158904: step 2783, loss 0.498983, acc 0.6875\n",
      "2019-03-27T13:14:02.219910: step 2784, loss 0.447435, acc 0.75\n",
      "2019-03-27T13:14:02.278652: step 2785, loss 0.421994, acc 0.703125\n",
      "2019-03-27T13:14:02.337757: step 2786, loss 0.318551, acc 0.796875\n",
      "2019-03-27T13:14:02.395084: step 2787, loss 0.466166, acc 0.6875\n",
      "2019-03-27T13:14:02.452767: step 2788, loss 0.52312, acc 0.6875\n",
      "2019-03-27T13:14:02.515989: step 2789, loss 0.540162, acc 0.65625\n",
      "2019-03-27T13:14:02.574786: step 2790, loss 0.489886, acc 0.640625\n",
      "2019-03-27T13:14:02.635847: step 2791, loss 0.444269, acc 0.6875\n",
      "2019-03-27T13:14:02.694758: step 2792, loss 0.438047, acc 0.765625\n",
      "2019-03-27T13:14:02.754557: step 2793, loss 0.482821, acc 0.65625\n",
      "2019-03-27T13:14:02.814078: step 2794, loss 0.620784, acc 0.6875\n",
      "2019-03-27T13:14:02.877108: step 2795, loss 0.493975, acc 0.625\n",
      "2019-03-27T13:14:02.938941: step 2796, loss 0.511701, acc 0.703125\n",
      "2019-03-27T13:14:03.001802: step 2797, loss 0.502693, acc 0.59375\n",
      "2019-03-27T13:14:03.062293: step 2798, loss 0.555055, acc 0.65625\n",
      "2019-03-27T13:14:03.121896: step 2799, loss 0.438219, acc 0.703125\n",
      "2019-03-27T13:14:03.185000: step 2800, loss 0.507389, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:14:03.216515: step 2800, loss 1.07526, acc 0.52214\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2800\n",
      "\n",
      "2019-03-27T13:14:03.384635: step 2801, loss 0.464323, acc 0.703125\n",
      "2019-03-27T13:14:03.443972: step 2802, loss 0.444751, acc 0.765625\n",
      "2019-03-27T13:14:03.501009: step 2803, loss 0.420689, acc 0.75\n",
      "2019-03-27T13:14:03.559808: step 2804, loss 0.540083, acc 0.71875\n",
      "2019-03-27T13:14:03.619572: step 2805, loss 0.45979, acc 0.671875\n",
      "2019-03-27T13:14:03.680603: step 2806, loss 0.397144, acc 0.796875\n",
      "2019-03-27T13:14:03.741814: step 2807, loss 0.460427, acc 0.765625\n",
      "2019-03-27T13:14:03.799677: step 2808, loss 0.49115, acc 0.671875\n",
      "2019-03-27T13:14:03.860468: step 2809, loss 0.500394, acc 0.78125\n",
      "2019-03-27T13:14:03.923059: step 2810, loss 0.487246, acc 0.671875\n",
      "2019-03-27T13:14:03.981031: step 2811, loss 0.566259, acc 0.703125\n",
      "2019-03-27T13:14:04.039514: step 2812, loss 0.455759, acc 0.734375\n",
      "2019-03-27T13:14:04.103195: step 2813, loss 0.448679, acc 0.734375\n",
      "2019-03-27T13:14:04.165087: step 2814, loss 0.586682, acc 0.5625\n",
      "2019-03-27T13:14:04.227441: step 2815, loss 0.367545, acc 0.78125\n",
      "2019-03-27T13:14:04.289727: step 2816, loss 0.505494, acc 0.671875\n",
      "2019-03-27T13:14:04.348235: step 2817, loss 0.42294, acc 0.765625\n",
      "2019-03-27T13:14:04.409782: step 2818, loss 0.507982, acc 0.71875\n",
      "2019-03-27T13:14:04.480243: step 2819, loss 0.604269, acc 0.53125\n",
      "2019-03-27T13:14:04.542176: step 2820, loss 0.453568, acc 0.65625\n",
      "2019-03-27T13:14:04.604641: step 2821, loss 0.518858, acc 0.6875\n",
      "2019-03-27T13:14:04.666804: step 2822, loss 0.457766, acc 0.671875\n",
      "2019-03-27T13:14:04.730631: step 2823, loss 0.451676, acc 0.765625\n",
      "2019-03-27T13:14:04.791105: step 2824, loss 0.478124, acc 0.640625\n",
      "2019-03-27T13:14:04.854834: step 2825, loss 0.42756, acc 0.734375\n",
      "2019-03-27T13:14:04.917316: step 2826, loss 0.447525, acc 0.703125\n",
      "2019-03-27T13:14:04.979115: step 2827, loss 0.347114, acc 0.78125\n",
      "2019-03-27T13:14:05.038541: step 2828, loss 0.474984, acc 0.671875\n",
      "2019-03-27T13:14:05.099276: step 2829, loss 0.457638, acc 0.75\n",
      "2019-03-27T13:14:05.159798: step 2830, loss 0.461918, acc 0.671875\n",
      "2019-03-27T13:14:05.221360: step 2831, loss 0.505298, acc 0.609375\n",
      "2019-03-27T13:14:05.281976: step 2832, loss 0.437972, acc 0.75\n",
      "2019-03-27T13:14:05.339954: step 2833, loss 0.537113, acc 0.734375\n",
      "2019-03-27T13:14:05.401200: step 2834, loss 0.454731, acc 0.71875\n",
      "2019-03-27T13:14:05.459898: step 2835, loss 0.483385, acc 0.65625\n",
      "2019-03-27T13:14:05.521128: step 2836, loss 0.461095, acc 0.734375\n",
      "2019-03-27T13:14:05.582929: step 2837, loss 0.519343, acc 0.578125\n",
      "2019-03-27T13:14:05.644360: step 2838, loss 0.487989, acc 0.625\n",
      "2019-03-27T13:14:05.703264: step 2839, loss 0.464957, acc 0.703125\n",
      "2019-03-27T13:14:05.760465: step 2840, loss 0.459338, acc 0.6875\n",
      "2019-03-27T13:14:05.821798: step 2841, loss 0.563115, acc 0.609375\n",
      "2019-03-27T13:14:05.884109: step 2842, loss 0.486021, acc 0.625\n",
      "2019-03-27T13:14:05.946568: step 2843, loss 0.475859, acc 0.65625\n",
      "2019-03-27T13:14:06.009938: step 2844, loss 0.451094, acc 0.625\n",
      "2019-03-27T13:14:06.073373: step 2845, loss 0.514614, acc 0.65625\n",
      "2019-03-27T13:14:06.135966: step 2846, loss 0.485005, acc 0.75\n",
      "2019-03-27T13:14:06.196851: step 2847, loss 0.520661, acc 0.59375\n",
      "2019-03-27T13:14:06.260117: step 2848, loss 0.445087, acc 0.6875\n",
      "2019-03-27T13:14:06.310592: step 2849, loss 0.538868, acc 0.571429\n",
      "2019-03-27T13:14:06.373557: step 2850, loss 0.471144, acc 0.640625\n",
      "2019-03-27T13:14:06.433791: step 2851, loss 0.34861, acc 0.78125\n",
      "2019-03-27T13:14:06.496614: step 2852, loss 0.527809, acc 0.625\n",
      "2019-03-27T13:14:06.559979: step 2853, loss 0.477734, acc 0.640625\n",
      "2019-03-27T13:14:06.621742: step 2854, loss 0.457008, acc 0.703125\n",
      "2019-03-27T13:14:06.685173: step 2855, loss 0.551465, acc 0.65625\n",
      "2019-03-27T13:14:06.747837: step 2856, loss 0.369389, acc 0.78125\n",
      "2019-03-27T13:14:06.816122: step 2857, loss 0.486281, acc 0.609375\n",
      "2019-03-27T13:14:06.880313: step 2858, loss 0.530829, acc 0.625\n",
      "2019-03-27T13:14:06.943337: step 2859, loss 0.390271, acc 0.71875\n",
      "2019-03-27T13:14:07.004022: step 2860, loss 0.461669, acc 0.671875\n",
      "2019-03-27T13:14:07.064582: step 2861, loss 0.495377, acc 0.640625\n",
      "2019-03-27T13:14:07.125325: step 2862, loss 0.471607, acc 0.6875\n",
      "2019-03-27T13:14:07.188896: step 2863, loss 0.477051, acc 0.734375\n",
      "2019-03-27T13:14:07.255118: step 2864, loss 0.522385, acc 0.640625\n",
      "2019-03-27T13:14:07.315174: step 2865, loss 0.561819, acc 0.75\n",
      "2019-03-27T13:14:07.374389: step 2866, loss 0.484261, acc 0.734375\n",
      "2019-03-27T13:14:07.437359: step 2867, loss 0.397172, acc 0.765625\n",
      "2019-03-27T13:14:07.497161: step 2868, loss 0.477659, acc 0.671875\n",
      "2019-03-27T13:14:07.560621: step 2869, loss 0.455238, acc 0.703125\n",
      "2019-03-27T13:14:07.620433: step 2870, loss 0.39889, acc 0.71875\n",
      "2019-03-27T13:14:07.681517: step 2871, loss 0.359255, acc 0.78125\n",
      "2019-03-27T13:14:07.749419: step 2872, loss 0.377862, acc 0.765625\n",
      "2019-03-27T13:14:07.813684: step 2873, loss 0.42172, acc 0.703125\n",
      "2019-03-27T13:14:07.871886: step 2874, loss 0.455678, acc 0.734375\n",
      "2019-03-27T13:14:07.931359: step 2875, loss 0.483566, acc 0.703125\n",
      "2019-03-27T13:14:07.992259: step 2876, loss 0.56051, acc 0.625\n",
      "2019-03-27T13:14:08.054886: step 2877, loss 0.477067, acc 0.6875\n",
      "2019-03-27T13:14:08.117173: step 2878, loss 0.426836, acc 0.65625\n",
      "2019-03-27T13:14:08.177919: step 2879, loss 0.456017, acc 0.71875\n",
      "2019-03-27T13:14:08.238075: step 2880, loss 0.445842, acc 0.6875\n",
      "2019-03-27T13:14:08.299488: step 2881, loss 0.596138, acc 0.59375\n",
      "2019-03-27T13:14:08.363146: step 2882, loss 0.381474, acc 0.796875\n",
      "2019-03-27T13:14:08.425037: step 2883, loss 0.43567, acc 0.6875\n",
      "2019-03-27T13:14:08.485078: step 2884, loss 0.445295, acc 0.75\n",
      "2019-03-27T13:14:08.543813: step 2885, loss 0.476469, acc 0.6875\n",
      "2019-03-27T13:14:08.608106: step 2886, loss 0.459961, acc 0.65625\n",
      "2019-03-27T13:14:08.671587: step 2887, loss 0.526349, acc 0.65625\n",
      "2019-03-27T13:14:08.731016: step 2888, loss 0.53957, acc 0.546875\n",
      "2019-03-27T13:14:08.794518: step 2889, loss 0.439872, acc 0.765625\n",
      "2019-03-27T13:14:08.859799: step 2890, loss 0.479809, acc 0.65625\n",
      "2019-03-27T13:14:08.922826: step 2891, loss 0.483709, acc 0.71875\n",
      "2019-03-27T13:14:08.983110: step 2892, loss 0.572475, acc 0.640625\n",
      "2019-03-27T13:14:09.045707: step 2893, loss 0.540487, acc 0.578125\n",
      "2019-03-27T13:14:09.106635: step 2894, loss 0.465169, acc 0.65625\n",
      "2019-03-27T13:14:09.168853: step 2895, loss 0.440539, acc 0.71875\n",
      "2019-03-27T13:14:09.229965: step 2896, loss 0.444182, acc 0.734375\n",
      "2019-03-27T13:14:09.293547: step 2897, loss 0.533848, acc 0.578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:14:09.355136: step 2898, loss 0.435407, acc 0.6875\n",
      "2019-03-27T13:14:09.422055: step 2899, loss 0.453235, acc 0.640625\n",
      "2019-03-27T13:14:09.486816: step 2900, loss 0.562118, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:14:09.520797: step 2900, loss 1.00892, acc 0.306273\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-2900\n",
      "\n",
      "2019-03-27T13:14:09.699205: step 2901, loss 0.46917, acc 0.6875\n",
      "2019-03-27T13:14:09.761267: step 2902, loss 0.485876, acc 0.6875\n",
      "2019-03-27T13:14:09.824227: step 2903, loss 0.502787, acc 0.703125\n",
      "2019-03-27T13:14:09.888340: step 2904, loss 0.607644, acc 0.640625\n",
      "2019-03-27T13:14:09.947418: step 2905, loss 0.441734, acc 0.71875\n",
      "2019-03-27T13:14:10.017373: step 2906, loss 0.457435, acc 0.625\n",
      "2019-03-27T13:14:10.079405: step 2907, loss 0.514796, acc 0.53125\n",
      "2019-03-27T13:14:10.142393: step 2908, loss 0.582035, acc 0.59375\n",
      "2019-03-27T13:14:10.204763: step 2909, loss 0.475112, acc 0.703125\n",
      "2019-03-27T13:14:10.268841: step 2910, loss 0.482309, acc 0.6875\n",
      "2019-03-27T13:14:10.330528: step 2911, loss 0.480725, acc 0.703125\n",
      "2019-03-27T13:14:10.395834: step 2912, loss 0.516101, acc 0.734375\n",
      "2019-03-27T13:14:10.458534: step 2913, loss 0.553372, acc 0.578125\n",
      "2019-03-27T13:14:10.519990: step 2914, loss 0.358639, acc 0.8125\n",
      "2019-03-27T13:14:10.584021: step 2915, loss 0.480656, acc 0.6875\n",
      "2019-03-27T13:14:10.642240: step 2916, loss 0.439248, acc 0.640625\n",
      "2019-03-27T13:14:10.707376: step 2917, loss 0.508587, acc 0.703125\n",
      "2019-03-27T13:14:10.767236: step 2918, loss 0.505281, acc 0.65625\n",
      "2019-03-27T13:14:10.829330: step 2919, loss 0.321709, acc 0.75\n",
      "2019-03-27T13:14:10.889262: step 2920, loss 0.501827, acc 0.703125\n",
      "2019-03-27T13:14:10.951831: step 2921, loss 0.486649, acc 0.703125\n",
      "2019-03-27T13:14:11.014439: step 2922, loss 0.503612, acc 0.65625\n",
      "2019-03-27T13:14:11.079689: step 2923, loss 0.543881, acc 0.65625\n",
      "2019-03-27T13:14:11.140626: step 2924, loss 0.501593, acc 0.671875\n",
      "2019-03-27T13:14:11.203209: step 2925, loss 0.572828, acc 0.640625\n",
      "2019-03-27T13:14:11.255556: step 2926, loss 0.634021, acc 0.52381\n",
      "2019-03-27T13:14:11.319166: step 2927, loss 0.420914, acc 0.703125\n",
      "2019-03-27T13:14:11.382998: step 2928, loss 0.487859, acc 0.671875\n",
      "2019-03-27T13:14:11.444945: step 2929, loss 0.504553, acc 0.65625\n",
      "2019-03-27T13:14:11.507561: step 2930, loss 0.494924, acc 0.65625\n",
      "2019-03-27T13:14:11.565819: step 2931, loss 0.483857, acc 0.703125\n",
      "2019-03-27T13:14:11.629908: step 2932, loss 0.483029, acc 0.6875\n",
      "2019-03-27T13:14:11.693344: step 2933, loss 0.451838, acc 0.625\n",
      "2019-03-27T13:14:11.754558: step 2934, loss 0.46404, acc 0.6875\n",
      "2019-03-27T13:14:11.814787: step 2935, loss 0.496713, acc 0.671875\n",
      "2019-03-27T13:14:11.875482: step 2936, loss 0.473958, acc 0.6875\n",
      "2019-03-27T13:14:11.936614: step 2937, loss 0.454073, acc 0.640625\n",
      "2019-03-27T13:14:11.997514: step 2938, loss 0.487513, acc 0.609375\n",
      "2019-03-27T13:14:12.059949: step 2939, loss 0.567126, acc 0.609375\n",
      "2019-03-27T13:14:12.122235: step 2940, loss 0.384513, acc 0.796875\n",
      "2019-03-27T13:14:12.182181: step 2941, loss 0.471505, acc 0.703125\n",
      "2019-03-27T13:14:12.243405: step 2942, loss 0.437376, acc 0.75\n",
      "2019-03-27T13:14:12.303232: step 2943, loss 0.41156, acc 0.78125\n",
      "2019-03-27T13:14:12.363017: step 2944, loss 0.516852, acc 0.625\n",
      "2019-03-27T13:14:12.422748: step 2945, loss 0.487763, acc 0.59375\n",
      "2019-03-27T13:14:12.483277: step 2946, loss 0.391057, acc 0.78125\n",
      "2019-03-27T13:14:12.543330: step 2947, loss 0.411747, acc 0.734375\n",
      "2019-03-27T13:14:12.607009: step 2948, loss 0.498165, acc 0.65625\n",
      "2019-03-27T13:14:12.667707: step 2949, loss 0.493671, acc 0.734375\n",
      "2019-03-27T13:14:12.733809: step 2950, loss 0.442953, acc 0.671875\n",
      "2019-03-27T13:14:12.793538: step 2951, loss 0.507643, acc 0.640625\n",
      "2019-03-27T13:14:12.852063: step 2952, loss 0.465169, acc 0.703125\n",
      "2019-03-27T13:14:12.917043: step 2953, loss 0.538389, acc 0.640625\n",
      "2019-03-27T13:14:12.985404: step 2954, loss 0.447553, acc 0.6875\n",
      "2019-03-27T13:14:13.044176: step 2955, loss 0.45513, acc 0.65625\n",
      "2019-03-27T13:14:13.105961: step 2956, loss 0.445257, acc 0.734375\n",
      "2019-03-27T13:14:13.172276: step 2957, loss 0.511179, acc 0.671875\n",
      "2019-03-27T13:14:13.236211: step 2958, loss 0.491522, acc 0.65625\n",
      "2019-03-27T13:14:13.299833: step 2959, loss 0.534363, acc 0.578125\n",
      "2019-03-27T13:14:13.361081: step 2960, loss 0.535723, acc 0.5625\n",
      "2019-03-27T13:14:13.423527: step 2961, loss 0.520203, acc 0.65625\n",
      "2019-03-27T13:14:13.485603: step 2962, loss 0.39705, acc 0.734375\n",
      "2019-03-27T13:14:13.545872: step 2963, loss 0.431334, acc 0.71875\n",
      "2019-03-27T13:14:13.606143: step 2964, loss 0.507073, acc 0.640625\n",
      "2019-03-27T13:14:13.669280: step 2965, loss 0.588053, acc 0.609375\n",
      "2019-03-27T13:14:13.729516: step 2966, loss 0.444865, acc 0.6875\n",
      "2019-03-27T13:14:13.790372: step 2967, loss 0.40162, acc 0.71875\n",
      "2019-03-27T13:14:13.850911: step 2968, loss 0.544154, acc 0.546875\n",
      "2019-03-27T13:14:13.914045: step 2969, loss 0.485764, acc 0.609375\n",
      "2019-03-27T13:14:13.972761: step 2970, loss 0.495823, acc 0.6875\n",
      "2019-03-27T13:14:14.032552: step 2971, loss 0.478445, acc 0.640625\n",
      "2019-03-27T13:14:14.095132: step 2972, loss 0.517975, acc 0.671875\n",
      "2019-03-27T13:14:14.154555: step 2973, loss 0.493298, acc 0.640625\n",
      "2019-03-27T13:14:14.213760: step 2974, loss 0.357876, acc 0.75\n",
      "2019-03-27T13:14:14.276667: step 2975, loss 0.427412, acc 0.734375\n",
      "2019-03-27T13:14:14.338418: step 2976, loss 0.542891, acc 0.625\n",
      "2019-03-27T13:14:14.399417: step 2977, loss 0.433438, acc 0.703125\n",
      "2019-03-27T13:14:14.461892: step 2978, loss 0.391388, acc 0.609375\n",
      "2019-03-27T13:14:14.518749: step 2979, loss 0.466178, acc 0.671875\n",
      "2019-03-27T13:14:14.580434: step 2980, loss 0.446665, acc 0.65625\n",
      "2019-03-27T13:14:14.640830: step 2981, loss 0.529257, acc 0.71875\n",
      "2019-03-27T13:14:14.703522: step 2982, loss 0.449294, acc 0.703125\n",
      "2019-03-27T13:14:14.765049: step 2983, loss 0.418734, acc 0.671875\n",
      "2019-03-27T13:14:14.828307: step 2984, loss 0.568795, acc 0.65625\n",
      "2019-03-27T13:14:14.890700: step 2985, loss 0.51181, acc 0.640625\n",
      "2019-03-27T13:14:14.952430: step 2986, loss 0.460609, acc 0.6875\n",
      "2019-03-27T13:14:15.011735: step 2987, loss 0.501449, acc 0.671875\n",
      "2019-03-27T13:14:15.069166: step 2988, loss 0.457801, acc 0.671875\n",
      "2019-03-27T13:14:15.130264: step 2989, loss 0.518416, acc 0.625\n",
      "2019-03-27T13:14:15.189009: step 2990, loss 0.499777, acc 0.609375\n",
      "2019-03-27T13:14:15.251683: step 2991, loss 0.529051, acc 0.625\n",
      "2019-03-27T13:14:15.313140: step 2992, loss 0.455956, acc 0.578125\n",
      "2019-03-27T13:14:15.373018: step 2993, loss 0.379583, acc 0.765625\n",
      "2019-03-27T13:14:15.435166: step 2994, loss 0.489699, acc 0.703125\n",
      "2019-03-27T13:14:15.498788: step 2995, loss 0.453189, acc 0.671875\n",
      "2019-03-27T13:14:15.557972: step 2996, loss 0.481856, acc 0.609375\n",
      "2019-03-27T13:14:15.617491: step 2997, loss 0.611123, acc 0.59375\n",
      "2019-03-27T13:14:15.682011: step 2998, loss 0.468693, acc 0.578125\n",
      "2019-03-27T13:14:15.743979: step 2999, loss 0.454248, acc 0.734375\n",
      "2019-03-27T13:14:15.807221: step 3000, loss 0.472771, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:14:15.841287: step 3000, loss 0.982966, acc 0.555351\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-3000\n",
      "\n",
      "2019-03-27T13:14:16.018205: step 3001, loss 0.500149, acc 0.6875\n",
      "2019-03-27T13:14:16.079604: step 3002, loss 0.501784, acc 0.640625\n",
      "2019-03-27T13:14:16.129210: step 3003, loss 0.413343, acc 0.761905\n",
      "2019-03-27T13:14:16.188791: step 3004, loss 0.618223, acc 0.578125\n",
      "2019-03-27T13:14:16.251324: step 3005, loss 0.549997, acc 0.640625\n",
      "2019-03-27T13:14:16.314542: step 3006, loss 0.475925, acc 0.640625\n",
      "2019-03-27T13:14:16.379127: step 3007, loss 0.388314, acc 0.734375\n",
      "2019-03-27T13:14:16.438859: step 3008, loss 0.507935, acc 0.65625\n",
      "2019-03-27T13:14:16.499664: step 3009, loss 0.470362, acc 0.703125\n",
      "2019-03-27T13:14:16.562477: step 3010, loss 0.527916, acc 0.640625\n",
      "2019-03-27T13:14:16.624969: step 3011, loss 0.404629, acc 0.75\n",
      "2019-03-27T13:14:16.683598: step 3012, loss 0.406249, acc 0.703125\n",
      "2019-03-27T13:14:16.745599: step 3013, loss 0.45774, acc 0.6875\n",
      "2019-03-27T13:14:16.809928: step 3014, loss 0.431425, acc 0.71875\n",
      "2019-03-27T13:14:16.870842: step 3015, loss 0.372693, acc 0.765625\n",
      "2019-03-27T13:14:16.935175: step 3016, loss 0.423357, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:14:16.993230: step 3017, loss 0.463236, acc 0.703125\n",
      "2019-03-27T13:14:17.051531: step 3018, loss 0.507231, acc 0.6875\n",
      "2019-03-27T13:14:17.111946: step 3019, loss 0.588609, acc 0.546875\n",
      "2019-03-27T13:14:17.175839: step 3020, loss 0.458435, acc 0.625\n",
      "2019-03-27T13:14:17.236228: step 3021, loss 0.466398, acc 0.671875\n",
      "2019-03-27T13:14:17.296753: step 3022, loss 0.564639, acc 0.671875\n",
      "2019-03-27T13:14:17.355356: step 3023, loss 0.412572, acc 0.8125\n",
      "2019-03-27T13:14:17.413983: step 3024, loss 0.461018, acc 0.671875\n",
      "2019-03-27T13:14:17.475087: step 3025, loss 0.600767, acc 0.625\n",
      "2019-03-27T13:14:17.536981: step 3026, loss 0.50391, acc 0.703125\n",
      "2019-03-27T13:14:17.596457: step 3027, loss 0.48651, acc 0.703125\n",
      "2019-03-27T13:14:17.653109: step 3028, loss 0.506343, acc 0.765625\n",
      "2019-03-27T13:14:17.713544: step 3029, loss 0.503496, acc 0.640625\n",
      "2019-03-27T13:14:17.775700: step 3030, loss 0.470276, acc 0.625\n",
      "2019-03-27T13:14:17.839120: step 3031, loss 0.432487, acc 0.6875\n",
      "2019-03-27T13:14:17.900595: step 3032, loss 0.373189, acc 0.796875\n",
      "2019-03-27T13:14:17.961329: step 3033, loss 0.522183, acc 0.65625\n",
      "2019-03-27T13:14:18.023904: step 3034, loss 0.418659, acc 0.703125\n",
      "2019-03-27T13:14:18.083317: step 3035, loss 0.463447, acc 0.640625\n",
      "2019-03-27T13:14:18.143362: step 3036, loss 0.446687, acc 0.671875\n",
      "2019-03-27T13:14:18.203543: step 3037, loss 0.558894, acc 0.578125\n",
      "2019-03-27T13:14:18.265285: step 3038, loss 0.359691, acc 0.78125\n",
      "2019-03-27T13:14:18.327015: step 3039, loss 0.591404, acc 0.6875\n",
      "2019-03-27T13:14:18.394975: step 3040, loss 0.449489, acc 0.6875\n",
      "2019-03-27T13:14:18.455861: step 3041, loss 0.459515, acc 0.734375\n",
      "2019-03-27T13:14:18.519614: step 3042, loss 0.427082, acc 0.734375\n",
      "2019-03-27T13:14:18.579331: step 3043, loss 0.495235, acc 0.65625\n",
      "2019-03-27T13:14:18.643288: step 3044, loss 0.437064, acc 0.703125\n",
      "2019-03-27T13:14:18.704301: step 3045, loss 0.507282, acc 0.625\n",
      "2019-03-27T13:14:18.762012: step 3046, loss 0.576257, acc 0.578125\n",
      "2019-03-27T13:14:18.822312: step 3047, loss 0.494978, acc 0.65625\n",
      "2019-03-27T13:14:18.883860: step 3048, loss 0.432496, acc 0.75\n",
      "2019-03-27T13:14:18.945532: step 3049, loss 0.507027, acc 0.65625\n",
      "2019-03-27T13:14:19.004805: step 3050, loss 0.412111, acc 0.703125\n",
      "2019-03-27T13:14:19.067971: step 3051, loss 0.510417, acc 0.59375\n",
      "2019-03-27T13:14:19.133728: step 3052, loss 0.459669, acc 0.734375\n",
      "2019-03-27T13:14:19.194840: step 3053, loss 0.479595, acc 0.6875\n",
      "2019-03-27T13:14:19.255536: step 3054, loss 0.448688, acc 0.6875\n",
      "2019-03-27T13:14:19.320435: step 3055, loss 0.544973, acc 0.65625\n",
      "2019-03-27T13:14:19.381931: step 3056, loss 0.426416, acc 0.671875\n",
      "2019-03-27T13:14:19.446477: step 3057, loss 0.382916, acc 0.78125\n",
      "2019-03-27T13:14:19.512359: step 3058, loss 0.52467, acc 0.65625\n",
      "2019-03-27T13:14:19.574611: step 3059, loss 0.421011, acc 0.78125\n",
      "2019-03-27T13:14:19.636809: step 3060, loss 0.466304, acc 0.71875\n",
      "2019-03-27T13:14:19.698868: step 3061, loss 0.497838, acc 0.671875\n",
      "2019-03-27T13:14:19.763198: step 3062, loss 0.398782, acc 0.765625\n",
      "2019-03-27T13:14:19.825266: step 3063, loss 0.371426, acc 0.765625\n",
      "2019-03-27T13:14:19.890873: step 3064, loss 0.540874, acc 0.640625\n",
      "2019-03-27T13:14:19.954577: step 3065, loss 0.377056, acc 0.703125\n",
      "2019-03-27T13:14:20.016890: step 3066, loss 0.473159, acc 0.625\n",
      "2019-03-27T13:14:20.079405: step 3067, loss 0.43564, acc 0.640625\n",
      "2019-03-27T13:14:20.140544: step 3068, loss 0.459831, acc 0.71875\n",
      "2019-03-27T13:14:20.200477: step 3069, loss 0.485134, acc 0.734375\n",
      "2019-03-27T13:14:20.262516: step 3070, loss 0.466079, acc 0.703125\n",
      "2019-03-27T13:14:20.326330: step 3071, loss 0.482935, acc 0.59375\n",
      "2019-03-27T13:14:20.385707: step 3072, loss 0.388068, acc 0.703125\n",
      "2019-03-27T13:14:20.444875: step 3073, loss 0.69729, acc 0.546875\n",
      "2019-03-27T13:14:20.505054: step 3074, loss 0.445808, acc 0.6875\n",
      "2019-03-27T13:14:20.564913: step 3075, loss 0.506879, acc 0.609375\n",
      "2019-03-27T13:14:20.630191: step 3076, loss 0.502126, acc 0.640625\n",
      "2019-03-27T13:14:20.688783: step 3077, loss 0.530248, acc 0.640625\n",
      "2019-03-27T13:14:20.749499: step 3078, loss 0.601589, acc 0.5625\n",
      "2019-03-27T13:14:20.815600: step 3079, loss 0.526147, acc 0.546875\n",
      "2019-03-27T13:14:20.862242: step 3080, loss 0.528779, acc 0.666667\n",
      "2019-03-27T13:14:20.925204: step 3081, loss 0.443118, acc 0.71875\n",
      "2019-03-27T13:14:20.992688: step 3082, loss 0.532557, acc 0.625\n",
      "2019-03-27T13:14:21.056761: step 3083, loss 0.403872, acc 0.78125\n",
      "2019-03-27T13:14:21.117867: step 3084, loss 0.421186, acc 0.671875\n",
      "2019-03-27T13:14:21.180532: step 3085, loss 0.451002, acc 0.703125\n",
      "2019-03-27T13:14:21.240575: step 3086, loss 0.382373, acc 0.828125\n",
      "2019-03-27T13:14:21.307449: step 3087, loss 0.469039, acc 0.71875\n",
      "2019-03-27T13:14:21.369213: step 3088, loss 0.474285, acc 0.734375\n",
      "2019-03-27T13:14:21.434445: step 3089, loss 0.499706, acc 0.625\n",
      "2019-03-27T13:14:21.498327: step 3090, loss 0.548528, acc 0.59375\n",
      "2019-03-27T13:14:21.561024: step 3091, loss 0.431867, acc 0.703125\n",
      "2019-03-27T13:14:21.624458: step 3092, loss 0.405414, acc 0.71875\n",
      "2019-03-27T13:14:21.688944: step 3093, loss 0.437601, acc 0.765625\n",
      "2019-03-27T13:14:21.749155: step 3094, loss 0.441535, acc 0.734375\n",
      "2019-03-27T13:14:21.809275: step 3095, loss 0.428581, acc 0.703125\n",
      "2019-03-27T13:14:21.868881: step 3096, loss 0.423939, acc 0.703125\n",
      "2019-03-27T13:14:21.936550: step 3097, loss 0.63924, acc 0.640625\n",
      "2019-03-27T13:14:21.998792: step 3098, loss 0.386625, acc 0.78125\n",
      "2019-03-27T13:14:22.059368: step 3099, loss 0.558174, acc 0.671875\n",
      "2019-03-27T13:14:22.121655: step 3100, loss 0.457342, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:14:22.153855: step 3100, loss 1.03848, acc 0.332103\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-3100\n",
      "\n",
      "2019-03-27T13:14:22.321246: step 3101, loss 0.351735, acc 0.734375\n",
      "2019-03-27T13:14:22.382014: step 3102, loss 0.508268, acc 0.671875\n",
      "2019-03-27T13:14:22.442555: step 3103, loss 0.414145, acc 0.734375\n",
      "2019-03-27T13:14:22.505702: step 3104, loss 0.368363, acc 0.765625\n",
      "2019-03-27T13:14:22.566232: step 3105, loss 0.488763, acc 0.671875\n",
      "2019-03-27T13:14:22.629025: step 3106, loss 0.516347, acc 0.65625\n",
      "2019-03-27T13:14:22.688212: step 3107, loss 0.51541, acc 0.640625\n",
      "2019-03-27T13:14:22.748375: step 3108, loss 0.405535, acc 0.828125\n",
      "2019-03-27T13:14:22.808448: step 3109, loss 0.482045, acc 0.6875\n",
      "2019-03-27T13:14:22.871014: step 3110, loss 0.535877, acc 0.59375\n",
      "2019-03-27T13:14:22.931244: step 3111, loss 0.533218, acc 0.625\n",
      "2019-03-27T13:14:22.993949: step 3112, loss 0.446891, acc 0.75\n",
      "2019-03-27T13:14:23.053919: step 3113, loss 0.36127, acc 0.734375\n",
      "2019-03-27T13:14:23.117027: step 3114, loss 0.510146, acc 0.625\n",
      "2019-03-27T13:14:23.177027: step 3115, loss 0.494054, acc 0.640625\n",
      "2019-03-27T13:14:23.238538: step 3116, loss 0.524987, acc 0.640625\n",
      "2019-03-27T13:14:23.299762: step 3117, loss 0.51792, acc 0.609375\n",
      "2019-03-27T13:14:23.363787: step 3118, loss 0.422055, acc 0.765625\n",
      "2019-03-27T13:14:23.422602: step 3119, loss 0.474718, acc 0.640625\n",
      "2019-03-27T13:14:23.481564: step 3120, loss 0.464567, acc 0.75\n",
      "2019-03-27T13:14:23.539667: step 3121, loss 0.554016, acc 0.71875\n",
      "2019-03-27T13:14:23.599316: step 3122, loss 0.457318, acc 0.71875\n",
      "2019-03-27T13:14:23.656430: step 3123, loss 0.436834, acc 0.65625\n",
      "2019-03-27T13:14:23.722756: step 3124, loss 0.488857, acc 0.609375\n",
      "2019-03-27T13:14:23.783375: step 3125, loss 0.483879, acc 0.65625\n",
      "2019-03-27T13:14:23.846534: step 3126, loss 0.428613, acc 0.671875\n",
      "2019-03-27T13:14:23.908541: step 3127, loss 0.439155, acc 0.703125\n",
      "2019-03-27T13:14:23.971729: step 3128, loss 0.570415, acc 0.671875\n",
      "2019-03-27T13:14:24.034164: step 3129, loss 0.453759, acc 0.734375\n",
      "2019-03-27T13:14:24.098842: step 3130, loss 0.446146, acc 0.671875\n",
      "2019-03-27T13:14:24.160884: step 3131, loss 0.470756, acc 0.6875\n",
      "2019-03-27T13:14:24.235536: step 3132, loss 0.492726, acc 0.625\n",
      "2019-03-27T13:14:24.300365: step 3133, loss 0.505109, acc 0.640625\n",
      "2019-03-27T13:14:24.364997: step 3134, loss 0.430576, acc 0.734375\n",
      "2019-03-27T13:14:24.426931: step 3135, loss 0.455987, acc 0.703125\n",
      "2019-03-27T13:14:24.490089: step 3136, loss 0.429037, acc 0.71875\n",
      "2019-03-27T13:14:24.551835: step 3137, loss 0.598371, acc 0.625\n",
      "2019-03-27T13:14:24.613518: step 3138, loss 0.465903, acc 0.734375\n",
      "2019-03-27T13:14:24.674990: step 3139, loss 0.479772, acc 0.671875\n",
      "2019-03-27T13:14:24.733382: step 3140, loss 0.551001, acc 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:14:24.798944: step 3141, loss 0.464432, acc 0.578125\n",
      "2019-03-27T13:14:24.861171: step 3142, loss 0.512725, acc 0.640625\n",
      "2019-03-27T13:14:24.920122: step 3143, loss 0.539207, acc 0.65625\n",
      "2019-03-27T13:14:24.980002: step 3144, loss 0.476772, acc 0.671875\n",
      "2019-03-27T13:14:25.043562: step 3145, loss 0.641376, acc 0.53125\n",
      "2019-03-27T13:14:25.106556: step 3146, loss 0.506669, acc 0.65625\n",
      "2019-03-27T13:14:25.171941: step 3147, loss 0.574255, acc 0.640625\n",
      "2019-03-27T13:14:25.237686: step 3148, loss 0.402769, acc 0.703125\n",
      "2019-03-27T13:14:25.300991: step 3149, loss 0.407814, acc 0.78125\n",
      "2019-03-27T13:14:25.361482: step 3150, loss 0.515046, acc 0.640625\n",
      "2019-03-27T13:14:25.423203: step 3151, loss 0.488896, acc 0.625\n",
      "2019-03-27T13:14:25.487464: step 3152, loss 0.580868, acc 0.59375\n",
      "2019-03-27T13:14:25.548169: step 3153, loss 0.402511, acc 0.703125\n",
      "2019-03-27T13:14:25.608533: step 3154, loss 0.414889, acc 0.703125\n",
      "2019-03-27T13:14:25.670139: step 3155, loss 0.459532, acc 0.734375\n",
      "2019-03-27T13:14:25.730314: step 3156, loss 0.448777, acc 0.6875\n",
      "2019-03-27T13:14:25.779361: step 3157, loss 0.567314, acc 0.52381\n",
      "2019-03-27T13:14:25.844594: step 3158, loss 0.530748, acc 0.65625\n",
      "2019-03-27T13:14:25.904391: step 3159, loss 0.53718, acc 0.6875\n",
      "2019-03-27T13:14:25.966793: step 3160, loss 0.400432, acc 0.734375\n",
      "2019-03-27T13:14:26.028122: step 3161, loss 0.445069, acc 0.6875\n",
      "2019-03-27T13:14:26.092882: step 3162, loss 0.472901, acc 0.671875\n",
      "2019-03-27T13:14:26.155831: step 3163, loss 0.422938, acc 0.75\n",
      "2019-03-27T13:14:26.218260: step 3164, loss 0.508512, acc 0.6875\n",
      "2019-03-27T13:14:26.281238: step 3165, loss 0.397303, acc 0.75\n",
      "2019-03-27T13:14:26.341662: step 3166, loss 0.382385, acc 0.703125\n",
      "2019-03-27T13:14:26.402735: step 3167, loss 0.369422, acc 0.734375\n",
      "2019-03-27T13:14:26.462446: step 3168, loss 0.4149, acc 0.71875\n",
      "2019-03-27T13:14:26.524407: step 3169, loss 0.568288, acc 0.640625\n",
      "2019-03-27T13:14:26.583666: step 3170, loss 0.407095, acc 0.671875\n",
      "2019-03-27T13:14:26.643389: step 3171, loss 0.380875, acc 0.765625\n",
      "2019-03-27T13:14:26.706063: step 3172, loss 0.393387, acc 0.71875\n",
      "2019-03-27T13:14:26.766777: step 3173, loss 0.475044, acc 0.65625\n",
      "2019-03-27T13:14:26.830429: step 3174, loss 0.479381, acc 0.6875\n",
      "2019-03-27T13:14:26.893816: step 3175, loss 0.464555, acc 0.6875\n",
      "2019-03-27T13:14:26.955182: step 3176, loss 0.540738, acc 0.640625\n",
      "2019-03-27T13:14:27.015366: step 3177, loss 0.498251, acc 0.640625\n",
      "2019-03-27T13:14:27.077063: step 3178, loss 0.522671, acc 0.734375\n",
      "2019-03-27T13:14:27.144442: step 3179, loss 0.413304, acc 0.734375\n",
      "2019-03-27T13:14:27.202105: step 3180, loss 0.426489, acc 0.734375\n",
      "2019-03-27T13:14:27.263204: step 3181, loss 0.360689, acc 0.78125\n",
      "2019-03-27T13:14:27.327914: step 3182, loss 0.459004, acc 0.703125\n",
      "2019-03-27T13:14:27.388987: step 3183, loss 0.478818, acc 0.640625\n",
      "2019-03-27T13:14:27.447727: step 3184, loss 0.419868, acc 0.75\n",
      "2019-03-27T13:14:27.507081: step 3185, loss 0.424923, acc 0.71875\n",
      "2019-03-27T13:14:27.570085: step 3186, loss 0.405348, acc 0.765625\n",
      "2019-03-27T13:14:27.630385: step 3187, loss 0.425237, acc 0.734375\n",
      "2019-03-27T13:14:27.694384: step 3188, loss 0.457447, acc 0.625\n",
      "2019-03-27T13:14:27.755901: step 3189, loss 0.466225, acc 0.640625\n",
      "2019-03-27T13:14:27.817863: step 3190, loss 0.516606, acc 0.734375\n",
      "2019-03-27T13:14:27.878467: step 3191, loss 0.322755, acc 0.84375\n",
      "2019-03-27T13:14:27.940479: step 3192, loss 0.535189, acc 0.671875\n",
      "2019-03-27T13:14:28.002814: step 3193, loss 0.402005, acc 0.765625\n",
      "2019-03-27T13:14:28.066751: step 3194, loss 0.430598, acc 0.71875\n",
      "2019-03-27T13:14:28.126660: step 3195, loss 0.42838, acc 0.703125\n",
      "2019-03-27T13:14:28.187463: step 3196, loss 0.491356, acc 0.671875\n",
      "2019-03-27T13:14:28.247141: step 3197, loss 0.567267, acc 0.578125\n",
      "2019-03-27T13:14:28.308592: step 3198, loss 0.48026, acc 0.6875\n",
      "2019-03-27T13:14:28.370500: step 3199, loss 0.451967, acc 0.734375\n",
      "2019-03-27T13:14:28.431966: step 3200, loss 0.502781, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-27T13:14:28.465301: step 3200, loss 1.03293, acc 0.4631\n",
      "\n",
      "Saved model checkpoint to /home/jackson/GitHub/cnn-text-political-bias-tf/runs/1553706664/checkpoints/model-3200\n",
      "\n",
      "2019-03-27T13:14:28.634154: step 3201, loss 0.543374, acc 0.609375\n",
      "2019-03-27T13:14:28.691315: step 3202, loss 0.734885, acc 0.546875\n",
      "2019-03-27T13:14:28.756618: step 3203, loss 0.460283, acc 0.6875\n",
      "2019-03-27T13:14:28.815789: step 3204, loss 0.446103, acc 0.78125\n",
      "2019-03-27T13:14:28.878085: step 3205, loss 0.594531, acc 0.578125\n",
      "2019-03-27T13:14:28.936608: step 3206, loss 0.415653, acc 0.703125\n",
      "2019-03-27T13:14:28.996346: step 3207, loss 0.457264, acc 0.734375\n",
      "2019-03-27T13:14:29.057139: step 3208, loss 0.568113, acc 0.546875\n",
      "2019-03-27T13:14:29.122804: step 3209, loss 0.472077, acc 0.703125\n",
      "2019-03-27T13:14:29.184171: step 3210, loss 0.510275, acc 0.65625\n",
      "2019-03-27T13:14:29.246440: step 3211, loss 0.538525, acc 0.609375\n",
      "2019-03-27T13:14:29.311917: step 3212, loss 0.481589, acc 0.65625\n",
      "2019-03-27T13:14:29.372016: step 3213, loss 0.40055, acc 0.71875\n",
      "2019-03-27T13:14:29.432239: step 3214, loss 0.42169, acc 0.71875\n",
      "2019-03-27T13:14:29.494429: step 3215, loss 0.389302, acc 0.734375\n",
      "2019-03-27T13:14:29.558470: step 3216, loss 0.483136, acc 0.640625\n",
      "2019-03-27T13:14:29.624542: step 3217, loss 0.477346, acc 0.65625\n",
      "2019-03-27T13:14:29.689172: step 3218, loss 0.465285, acc 0.640625\n",
      "2019-03-27T13:14:29.750280: step 3219, loss 0.559964, acc 0.625\n",
      "2019-03-27T13:14:29.812014: step 3220, loss 0.570873, acc 0.640625\n",
      "2019-03-27T13:14:29.873359: step 3221, loss 0.514977, acc 0.59375\n",
      "2019-03-27T13:14:29.942213: step 3222, loss 0.486321, acc 0.625\n",
      "2019-03-27T13:14:30.004188: step 3223, loss 0.467527, acc 0.703125\n",
      "2019-03-27T13:14:30.066939: step 3224, loss 0.572115, acc 0.609375\n",
      "2019-03-27T13:14:30.129567: step 3225, loss 0.462536, acc 0.734375\n",
      "2019-03-27T13:14:30.190476: step 3226, loss 0.523739, acc 0.734375\n",
      "2019-03-27T13:14:30.250977: step 3227, loss 0.531203, acc 0.703125\n",
      "2019-03-27T13:14:30.311943: step 3228, loss 0.535705, acc 0.71875\n",
      "2019-03-27T13:14:30.378890: step 3229, loss 0.647302, acc 0.703125\n",
      "2019-03-27T13:14:30.442322: step 3230, loss 0.517828, acc 0.59375\n",
      "2019-03-27T13:14:30.503749: step 3231, loss 0.57518, acc 0.59375\n",
      "2019-03-27T13:14:30.566057: step 3232, loss 0.436753, acc 0.640625\n",
      "2019-03-27T13:14:30.624967: step 3233, loss 0.433434, acc 0.625\n",
      "2019-03-27T13:14:30.676055: step 3234, loss 0.582986, acc 0.666667\n",
      "2019-03-27T13:14:30.748675: step 3235, loss 0.49034, acc 0.6875\n",
      "2019-03-27T13:14:30.812535: step 3236, loss 0.436096, acc 0.75\n",
      "2019-03-27T13:14:30.876113: step 3237, loss 0.522699, acc 0.640625\n",
      "2019-03-27T13:14:30.939793: step 3238, loss 0.5253, acc 0.671875\n",
      "2019-03-27T13:14:31.000936: step 3239, loss 0.530996, acc 0.671875\n",
      "2019-03-27T13:14:31.064565: step 3240, loss 0.449086, acc 0.671875\n",
      "2019-03-27T13:14:31.122877: step 3241, loss 0.612521, acc 0.609375\n",
      "2019-03-27T13:14:31.188651: step 3242, loss 0.457253, acc 0.71875\n",
      "2019-03-27T13:14:31.248720: step 3243, loss 0.498038, acc 0.703125\n",
      "2019-03-27T13:14:31.313330: step 3244, loss 0.606142, acc 0.59375\n",
      "2019-03-27T13:14:31.373963: step 3245, loss 0.559881, acc 0.6875\n",
      "2019-03-27T13:14:31.434609: step 3246, loss 0.50503, acc 0.671875\n",
      "2019-03-27T13:14:31.500428: step 3247, loss 0.65545, acc 0.671875\n",
      "2019-03-27T13:14:31.564370: step 3248, loss 0.498684, acc 0.703125\n",
      "2019-03-27T13:14:31.631238: step 3249, loss 0.478657, acc 0.6875\n",
      "2019-03-27T13:14:31.693920: step 3250, loss 0.358048, acc 0.703125\n",
      "2019-03-27T13:14:31.757178: step 3251, loss 0.501507, acc 0.6875\n",
      "2019-03-27T13:14:31.816955: step 3252, loss 0.486176, acc 0.671875\n",
      "2019-03-27T13:14:31.877486: step 3253, loss 0.458299, acc 0.75\n",
      "2019-03-27T13:14:31.940937: step 3254, loss 0.436755, acc 0.6875\n",
      "2019-03-27T13:14:32.007453: step 3255, loss 0.452494, acc 0.75\n",
      "2019-03-27T13:14:32.069455: step 3256, loss 0.388317, acc 0.71875\n",
      "2019-03-27T13:14:32.130523: step 3257, loss 0.427917, acc 0.671875\n",
      "2019-03-27T13:14:32.198223: step 3258, loss 0.557649, acc 0.640625\n",
      "2019-03-27T13:14:32.257512: step 3259, loss 0.501146, acc 0.6875\n",
      "2019-03-27T13:14:32.317174: step 3260, loss 0.36701, acc 0.71875\n",
      "2019-03-27T13:14:32.379536: step 3261, loss 0.423877, acc 0.796875\n",
      "2019-03-27T13:14:32.439625: step 3262, loss 0.450346, acc 0.703125\n",
      "2019-03-27T13:14:32.502488: step 3263, loss 0.436848, acc 0.734375\n",
      "2019-03-27T13:14:32.561926: step 3264, loss 0.434789, acc 0.703125\n",
      "2019-03-27T13:14:32.618728: step 3265, loss 0.365843, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-27T13:14:32.681291: step 3266, loss 0.453558, acc 0.703125\n",
      "2019-03-27T13:14:32.749589: step 3267, loss 0.444187, acc 0.6875\n",
      "2019-03-27T13:14:32.815996: step 3268, loss 0.492979, acc 0.609375\n",
      "2019-03-27T13:14:32.880029: step 3269, loss 0.445248, acc 0.703125\n",
      "2019-03-27T13:14:32.942617: step 3270, loss 0.515146, acc 0.59375\n",
      "2019-03-27T13:14:33.002312: step 3271, loss 0.602446, acc 0.609375\n",
      "2019-03-27T13:14:33.061487: step 3272, loss 0.475247, acc 0.671875\n",
      "2019-03-27T13:14:33.120960: step 3273, loss 0.531912, acc 0.59375\n",
      "2019-03-27T13:14:33.184480: step 3274, loss 0.5501, acc 0.5625\n",
      "2019-03-27T13:14:33.248969: step 3275, loss 0.439215, acc 0.75\n",
      "2019-03-27T13:14:33.309451: step 3276, loss 0.439515, acc 0.6875\n",
      "2019-03-27T13:14:33.370940: step 3277, loss 0.472666, acc 0.609375\n",
      "2019-03-27T13:14:33.432930: step 3278, loss 0.423332, acc 0.75\n",
      "2019-03-27T13:14:33.494540: step 3279, loss 0.525217, acc 0.75\n",
      "2019-03-27T13:14:33.555701: step 3280, loss 0.395362, acc 0.75\n",
      "2019-03-27T13:14:33.615212: step 3281, loss 0.459242, acc 0.703125\n",
      "2019-03-27T13:14:33.674519: step 3282, loss 0.46789, acc 0.734375\n",
      "2019-03-27T13:14:33.735617: step 3283, loss 0.4406, acc 0.71875\n",
      "2019-03-27T13:14:33.795098: step 3284, loss 0.429808, acc 0.71875\n",
      "2019-03-27T13:14:33.856250: step 3285, loss 0.561582, acc 0.609375\n",
      "2019-03-27T13:14:33.918335: step 3286, loss 0.44506, acc 0.6875\n",
      "2019-03-27T13:14:33.978850: step 3287, loss 0.498068, acc 0.671875\n",
      "2019-03-27T13:14:34.038506: step 3288, loss 0.411513, acc 0.78125\n",
      "2019-03-27T13:14:34.096891: step 3289, loss 0.463336, acc 0.640625\n",
      "2019-03-27T13:14:34.156086: step 3290, loss 0.488772, acc 0.640625\n",
      "2019-03-27T13:14:34.216313: step 3291, loss 0.546424, acc 0.578125\n",
      "2019-03-27T13:14:34.280446: step 3292, loss 0.463134, acc 0.703125\n",
      "2019-03-27T13:14:34.344963: step 3293, loss 0.408908, acc 0.703125\n",
      "2019-03-27T13:14:34.404477: step 3294, loss 0.463591, acc 0.671875\n",
      "2019-03-27T13:14:34.466929: step 3295, loss 0.502285, acc 0.609375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/jackson/GitHub/cnn-text-political-bias-tf/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/GitHub/cnn-text-political-bias-tf/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/GitHub/cnn-text-political-bias-tf/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train, vocab_processor, x_dev, y_dev)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/GitHub/cnn-text-political-bias-tf/train.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 _, step, summaries, loss, accuracy = sess.run(\n\u001b[1;32m    153\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                     feed_dict)\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run -i train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7fab1cfb5ba8>\n",
      "BATCH_SIZE=<absl.flags._flag.Flag object at 0x7fab1cfb5e80>\n",
      "CHECKPOINT_DIR=<absl.flags._flag.Flag object at 0x7fab1cfb5d30>\n",
      "EVAL_TRAIN=<absl.flags._flag.BooleanFlag object at 0x7fab1cfb5cc0>\n",
      "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7fab1cfb5c18>\n",
      "NEGATIVE_DATA_FILE=<absl.flags._flag.Flag object at 0x7fab1cfb5f60>\n",
      "NEUTRAL_DATA_FILE=<absl.flags._flag.Flag object at 0x7fab1cfb5ef0>\n",
      "POSITIVE_DATA_FILE=<absl.flags._flag.Flag object at 0x7fab1cfb5fd0>\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "../vocab; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/jackson/GitHub/cnn-text-political-bias-tf/eval.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Map data into vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mvocab_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mvocab_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabularyProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 85\u001b[0;31m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jackson/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ../vocab; No such file or directory"
     ]
    }
   ],
   "source": [
    "run -i eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
